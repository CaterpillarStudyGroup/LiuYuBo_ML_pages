# 主成分分析法 PCA Principal Component Analysis

一个非监督的机器学习算法  
主要用于数据的降维  
通过降维，可以发现更便于人类理解的特征  
其他应用：可视化，去噪

# 二维降至一维

![](http://windmissing.github.io/images/2019/88.png)
如何把图中的二维空间中的点降至一维？  
![](http://windmissing.github.io/images/2019/89.png)
以下三种方法哪种更好？  
第三种，因为第三种最大程度地保留了原数据之间的关系  

# 如 何找到这个刘样本间间距(方差)最大的轴？  

![](http://windmissing.github.io/images/2019/90.png)
![](http://windmissing.github.io/images/2019/91.png)

第一步：将所有样本的均值归0 （demean）  
![](http://windmissing.github.io/images/2019/92.png)   
由于均值为0，则可以化简以上公式：
![](http://windmissing.github.io/images/2019/93.png)  
这里的x是样本映射到坐标轴以后的新的样本的值。  
第二步：求一个轴的方向w = (w1, w2)，使用所有的样本映射到w以后，有：
![](http://windmissing.github.io/images/2019/94.png)  
推导过程：  
![](http://windmissing.github.io/images/2019/95.png)
推导结果：
![](http://windmissing.github.io/images/2019/96.png)
这是一个求目标函数的最优化问题，使用梯度上升法解决。  

# 主成分分析法和线性回归的区别  
![](http://windmissing.github.io/images/2019/97.png)

  | 主成分分析  | 线性回归
--|---|--
坐标轴  | 2个特征  | 1个特征和输出标记
要求的是什么  | 一个方向  | 一根直线
经过点的线与什么垂直  | 所求的方向垂直  |  x轴
目标  | 方差最大  | MSE最小
