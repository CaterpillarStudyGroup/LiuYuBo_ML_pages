<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>LYBStudy</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="The note of LYB">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="Introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="Chapter4/4-0.html"><strong aria-hidden="true">2.</strong> 第四章：K近邻算法</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="Chapter4/4-1.html"><strong aria-hidden="true">2.1.</strong> 4-1 K近邻算法基础</a></li><li class="chapter-item expanded "><a href="Chapter4/4-2.html"><strong aria-hidden="true">2.2.</strong> 4-2 scikit-learn中的机器学习算法的封装</a></li><li class="chapter-item expanded "><a href="Chapter4/4-3.html"><strong aria-hidden="true">2.3.</strong> 4-3 训练数据集，测试数据集</a></li><li class="chapter-item expanded "><a href="Chapter4/4-4.html"><strong aria-hidden="true">2.4.</strong> 4-4 分类准确度</a></li><li class="chapter-item expanded "><a href="Chapter4/4-5.html"><strong aria-hidden="true">2.5.</strong> 4-5 超参数</a></li><li class="chapter-item expanded "><a href="Chapter4/4-6.html"><strong aria-hidden="true">2.6.</strong> 4-6 网格搜索</a></li><li class="chapter-item expanded "><a href="Chapter4/4-7.html"><strong aria-hidden="true">2.7.</strong> 4-7 数据归一化 Feature Scaling</a></li><li class="chapter-item expanded "><a href="Chapter4/4-8.html"><strong aria-hidden="true">2.8.</strong> 4-8 scikit-learn中的Scaler</a></li><li class="chapter-item expanded "><a href="Chapter4/4-9.html"><strong aria-hidden="true">2.9.</strong> 4-9 更多有关K近邻算法的思考</a></li></ol></li><li class="chapter-item expanded "><a href="Chapter5/5-0.html"><strong aria-hidden="true">3.</strong> 第五章：线性回归法</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="Chapter5/5-1.html"><strong aria-hidden="true">3.1.</strong> 5-1 简单线性回归</a></li><li class="chapter-item expanded "><a href="Chapter5/5-2.html"><strong aria-hidden="true">3.2.</strong> 5-2 最小二乘法</a></li><li class="chapter-item expanded "><a href="Chapter5/5-3.html"><strong aria-hidden="true">3.3.</strong> 5-3 简单线性回归的实现</a></li><li class="chapter-item expanded "><a href="Chapter5/5-4.html"><strong aria-hidden="true">3.4.</strong> 5-4 参数计算向量化</a></li><li class="chapter-item expanded "><a href="Chapter5/5-5.html"><strong aria-hidden="true">3.5.</strong> 5-5 衡量线性回归算法的指标</a></li><li class="chapter-item expanded "><a href="Chapter5/5-6.html"><strong aria-hidden="true">3.6.</strong> 5-6 最好的衡量线性回归法的指标 R Squared</a></li><li class="chapter-item expanded "><a href="Chapter5/5-7.html"><strong aria-hidden="true">3.7.</strong> 5-7 简单线性回归和正规方程解</a></li><li class="chapter-item expanded "><a href="Chapter5/5-8.html"><strong aria-hidden="true">3.8.</strong> 5-8 实现多元线性回归</a></li><li class="chapter-item expanded "><a href="Chapter5/5-9.html"><strong aria-hidden="true">3.9.</strong> 5-9 scikit-learn中的回归算法</a></li><li class="chapter-item expanded "><a href="Chapter5/5-10.html"><strong aria-hidden="true">3.10.</strong> 5-10 线性回归的可解释性和更多思考</a></li></ol></li><li class="chapter-item expanded "><a href="Chapter6/6-1.html"><strong aria-hidden="true">4.</strong> 第六章：梯度下降法</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="Chapter6/6-1.html"><strong aria-hidden="true">4.1.</strong> 6-1 什么是梯度下降法</a></li><li class="chapter-item expanded "><a href="Chapter6/6-2.html"><strong aria-hidden="true">4.2.</strong> 6-2 模拟实现梯度下降法</a></li><li class="chapter-item expanded "><a href="Chapter6/6-3.html"><strong aria-hidden="true">4.3.</strong> 6-3 多元线性回归中的梯度下降法</a></li><li class="chapter-item expanded "><a href="Chapter6/6-4.html"><strong aria-hidden="true">4.4.</strong> 6-4 在线性回归模型中使用梯度下降法</a></li><li class="chapter-item expanded "><a href="Chapter6/6-5.html"><strong aria-hidden="true">4.5.</strong> 6-5 梯度下降的向量化</a></li><li class="chapter-item expanded "><a href="Chapter6/6-6.html"><strong aria-hidden="true">4.6.</strong> 6-6 随机梯度下降</a></li><li class="chapter-item expanded "><a href="Chapter6/6-7.html"><strong aria-hidden="true">4.7.</strong> 6-7 代码实现随机梯度下降</a></li><li class="chapter-item expanded "><a href="Chapter6/6-8.html"><strong aria-hidden="true">4.8.</strong> 6-8 调试梯度下降法</a></li><li class="chapter-item expanded "><a href="Chapter6/6-9.html"><strong aria-hidden="true">4.9.</strong> 6-9 有关梯度下降法的更多深入讨论</a></li></ol></li><li class="chapter-item expanded "><a href="Chapter7/7-0.html"><strong aria-hidden="true">5.</strong> 第七章：PCA与梯度上升法</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="Chapter7/7-1.html"><strong aria-hidden="true">5.1.</strong> 7-1 什么是PCA</a></li><li class="chapter-item expanded "><a href="Chapter7/7-2.html"><strong aria-hidden="true">5.2.</strong> 7-2 使用梯度上升法求解主成分分析问题</a></li><li class="chapter-item expanded "><a href="Chapter7/7-3.html"><strong aria-hidden="true">5.3.</strong> 7-3 代码实现主成分分析问题</a></li><li class="chapter-item expanded "><a href="Chapter7/7-4.html"><strong aria-hidden="true">5.4.</strong> 7-4 求数据的前N个主成分</a></li><li class="chapter-item expanded "><a href="Chapter7/7-5.html"><strong aria-hidden="true">5.5.</strong> 7-5 高维数据向低维数据映射</a></li><li class="chapter-item expanded "><a href="Chapter7/7-6.html"><strong aria-hidden="true">5.6.</strong> 7-6 scikit learn中的PCA</a></li><li class="chapter-item expanded "><a href="Chapter7/7-7.html"><strong aria-hidden="true">5.7.</strong> 7-7 MNIST数据集</a></li><li class="chapter-item expanded "><a href="Chapter7/7-8.html"><strong aria-hidden="true">5.8.</strong> 7-8 使用PCA降噪</a></li><li class="chapter-item expanded "><a href="Chapter7/7-9.html"><strong aria-hidden="true">5.9.</strong> 7-9 人脸识别和特征脸(未完成)</a></li></ol></li><li class="chapter-item expanded "><a href="Chapter8/8-1.html"><strong aria-hidden="true">6.</strong> 第八章：多项式回归与模型泛化</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="Chapter8/8-1.html"><strong aria-hidden="true">6.1.</strong> 8-1 什么是多项式回归</a></li><li class="chapter-item expanded "><a href="Chapter8/8-2.html"><strong aria-hidden="true">6.2.</strong> 8-2 scikit-learn中的多项式回归和pipeline</a></li><li class="chapter-item expanded "><a href="Chapter8/8-3.html"><strong aria-hidden="true">6.3.</strong> 8-3 过拟合和欠拟合</a></li><li class="chapter-item expanded "><a href="Chapter8/8-4.html"><strong aria-hidden="true">6.4.</strong> 8-4 为什么要训练数据集和测试数据集</a></li><li class="chapter-item expanded "><a href="Chapter8/8-5.html"><strong aria-hidden="true">6.5.</strong> 8-5 学习曲线</a></li><li class="chapter-item expanded "><a href="Chapter8/8-6.html"><strong aria-hidden="true">6.6.</strong> 8-6 验证数据集与交叉验证</a></li><li class="chapter-item expanded "><a href="Chapter8/8-7.html"><strong aria-hidden="true">6.7.</strong> 8-7 偏差方差权衡 Bias Variance Trade off</a></li><li class="chapter-item expanded "><a href="Chapter8/8-8.html"><strong aria-hidden="true">6.8.</strong> 8-8 模型正则化 Regularization</a></li><li class="chapter-item expanded "><a href="Chapter8/8-9.html"><strong aria-hidden="true">6.9.</strong> 8-9 LASSO Regularization</a></li><li class="chapter-item expanded "><a href="Chapter8/8-10.html"><strong aria-hidden="true">6.10.</strong> 8-10 L1,L2和弹性网络</a></li></ol></li><li class="chapter-item expanded "><a href="Chapter9/9-1.html"><strong aria-hidden="true">7.</strong> 第九章：逻辑回归</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="Chapter9/9-1.html"><strong aria-hidden="true">7.1.</strong> 9-1 逻辑回归 Logistic Regression</a></li><li class="chapter-item expanded "><a href="Chapter9/9-2.html"><strong aria-hidden="true">7.2.</strong> 9-2 逻辑回归的损失函数</a></li><li class="chapter-item expanded "><a href="Chapter9/9-3.html"><strong aria-hidden="true">7.3.</strong> 9-3 逻辑回归算法损失函数的梯度</a></li><li class="chapter-item expanded "><a href="Chapter9/9-4.html"><strong aria-hidden="true">7.4.</strong> 9-4 实现逻辑回归算法</a></li><li class="chapter-item expanded "><a href="Chapter9/9-5.html"><strong aria-hidden="true">7.5.</strong> 9-5 决策边界</a></li><li class="chapter-item expanded "><a href="Chapter9/9-6.html"><strong aria-hidden="true">7.6.</strong> 9-6 在逻辑回归中使用多项式特征</a></li><li class="chapter-item expanded "><a href="Chapter9/9-7.html"><strong aria-hidden="true">7.7.</strong> 9-7 scikit-learn中的逻辑回归</a></li><li class="chapter-item expanded "><a href="Chapter9/9-8.html"><strong aria-hidden="true">7.8.</strong> 9-8 OvR与OvO</a></li></ol></li><li class="chapter-item expanded "><a href="Chapter10/10-1.html"><strong aria-hidden="true">8.</strong> 第十章：评价分类结果</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="Chapter10/10-1.html"><strong aria-hidden="true">8.1.</strong> 10-1 准确度的陷阱和混淆矩阵</a></li><li class="chapter-item expanded "><a href="Chapter10/10-2.html"><strong aria-hidden="true">8.2.</strong> 10-2 精确率和召回率</a></li><li class="chapter-item expanded "><a href="Chapter10/10-3.html"><strong aria-hidden="true">8.3.</strong> 10-3 实现混淆矩阵、精准率、召回率</a></li><li class="chapter-item expanded "><a href="Chapter10/10-4.html"><strong aria-hidden="true">8.4.</strong> 10-4 F1 score</a></li><li class="chapter-item expanded "><a href="Chapter10/10-5.html"><strong aria-hidden="true">8.5.</strong> 10-5 Precision-Recall平衡</a></li><li class="chapter-item expanded "><a href="Chapter10/10-6.html"><strong aria-hidden="true">8.6.</strong> 10-6 precision-recall曲线</a></li><li class="chapter-item expanded "><a href="Chapter10/10-7.html"><strong aria-hidden="true">8.7.</strong> 10-7 ROC曲线</a></li><li class="chapter-item expanded "><a href="Chapter10/10-8.html"><strong aria-hidden="true">8.8.</strong> 10-8 多分类问题中的混淆矩阵</a></li></ol></li><li class="chapter-item expanded "><a href="Chapter11/11-1.html"><strong aria-hidden="true">9.</strong> 第十一章：支撑向量机 SVM</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="Chapter11/11-1.html"><strong aria-hidden="true">9.1.</strong> 11-1 什么是支撑向量机</a></li><li class="chapter-item expanded "><a href="Chapter11/11-2.html"><strong aria-hidden="true">9.2.</strong> 11-2 支撑向量机的推导过程</a></li><li class="chapter-item expanded "><a href="Chapter11/11-3.html"><strong aria-hidden="true">9.3.</strong> 11-3 Soft Margin和SVM的正则化</a></li><li class="chapter-item expanded "><a href="Chapter11/11-4.html"><strong aria-hidden="true">9.4.</strong> 11-4 scikit-leran中的SVM</a></li><li class="chapter-item expanded "><a href="Chapter11/11-5.html"><strong aria-hidden="true">9.5.</strong> 11-5 SVM中使用多项式特征</a></li><li class="chapter-item expanded "><a href="Chapter11/11-6.html"><strong aria-hidden="true">9.6.</strong> 11-6 什么是核函数</a></li><li class="chapter-item expanded "><a href="Chapter11/11-7.html"><strong aria-hidden="true">9.7.</strong> 11-7 高斯核函数</a></li><li class="chapter-item expanded "><a href="Chapter11/11-8.html"><strong aria-hidden="true">9.8.</strong> 11-8 scikit-learn中的高斯核函数</a></li><li class="chapter-item expanded "><a href="Chapter11/11-9.html"><strong aria-hidden="true">9.9.</strong> 11-9 SVM思想解决回归问题</a></li></ol></li><li class="chapter-item expanded "><a href="Chapter12/12-1.html"><strong aria-hidden="true">10.</strong> 第十二章：决策树</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="Chapter12/12-1.html"><strong aria-hidden="true">10.1.</strong> 12-1 什么是决策树</a></li><li class="chapter-item expanded "><a href="Chapter12/12-2.html"><strong aria-hidden="true">10.2.</strong> 12-2 信息熵</a></li><li class="chapter-item expanded "><a href="Chapter12/12-3.html"><strong aria-hidden="true">10.3.</strong> 12-3 使用信息寻找最优划分</a></li><li class="chapter-item expanded "><a href="Chapter12/12-4.html"><strong aria-hidden="true">10.4.</strong> 12-4 基尼系数</a></li><li class="chapter-item expanded "><a href="Chapter12/12-5.html"><strong aria-hidden="true">10.5.</strong> 12-5 CART和决策树中的超参数</a></li><li class="chapter-item expanded "><a href="Chapter12/12-6.html"><strong aria-hidden="true">10.6.</strong> 12-6 决策树解决回归问题</a></li><li class="chapter-item expanded "><a href="Chapter12/12-7.html"><strong aria-hidden="true">10.7.</strong> 12-7 决策树的局限性</a></li></ol></li><li class="chapter-item expanded "><a href="Chapter13/13-1.html"><strong aria-hidden="true">11.</strong> 第十三章：集成学习和随机森林</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="Chapter13/13-1.html"><strong aria-hidden="true">11.1.</strong> 13-1 什么是集成学习</a></li><li class="chapter-item expanded "><a href="Chapter13/13-2.html"><strong aria-hidden="true">11.2.</strong> 13-2 soft voting</a></li><li class="chapter-item expanded "><a href="Chapter13/13-3.html"><strong aria-hidden="true">11.3.</strong> 13-3 bagging和pasting</a></li><li class="chapter-item expanded "><a href="Chapter13/13-4.html"><strong aria-hidden="true">11.4.</strong> 13-4 更多关于bagging的讨论</a></li><li class="chapter-item expanded "><a href="Chapter13/13-5.html"><strong aria-hidden="true">11.5.</strong> 13-5 随机森林和extra-trees</a></li><li class="chapter-item expanded "><a href="Chapter13/13-6.html"><strong aria-hidden="true">11.6.</strong> 13-6 ada boosting和gradiesnt boosting</a></li><li class="chapter-item expanded "><a href="Chapter13/13-7.html"><strong aria-hidden="true">11.7.</strong> 13-7 Stacking</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">LYBStudy</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/CaterpillarStudyGroup/LiuYuBo_ML_pages.git" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>在线阅读地址：https://caterpillarstudygroup.github.io/LiuYuBo_ML_pages/</p>
<p>《python3入门机器学习 经典算法与应用》学习笔记<br />
视频资源链接：https://coding.imooc.com/class/169.html</p>
<p>这套视频课系列原作者刘宇波。课程发布于慕课网。<br />
课程对常见机器学习算法做了基础的介绍。既有原理也有编程实践。还有一些对算法的思考。<br />
在讲原理有少量的数学推导，数学基础不好也能看懂。<br />
简单的算法配有python3实现，复杂算法使用slearn观察算法效果，对算法有直观印象。<br />
适用于机器学习入门，也可用于熟悉python、numpy和sklearn。<br />
这个系列课程不错，墙裂推荐</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="knn---k近邻算法---k-nearest-neighbors"><a class="header" href="#knn---k近邻算法---k-nearest-neighbors">KNN - K近邻算法 - K-Nearest Neighbors</a></h1>
<p>KNN算法是非常适合入门机器学习的算法，因为</p>
<ul>
<li>思想极度简单</li>
<li>应用数学知识少</li>
<li>效果好</li>
<li>可以解释机器学习算法使用过程中的很多细节问题</li>
<li>更完整地刻画机器学习应用的流程</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p>2019-10-19</p>
<h1 id="knn---k近邻算法---k-nearest-neighbors-1"><a class="header" href="#knn---k近邻算法---k-nearest-neighbors-1">KNN - K近邻算法 - K-Nearest Neighbors</a></h1>
<p><img src="http://windmissing.github.io/images_for_gitbook/liu_yu_bo_play_with_machine_learning/17.png" alt="" /></p>
<p>本质：如果两个样本足够相似，它们有更高的概率属于同一个类别</p>
<h1 id="代码实现knn算法"><a class="header" href="#代码实现knn算法">代码实现KNN算法</a></h1>
<p>假设原始训练数据如下：</p>
<pre><code class="language-python">raw_data_X = [[3.39, 2.33],
              [3.11, 1.78],
              [1.34, 3.36],
              [3.58, 4.67],
              [2.28, 2.86],
              [7.42, 4.69],
              [5.74, 3.53],
              [9.17, 2.51],
              [7.79, 3.42],
              [7.93, 0.79]
             ]
raw_data_y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
</code></pre>
<p>待求数据如下：</p>
<pre><code class="language-python">x = np.array([8.09, 3.36])
</code></pre>
<h2 id="数据准备"><a class="header" href="#数据准备">数据准备</a></h2>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

X_train = np.array(raw_data_X)
y_train = np.array(raw_data_y)

plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], color = 'g')
plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], color = 'r')
plt.scatter(x[0], x[1], color = 'b')
plt.show()
</code></pre>
<p>效果：<br />
<img src="http://windmissing.github.io/images_for_gitbook/liu_yu_bo_play_with_machine_learning/18.png" alt="" /></p>
<h2 id="knn过程"><a class="header" href="#knn过程">KNN过程</a></h2>
<h3 id="欧拉距离"><a class="header" href="#欧拉距离">欧拉距离</a></h3>
<p>假设有a, b两个点，平面中两个点之间的欧拉距离为:</p>
<p>$$
\sqrt {(x^{(a)}-x^{(b)})^2+(y^{(a)}-y^{(b)})^2}
$$</p>
<p>立体中两个点的欧拉距离为:</p>
<p>$$
\sqrt {(x^{(a)}-x^{(b)})^2+(y^{(a)}-y^{(b)})^2+(z^{(a)}-z^{(b)})^2}
$$</p>
<p>任意维度中两个点的欧拉距离为:</p>
<p>$$
\sqrt {(X^{(a)}_1-X^{(b)}_1)^2+ (X^{(a)}_2-X^{(b)}_2)^2+...+(X^{(a)}_n-X^{(b)}_n)^2}
$$</p>
<p>或</p>
<p>$$
\sqrt {\sum^n_{i=1} (X^{(a)}_i-X^{(b)}_i)^2}
$$</p>
<p>其中上标a, b代码第a, b个数据。下标1, 2代码数据的第1, 2个特征</p>
<p>代码如下：</p>
<pre><code class="language-python">distances = [np.sum((x_train - x) ** 2) for x_train in X_train]
nearest = np.argsort(distances)
topK_y = [y_train[i] for i in nearest[:k]]
from collections import Counter
votes = Counter(topK_y)
predict_y = votes.most_common(1)[0][0]
</code></pre>
<p>运行结果：predict_y = 1</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="代码实现knn算法-1"><a class="header" href="#代码实现knn算法-1">代码实现KNN算法</a></h1>
<pre><code class="language-python">import numpy as np
from math import sqrt
from collections import Counter

def kNN_classify(k, X_train, y_train, x):
    assert 1 &lt;= k &lt;= X_train.shape[0], &quot;k must be valid&quot;
    assert X_train.shape[0] == y_train.shape[0], &quot;the size of X_train must equal to the size of y_train&quot;
    assert X_train.shape[1] == x.shape[0], &quot;the feature number of x must be equal to X_train&quot;

    distances = [sqrt(np.sum((x_train-x)**2)) for x_train in X_train]
    nearst = np.argsort(distances)

    topK_y = [y_train[i] for i in nearst[:k]]
    votes = Counter(topK_y)

    return votes.most_common(1)[0][0]
</code></pre>
<h2 id="准备数据"><a class="header" href="#准备数据">准备数据</a></h2>
<pre><code class="language-python">import numpy as np
raw_data_X = [[3.39, 2.33],
              [3.11, 1.78],
              [1.34, 3.36],
              [3.58, 4.67],
              [2.28, 2.86],
              [7.42, 4.69],
              [5.74, 3.53],
              [9.17, 2.51],
              [7.79, 3.42],
              [7.93, 0.79]
             ]
raw_data_y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]

X_train = raw_data_X
y_train = raw_data_y

x = np.array([8.09, 3.36])
</code></pre>
<h2 id="调用算法"><a class="header" href="#调用算法">调用算法</a></h2>
<pre><code class="language-python">predict_y = kNN_classify(6, X_train, y_train, x)
</code></pre>
<p>运行结果：predict_y = 1</p>
<h1 id="什么是机器学习"><a class="header" href="#什么是机器学习">什么是机器学习</a></h1>
<p><img src="http://windmissing.github.io/images_for_gitbook/liu_yu_bo_play_with_machine_learning/23.png" alt="" /></p>
<p>KNN是一个不需要训练的算法<br />
KNN没有模型，或者说训练数据就是它的模型</p>
<h1 id="使用scikit-learn中的knn"><a class="header" href="#使用scikit-learn中的knn">使用scikit-learn中的kNN</a></h1>
<h2 id="错误写法"><a class="header" href="#错误写法">错误写法</a></h2>
<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier

kNN_classifier.fit(X_train, y_train)
kNN_classifier.predict(x)
</code></pre>
<p>这样写会报错：<br />
<img src="http://windmissing.github.io/images_for_gitbook/liu_yu_bo_play_with_machine_learning/24.png" alt="" /><br />
原因是，predict为了兼容多组测试数据的场景，要求参数是个矩阵</p>
<h2 id="正确写法"><a class="header" href="#正确写法">正确写法</a></h2>
<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier

kNN_classifier.fit(X_train, y_train)
X_predict = x.reshape(1, -1)
y_predict = kNN_classifier.predict(X_predict)
</code></pre>
<p>运行结果：predict_y[0] = 1</p>
<h1 id="重新整理我们的knn的代码"><a class="header" href="#重新整理我们的knn的代码">重新整理我们的kNN的代码</a></h1>
<h2 id="封装成sklearn风格的类"><a class="header" href="#封装成sklearn风格的类">封装成sklearn风格的类</a></h2>
<pre><code class="language-python">import numpy as np
from math import sqrt
from collections import Counter

class kNNClassifier:

    def __init__(self, k):
        &quot;&quot;&quot;初始化kNN分类器&quot;&quot;&quot;
        assert k &gt;= 1, &quot;K must be valid!&quot;
        self.k = k
        self._X_train = None
        self._y_train = None

    def fit(self, X_train, y_train):
        &quot;&quot;&quot;根据训练数据集X_train和y_train训练kNN分类器&quot;&quot;&quot;
        assert X_train.shape[0] == y_train.shape[0], &quot;the size of X_train must equal to the size of y_train&quot;
        assert self.k &lt;= X_train.shape[0], &quot;the size of X_train must be at least k&quot;

        self._X_train = X_train
        self._y_train = y_train
        return self

    def predict(self, X_predict):
        &quot;&quot;&quot;给定待预测数据集X_predict, 返回表示X_predict的结果向量&quot;&quot;&quot;
        assert self._X_train is not None and self._X_train is not None, &quot;must fit before predict&quot;
        assert self._X_train.shape[1] == X_predict.shape[1], &quot;the feature number of X_predict must be equal to X_train&quot;

        y_predict = [self._predict(x) for x in X_predict]
        return np.array(y_predict)

    def _predict(self, x):
        &quot;&quot;&quot;给定单个待测数据x，返回x的预测结果&quot;&quot;&quot;
        assert self._X_train.shape[1] == x.shape[0], &quot;the feature number of x must be equal to X_train&quot;

        distances = [sqrt(np.sum((x_train-x)**2)) for x_train in self._X_train]
        nearst = np.argsort(distances)

        topK_y = [self._y_train[i] for i in nearst[:self.k]]
        votes = Counter(topK_y)

        return votes.most_common(1)[0][0]

    def __repr__(self):
        return &quot;KNN(k=%d)&quot; % self.k
</code></pre>
<h2 id="使用knnclassifier"><a class="header" href="#使用knnclassifier">使用kNNClassifier</a></h2>
<pre><code class="language-python">knn_clf = kNNClassifier(k=6)
knn_clf.fit(X_train, y_train)
y_predict = knn_clf.predict(X_predict)
</code></pre>
<p>运行结果：predict_y[0] = 1</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="判断机器学习算法的性能"><a class="header" href="#判断机器学习算法的性能">判断机器学习算法的性能</a></h1>
<p><img src="http://windmissing.github.io/images_for_gitbook/liu_yu_bo_play_with_machine_learning/25.png" alt="" /></p>
<p>改进：训练和测试数据集的分离，train test split<br />
<img src="http://windmissing.github.io/images_for_gitbook/liu_yu_bo_play_with_machine_learning/27.png" alt="" /></p>
<p>但这种方式也有它的问题，后面其它小节会讲到</p>
<h1 id="准备iris数据"><a class="header" href="#准备iris数据">准备iris数据</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data
y = iris.target
</code></pre>
<p>X.shape为(150, 4)，y.shape为(150,)</p>
<h1 id="train_test_split"><a class="header" href="#train_test_split">train_test_split</a></h1>
<p>注意1：本例中训练数据集的为如下：<br />
<img src="http://windmissing.github.io%5Cimages_for_gitbook/liu_yu_bo_play_with_machine_learning%5C26.png" alt="" /><br />
因此按顺序取前多少个样本不会有很好的效果，要先对数据乱序化</p>
<p>注意2：本例中X和y是分离的，但它们不能分别乱序化。乱序化的同时要保证样本和标签是对应的。</p>
<h2 id="在notebook实现"><a class="header" href="#在notebook实现">在Notebook实现</a></h2>
<pre><code class="language-python">shuffle_indexes = np.random.permutation(len(X))
test_ratio = 0.2
test_size = int(len(x) * test_ratio)
test_indexes = shuffle_indexes[:test_size]
train_indexes = shuffle_indexes[test_size:]

X_train = X[train_indexes]
y_train = y[train_indexes]
X_test = X[test_indexes]
y_test = y[test_indexes]
</code></pre>
<h2 id="将train_test_split封装成函数"><a class="header" href="#将train_test_split封装成函数">将train_test_split封装成函数</a></h2>
<pre><code class="language-python">import numpy as np

def train_test_split(X, y, test_ratio=0.2, seed=None):
    &quot;&quot;&quot;将X和y按照test_ratio分割成X_train，X_test，y_train，y_test&quot;&quot;&quot;

    assert X.shape[0] == y.shape[0], &quot;the size of X must be equal to the size of y&quot;
    assert 0.0 &lt;= test_ratio &lt;= 1.0, &quot;test_ration must be valid&quot;

    if seed:
        np.random.seed(seed)

    shuffle_indexes = np.random.permutation(len(X))

    test_size = int(len(x) * test_ratio)
    test_indexes = shuffle_indexes[:test_size]
    train_indexes = shuffle_indexes[test_size:]

    X_train = X[train_indexes]
    y_train = y[train_indexes]

    X_test = X[test_indexes]
    y_test = y[test_indexes]

    return X_train, X_test, y_train, y_test
</code></pre>
<h1 id="knn结合train_test_split计算分类准确度"><a class="header" href="#knn结合train_test_split计算分类准确度">KNN结合train_test_split计算分类准确度</a></h1>
<pre><code class="language-python">X_train, X_test, y_train, y_test = train_test_split(X, y)
my_Knn_clf = kNNClassifier(k = 3)    # kNNClassifier在上一节实现
my_Knn_clf.fit(X_train, y_train)
y_predict = my_Knn_clf.predict(X_test)
accuracy = sum(y_predict == y_test) / len(y_test)
</code></pre>
<h1 id="sklearn中的train_test_split"><a class="header" href="#sklearn中的train_test_split">sklearn中的train_test_split</a></h1>
<pre><code class="language-python">from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>在4-3中使用了分类准确度accuracy来评判KNN算法的性能。</p>
<h1 id="使用accuracy来评价knn算法对手写数据集的分类效果"><a class="header" href="#使用accuracy来评价knn算法对手写数据集的分类效果">使用accuracy来评价KNN算法对手写数据集的分类效果</a></h1>
<h2 id="加载手写数据集"><a class="header" href="#加载手写数据集">加载手写数据集</a></h2>
<pre><code class="language-python">from sklearn import datasets
digits = datasets.load_digits()
</code></pre>
<p>digits的内容如下：</p>
<p>输入：digits.keys()<br />
输出：dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])</p>
<p>输入：print(digits.DESCR)<br />
输出：digits的官方说明</p>
<p>输入：digits.data.shape()<br />
输出：(1797, 64)</p>
<p>输入：digits.target.shape()<br />
输出：(1797,)</p>
<p>输入：digits.target_names<br />
输出：array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</p>
<p>输入：</p>
<pre><code class="language-python">some_digit = X[666]

some_digit_image = some_digit.reshape(8, 8)
plt.imshow(some_digit_image, cmap = matplotlib.cm.binary)
plt.show()
</code></pre>
<p>输出：第666个样本的图像<br />
<img src="http://windmissing.github.io/images_for_gitbook/liu_yu_bo_play_with_machine_learning/28.png" alt="" /></p>
<h2 id="train_test_split--knn--accuracy"><a class="header" href="#train_test_split--knn--accuracy">train_test_split + KNN + accuracy</a></h2>
<pre><code class="language-python">import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_ratio=0.2)
my_knn_clf = KNNClassifier(k=3)
my_knn_clf.fit(X_train, y_train)
y_predict = my_knn_clf.predict(X_test)
accuracy = sum(y_predict == y_test) / len(y_test)
</code></pre>
<h1 id="scikit-learn中的accuracy_score"><a class="header" href="#scikit-learn中的accuracy_score">scikit-learn中的accuracy_score</a></h1>
<pre><code class="language-python">from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)

from sklearn.neighbors import KNeighborsClassifier
knn_clf = KNeighborsClassifier(n_neighbors=3)

knn_clf.fit(X_train, y_train)
y_predict = knn_clf.predict(X_test)

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_predict)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="超参数和模型参数"><a class="header" href="#超参数和模型参数">超参数和模型参数</a></h1>
<p>超参数是指运行机器学习算法之前要指定的参数<br />
KNN算法中的K就是一个超参数</p>
<p>模型参数：算法过程中学习的参数<br />
KNN算法没有模型参数</p>
<p>调参是指调超参数</p>
<h1 id="如何寻找好的超参数"><a class="header" href="#如何寻找好的超参数">如何寻找好的超参数</a></h1>
<ul>
<li>领域知识</li>
<li>经验数值</li>
<li>实验搜索</li>
</ul>
<h1 id="寻找最好的k"><a class="header" href="#寻找最好的k">寻找最好的K</a></h1>
<pre><code class="language-python">best_score = 0.0
best_k = -1
for k in range(1, 11):
    knn_clf = KNeighborsClassifier(n_neighbors=k)
    knn_clf.fit(X_train, y_train)
    score = knn_clf.score(X_test, y_test)
    if score &gt; best_score:
        best_k = k
        best_score = score

print(&quot;best_k = &quot;, best_k)
print(&quot;best_score = &quot;, best_score)
</code></pre>
<p>输出：<br />
best_k =  4<br />
best_score =  0.9916666666666667</p>
<h1 id="knn的超参数weights"><a class="header" href="#knn的超参数weights">KNN的超参数weights</a></h1>
<ul>
<li>
<p>普通的KNN算法：蓝色获胜<br />
<img src="http://windmissing.github.io/images_for_gitbook/liu_yu_bo_play_with_machine_learning/29.png" alt="" /></p>
</li>
<li>
<p>考虑距离的KNN算法：红色：1， 蓝色：1/3 + 1/4 = 7/12，蓝色获胜<br />
<img src="http://windmissing.github.io/images_for_gitbook/liu_yu_bo_play_with_machine_learning/30.png" alt="" /></p>
</li>
</ul>
<p>考虑距离的另一个优点：解决平票的情况</p>
<p><img src="http://windmissing.github.io/images_for_gitbook/liu_yu_bo_play_with_machine_learning/31.png" alt="" /></p>
<pre><code class="language-python">best_method = &quot;&quot;
best_score = 0.0
best_k = -1
for method in [&quot;uniform&quot;, &quot;distance&quot;]:
    for k in range(1, 11):
        knn_clf = KNeighborsClassifier(n_neighbors=k, weights=method)
        knn_clf.fit(X_train, y_train)
        score = knn_clf.score(X_test, y_test)
        if score &gt; best_score:
            best_k = k
            best_score = score
            best_method = method

print(&quot;best_k = &quot;, best_k)
print(&quot;best_score = &quot;, best_score)
print(&quot;best_method = &quot;, best_method)
</code></pre>
<p>输出结果：<br />
best_k =  4<br />
best_score =  0.9916666666666667<br />
best_method =  uniform</p>
<h1 id="knn的超参数p"><a class="header" href="#knn的超参数p">KNN的超参数p</a></h1>
<h2 id="关于距离的更多定义"><a class="header" href="#关于距离的更多定义">关于距离的更多定义</a></h2>
<ul>
<li>欧拉距离</li>
</ul>
<p>$$
\sqrt {\sum^n_{i=1} (X^{(a)}_i-X^{(b)}_i)^2}
$$</p>
<ul>
<li>曼哈顿距离</li>
</ul>
<p><img src="http://windmissing.github.io/images_for_gitbook/liu_yu_bo_play_with_machine_learning/32.png" alt="" /></p>
<ul>
<li>欧拉距离与曼哈顿距离的数学形式一致性</li>
</ul>
<p>$$
(\sum^n_{i=1} |X^{(a)}_i-X^{(b)}_i|^2)^\frac{1}{2}
$$</p>
<p>$$
(\sum^n_{i=1} |X^{(a)}_i-X^{(b)}_i|)^\frac{1}{1}
$$</p>
<ul>
<li>明可夫斯基距离 Minkowski distance</li>
</ul>
<p>$$
(\sum^n_{i=1} |X^{(a)}_i-X^{(b)}_i|^p)^\frac{1}{p}
$$</p>
<p>把欧拉距离和曼哈顿距离进一步抽象，得到以下公式</p>
<p><img src="http://windmissing.github.io%5Cimages%5C2019%5C33.png" alt="" /></p>
<p>p = 1: 曼哈顿距离<br />
p = 2: 欧拉距离<br />
p &gt; 2: 其他数学意义</p>
<pre><code class="language-python">%%time

best_p = -1
best_score = 0.0
best_k = -1

for k in range(1, 11):
    for p in range(1, 6):
        knn_clf = KNeighborsClassifier(n_neighbors=k, weights=&quot;distance&quot;, p = p)
        knn_clf.fit(X_train, y_train)
        score = knn_clf.score(X_test, y_test)
        if score &gt; best_score:
            best_k = k
            best_score = score
            best_p = p

print(&quot;best_k = &quot;, best_k)
print(&quot;best_score = &quot;, best_score)
print(&quot;best_p = &quot;, best_p)
</code></pre>
<p>输出结果：<br />
best_k =  3<br />
best_score =  0.9888888888888889<br />
best_p =  2<br />
Wall time: 37 s</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="grid-search"><a class="header" href="#grid-search">Grid Search</a></h1>
<h2 id="准备数据-1"><a class="header" href="#准备数据-1">准备数据</a></h2>
<pre><code class="language-python">from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)
</code></pre>
<h2 id="使用网格搜索"><a class="header" href="#使用网格搜索">使用网格搜索</a></h2>
<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

param_grid = [
    {
        'weights':['uniform'],
        'n_neighbors': [i for i in range(1, 11)]
    },
    {   
        'weights':['distance'],
        'n_neighbors': [i for i in range(1, 11)],
        'p': [i for i in range(1, 6)]
    }
]

knn_clf = KNeighborsClassifier()
grid_search = GridSearchCV(knn_clf, param_grid)

%%time
grid_search.fit(X_train, y_train)
</code></pre>
<p>输出结果：</p>
<p><img src="http://windmissing.github.io/images_for_gitbook/liu_yu_bo_play_with_machine_learning/34.png" alt="" /></p>
<h2 id="察看运行结果"><a class="header" href="#察看运行结果">察看运行结果</a></h2>
<p>输入：<code>grid_search.best_score_</code><br />
输出：0.9853862212943633</p>
<p>输入：<code>grid_search.best_params_</code>
输出：{'n_neighbors': 3, 'p': 3, 'weights': 'distance'}</p>
<p>注意1：这里的搜索结果与4-5中自己编写的网格搜索得到的结果不同，是因为评价方法不同，不用care<br />
注意2：以<code>_</code>结尾的参考表示为计算得到的参数，而不是用户输入的参数</p>
<h2 id="使用运行结果建立新的模型"><a class="header" href="#使用运行结果建立新的模型">使用运行结果建立新的模型</a></h2>
<pre><code class="language-python">knn_clf = grid_search.best_estimator_
knn_clf.score(X_test, y_test)
</code></pre>
<p>输出结果：
0.9833333333333333</p>
<h2 id="其它gridsearchcv参数"><a class="header" href="#其它gridsearchcv参数">其它GridSearchCV参数</a></h2>
<ul>
<li>njobs:使用多核计算</li>
<li>verbose：中间过程的打印级别</li>
</ul>
<h1 id="更多的距离定义"><a class="header" href="#更多的距离定义">更多的距离定义</a></h1>
<p><img src="http://windmissing.github.io%5Cimages%5C2019%5C35.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io%5Cimages%5C2019%5C36.png" alt="" /><br />
样本间的距离被发现时间所主导<br />
如果把发现时间改成以年为单位，样本间的距离又会被肿瘤大小所主导<br />
<strong>如果不对样本进行预处理，样本间的距离可能会被部分特征主导</strong><br />
解决方案：将所有的数据映射到同一个尺度，即归一化</p>
<h1 id="数据归一化-feature-scaling"><a class="header" href="#数据归一化-feature-scaling">数据归一化 (feature scaling)</a></h1>
<h2 id="最值归一化-normalization"><a class="header" href="#最值归一化-normalization">最值归一化 normalization</a></h2>
<p>把所有数据映射到0-1之间</p>
<p>$$
x_{scale} = \frac {x - x_{min}}{x_{max} - x_{min}}
$$</p>
<p>适用于分布有明显边界的情况，但是受outlier也就是极值(极端数据)影响较大会不准确</p>
<h2 id="均值方差归一化-standardization"><a class="header" href="#均值方差归一化-standardization">均值方差归一化 standardization</a></h2>
<p>把所有数据归一到均值为0方差为1的分布中</p>
<p>$$
x_{scale} = \frac {x-x_{mean}}{S}
$$</p>
<p>适用于数据分布没有明显的边界，有可能存在极端的数据值，其中S是标准差。由于均值方差归一化对于符合最值归一化的数据集有着同样好的归一化处理结果，所以一般推荐使用均值方差归一化方法。</p>
<h1 id="代码实现归一化"><a class="header" href="#代码实现归一化">代码实现归一化</a></h1>
<h2 id="最值归一化"><a class="header" href="#最值归一化">最值归一化</a></h2>
<h3 id="对向量所有元素归一化"><a class="header" href="#对向量所有元素归一化">对向量所有元素归一化</a></h3>
<p>生成随机数据</p>
<pre><code class="language-python">x = np.random.randint(0, 100, size = 100)
</code></pre>
<p><img src="http://windmissing.github.io%5Cimages%5C2019%5C39.png" alt="" /></p>
<p>归一化后</p>
<pre><code class="language-python">(x - np.min(x)) / (np.max(x) - np.min(x))
</code></pre>
<p><img src="http://windmissing.github.io%5Cimages%5C2019%5C40.png" alt="" /></p>
<h3 id="对矩阵元素归一化"><a class="header" href="#对矩阵元素归一化">对矩阵元素归一化</a></h3>
<pre><code class="language-python">X = np.random.randint(0, 100, (50, 2))
X = np.array(X, dtype = float)
X[:, 0] = (X[:,0] - np.min(X[:,0])) / (np.max(X[:,0])-np.min(X[:,0]))
X[:, 1] = (X[:,1] - np.min(X[:,1])) / (np.max(X[:,1])-np.min(X[:,1]))
</code></pre>
<h2 id="均值方差归一化-standardization-1"><a class="header" href="#均值方差归一化-standardization-1">均值方差归一化 standardization</a></h2>
<pre><code class="language-python">X2 = np.random.randint(0, 100, (50, 2))
X2 = np.array(X2, dtype = float)

X2[:,0] = (X2[:,0] - np.mean(X2[:,0])) / np.std(X2[:,0])
X2[:,1] = (X2[:,1] - np.mean(X2[:,1])) / np.std(X2[:,1])
</code></pre>
<p>输入：np.mean(X2[:,0])<br />
输出：7.771561172376095e-17</p>
<p>输入：np.std(X2[:,0])<br />
输出：1.0</p>
<div style="break-before: page; page-break-before: always;"></div><p>训练数据集和测试数据集不能分别归一化。<br />
测试数据集使用与训练数据集相同的mean和std做归一化<br />
即</p>
<pre><code>(x_test - mean_train) / std_train
</code></pre>
<h1 id="scikit-learn中的scaler的使用流程"><a class="header" href="#scikit-learn中的scaler的使用流程">scikit-learn中的Scaler的使用流程</a></h1>
<p><img src="http://windmissing.github.io%5Cimages%5C2019%5C41.png" alt="" /></p>
<h1 id="使用scikit-learn中的standardscaler"><a class="header" href="#使用scikit-learn中的standardscaler">使用scikit-learn中的StandardScaler</a></h1>
<h2 id="准备数据-2"><a class="header" href="#准备数据-2">准备数据</a></h2>
<pre><code class="language-python">import numpy as np
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data
y = iris.target

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=666)
</code></pre>
<h2 id="scikit-learn中的standardscaler"><a class="header" href="#scikit-learn中的standardscaler">scikit-learn中的StandardScaler</a></h2>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler

standardScaler = StandardScaler()
standardScaler.fit(X_train)

X_train_standard = standardScaler.transform(X_train)
X_test_standard = standardScaler.transform(X_test)
</code></pre>
<h2 id="standardscaler--knn--accuracy"><a class="header" href="#standardscaler--knn--accuracy">StandardScaler + KNN + accuracy</a></h2>
<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier
knn_clf = KNeighborsClassifier(n_neighbors=3)
knn_clf.fit(X_train_standard, y_train)
knn_clf.score(X_test_standard, y_test)
</code></pre>
<h1 id="自己实现standardscaler并封装成类"><a class="header" href="#自己实现standardscaler并封装成类">自己实现StandardScaler并封装成类</a></h1>
<pre><code class="language-python">import numpy as np

class StandardScaler:

    def __init__(self):
        self.mean_ = None
        self.scale_ = None

    def fit(self, X):
        &quot;&quot;&quot;根据训练数据集X获取数据的均值和方差&quot;&quot;&quot;
        assert X.ndim == 2, &quot;The dimension of X must be 2&quot;

        self.mean_ = np.array([np.mean(X[:,i]) for i in range(X.shape[1])])
        self.scale_ = np.array([np.std(X[:,i]) for i in range(X.shape[1])])

    def transform(self, X):
        &quot;&quot;&quot;将X根据这个StandardScaler进行均值方差归一化处理&quot;&quot;&quot;
        assert X.ndim == 2, &quot;The dimension of X must be 2&quot;
        assert self.mean_ is not None and self.scale_ is not None, &quot;must fit before transform!&quot;

        retX = np.empty(shape = X.shape, type = float)
        for col in range(X.shape[1]):
            retX[:, col] = (X[:,col] - self.mean_[col]) / self.scale_[col]
        return retX
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>优点：<br />
解决分类问题，天然可以解决多分类问题<br />
思想简单，效果强大<br />
可以解决回归问题：KNeighborsRegressor</p>
<p>缺点：<br />
效率低下，m个特征n个样本，预测一个数据的时间复杂度为O(m * n)<br />
高度数据相关<br />
预测结果不所有可解释性<br />
维数灾难：随着维度的增加，看似相近的两个点距离越来越大，解决方法：降维</p>
<!-- more -->
<h1 id="机器学习流程回顾"><a class="header" href="#机器学习流程回顾">机器学习流程回顾</a></h1>
<p><img src="http://windmissing.github.io%5Cimages%5C2019%5C42.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="线性回归算法"><a class="header" href="#线性回归算法">线性回归算法</a></h1>
<p>解决回归问题<br />
思想简单、实现容易<br />
许多强大的非线性模型的基础<br />
结果具有很好的可解释性<br />
蕴含机器学习中的很多重要思想</p>
<p><img src="http://windmissing.github.io/images/2019/43.png" alt="" /></p>
<p><strong>注意1：这里的二维平面图与分类问题的二维平面图不同</strong><br />
<strong>在分类问题中，横轴和纵轴都是样本特征，输出标记用颜色表示</strong><br />
<strong>在回归问题中，横轴是样本特征，纵轴是输出标记</strong></p>
<p><strong>注意2：</strong><br />
<strong>样本特征只有一个，称为简单线性回归</strong>
<strong>样本特征有多个，称为多元线性回归</strong></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="简单线性回归"><a class="header" href="#简单线性回归">简单线性回归</a></h1>
<p>找一条直线，这条直线最大程度地拟合所有的样本特征点
<img src="http://windmissing.github.io/images/2019/44.png" alt="" />
<img src="http://windmissing.github.io/images/2019/45.png" alt="" />
<img src="http://windmissing.github.io/images/2019/46.png" alt="" /></p>
<h1 id="一类机器学习算法的基本思路"><a class="header" href="#一类机器学习算法的基本思路">一类机器学习算法的基本思路</a></h1>
<p>通过分析问题，确定问题的损失函数(loss function)或效用函数(utility function), 有时也称为目标函数<br />
通过最优化损失函数或效用函数，获得机器学习的模型</p>
<p>近乎所有的参数学习算法都是这样的套路, e.g. 线性回归, 多项式回归, 逻辑回归, SVM, 神经网络, etc...本质都是在学习相应的参数来最优化目标函数, 区别在于模型不同从而建立的目标函数不同, 优化的方式也不尽相同.</p>
<p>最优化原理<br />
凸优化</p>
<h1 id="最小化本文中的损失函数"><a class="header" href="#最小化本文中的损失函数">最小化本文中的损失函数</a></h1>
<p>目标：找到$${a}$$和$${b}$$，使用$$\sum^m_{i=1}(y^{(i)}-ax^{(i)}-b)^2$$尽可能小。<br />
这是典型的最小二乘法问题：最小化误差的平方<br />
解得：</p>
<p>$$
a = \frac{\sum_{i=1}^m(x^{(i)}-\bar x)(y^{(i)}-\bar y)}{\sum_{i=1}^m(x^{(i)}-\bar x)^2}\quad \quad \quad \quad \quad b = \bar y - a\bar x 
$$</p>
<div style="break-before: page; page-break-before: always;"></div><p>5-1中的a和b的证明过程</p>
<p>这一节没有做笔记</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

x = np.array([1., 2., 3., 4., 5.])
y = np.array([1., 3., 2.

plt.scatter(x, y)
plt.axis([0, 6, 0, 6])
plt.show()
</code></pre>
<p>输出结果：
<img src="http://windmissing.github.io/images/2019/49.png" alt="" /></p>
<h1 id="在notebook中计算a-b"><a class="header" href="#在notebook中计算a-b">在Notebook中计算a, b</a></h1>
<p><img src="http://windmissing.github.io/images/2019/48.png" alt="" /></p>
<h2 id="计算a-b"><a class="header" href="#计算a-b">计算a, b</a></h2>
<pre><code class="language-python">x_mean = np.mean(x)
y_mean = np.mean(y)

num = 0.0
d = 0.0
for x_i, y_i in zip(x, y):
    num += (x_i - x_mean) * (y_i - y_mean)
    d += (x_i - x_mean) ** 2

a = num / d
b = y_mean - a * x_mean
</code></pre>
<h2 id="绘制结果"><a class="header" href="#绘制结果">绘制结果</a></h2>
<pre><code class="language-python">y_hat = a * x + b

plt.scatter(x, y)
plt.plot(x, y_hat, color='r')
plt.axis([0, 6, 0, 6])
plt.show()
</code></pre>
<p>输出结果：
<img src="http://windmissing.github.io/images/2019/50.png" alt="" /></p>
<h1 id="把上过程封装成类"><a class="header" href="#把上过程封装成类">把上过程封装成类</a></h1>
<pre><code class="language-python">import numpy as np

class SimpleLinearRegression1:
    def __init__(self):
        &quot;&quot;&quot;初始化Single Linear Regression模型&quot;&quot;&quot;
        self.a_ = None
        self.b_ = None

    def fit(self, x_train, y_train):
        &quot;&quot;&quot;根据训练数据集X_train, y_train训练Single Linear Regression模型&quot;&quot;&quot;
        assert x_train.ndim == 1, &quot;Simple Linear Regressor can only solve single feature training data&quot;
        assert len(x_train) == len(y_train), &quot;the size of x_train must be equal to the size of y_train&quot;

        x_mean = np.mean(x_train)
        y_mean = np.mean(y_train)

        num = 0.0
        d = 0.0
        for x_i, y_i in zip(x_train, y_train):
            num += (x_i - x_mean) * (y_i - y_mean)
            d += (x_i - x_mean) ** 2

        self.a_ = num / d
        self.b_ = y_mean - self.a_ * x_mean

    def predict(self, x_predict):
        &quot;&quot;&quot;给定待测数据集X_predict，返回表示x_predict的结果向量&quot;&quot;&quot;
        assert x_predict.ndim == 1, &quot;Simple Linear Regressor can only solve single feature training data&quot;
        assert self.a_ is not None and self.b_ is not None, &quot;must fit before predict&quot;
        return [self._predict(x) for x in x_predict]

    def _predict(self, x_single):
        &quot;&quot;&quot;给定单个待预测数据s_single，返回x_single的预测结果&quot;&quot;&quot;
        return self.a_ * x_single + self.b_

    def __repr__(self):
        return &quot;SimpleLinearRegression1()&quot;
</code></pre>
<h2 id="训练模型"><a class="header" href="#训练模型">训练模型</a></h2>
<pre><code class="language-python">reg1 = SimpleLinearRegression1()
reg1.fit(x, y)
</code></pre>
<h2 id="绘制结果-1"><a class="header" href="#绘制结果-1">绘制结果</a></h2>
<pre><code class="language-python">y_hat1 = a * x + b

plt.scatter(x, y)
plt.plot(x, y_hat1, color='r')
plt.axis([0, 6, 0, 6])
plt.show()
</code></pre>
<p>输出结果：
<img src="http://windmissing.github.io/images/2019/50.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p>5-3中计算a, b的实现方法性能较低，使用向量化运算能提高性能<br />
即把以下公式向量化：
<img src="http://windmissing.github.io/images/2019/48.png" alt="" /></p>
<p>向量化的依据：</p>
<p><img src="http://windmissing.github.io/images/2019/51.png" alt="" /></p>
<h1 id="向量化计算a-b"><a class="header" href="#向量化计算a-b">向量化计算a, b</a></h1>
<pre><code class="language-python">import numpy as np

class SimpleLinearRegression2:
    def __init__(self):
        &quot;&quot;&quot;初始化Single Linear Regression模型&quot;&quot;&quot;
        self.a_ = None
        self.b_ = None

    def fit(self, x_train, y_train):
        &quot;&quot;&quot;根据训练数据集X_train, y_train训练Single Linear Regression模型&quot;&quot;&quot;
        assert x_train.ndim == 1, &quot;Simple Linear Regressor can only solve single feature training data&quot;
        assert len(x_train) == len(y_train), &quot;the size of x_train must be equal to the size of y_train&quot;

        x_mean = np.mean(x_train)
        y_mean = np.mean(y_train)

        num = (x_train - x_mean).dot(y_train - y_mean)
        d = (x_train - x_mean).dot(x_train - x_mean)

        self.a_ = num / d
        self.b_ = y_mean - self.a_ * x_mean

    def predict(self, x_predict):
        &quot;&quot;&quot;给定待测数据集X_predict，返回表示x_predict的结果向量&quot;&quot;&quot;
        assert x_predict.ndim == 1, &quot;Simple Linear Regressor can only solve single feature training data&quot;
        assert self.a_ is not None and self.b_ is not None, &quot;must fit before predict&quot;
        return [self._predict(x) for x in x_predict]

    def _predict(self, x_single):
        &quot;&quot;&quot;给定单个待预测数据s_single，返回x_single的预测结果&quot;&quot;&quot;
        return self.a_ * x_single + self.b_

    def __repr__(self):
        return &quot;SimpleLinearRegression2()&quot;
</code></pre>
<h2 id="绘制结果-2"><a class="header" href="#绘制结果-2">绘制结果</a></h2>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

x = np.array([1., 2., 3., 4., 5.])
y = np.array([1., 3., 2., 3., 5.])

reg2 = SimpleLinearRegression2()
reg2.fit(x, y)

y_hat2 = reg2.predict(x)

plt.scatter(x, y)
plt.plot(x, y_hat2, color='r')
plt.axis([0, 6, 0, 6])
plt.show()
</code></pre>
<p>输出结果：
<img src="http://windmissing.github.io/images/2019/50.png" alt="" /></p>
<h1 id="向量化实现的性能测试"><a class="header" href="#向量化实现的性能测试">向量化实现的性能测试</a></h1>
<pre><code class="language-python">m = 1000000
big_x = np.random.random(size = m)
big_y = big_x * 3.0 + 2.0 + np.random.normal(size = m)

%timeit reg1.fit(big_x, big_y)    # reg1见5-4
%timeit reg2.fit(big_x, big_y)
</code></pre>
<p>输出结果：<br />
1.15 s ± 12.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)<br />
25.2 ms ± 2.08 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)</p>
<p>可见向量化计算能大幅度地提高性能，因此能用向量化计算的地方尽量用向量化计算</p>
<div style="break-before: page; page-break-before: always;"></div><p>分类问题使用accuracy来评价分类结果。<br />
回归问题怎样评价预测结果？</p>
<h1 id="mse-rmse-mae"><a class="header" href="#mse-rmse-mae">MSE RMSE MAE</a></h1>
<h2 id="均方误差-mse-mean-squared-error"><a class="header" href="#均方误差-mse-mean-squared-error">均方误差 MSE Mean Squared Error</a></h2>
<p><img src="http://windmissing.github.io/images/2019/52.png" alt="" /></p>
<p>问题：量纲</p>
<h2 id="均方根误差-rmse-root-mean-squared-error"><a class="header" href="#均方根误差-rmse-root-mean-squared-error">均方根误差 RMSE Root Mean Squared Error</a></h2>
<p><img src="http://windmissing.github.io/images/2019/53.png" alt="" />
与MSE本质上是一样的<br />
放大了最大的错误</p>
<h2 id="平均绝对误差-mae-mean-absolute-error"><a class="header" href="#平均绝对误差-mae-mean-absolute-error">平均绝对误差 MAE Mean Absolute Error</a></h2>
<p><img src="http://windmissing.github.io/images/2019/54.png" alt="" /></p>
<p>训练过程中，没有把这个函数定义成目标函数，是因为它不是处处可导。<br />
但它仍可以用于评价算法<br />
评价一个算法所使用的标准可以和训练时所用的标准不同</p>
<h1 id="编程实现三种"><a class="header" href="#编程实现三种">编程实现三种</a></h1>
<h2 id="波士顿房产数据"><a class="header" href="#波士顿房产数据">波士顿房产数据</a></h2>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()
x = boston.data[:, 5] # 5代码房间数，保使用房间数量这个特征
y = boston.target

plt.scatter(x, y)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/55.png" alt="" /></p>
<p>图中最上面有一排点比较奇怪，把它们去掉</p>
<pre><code class="language-python">x = x[y &lt; 50.0]
y = y[y &lt; 50.0]
plt.scatter(x, y)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/56.png" alt="" /></p>
<h2 id="训练模型预测结果"><a class="header" href="#训练模型预测结果">训练模型，预测结果</a></h2>
<pre><code class="language-python">from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=666)

reg = SimpleLinearRegression()  #见5-4
reg.fit(x_train, y_train)

plt.scatter(x_train, y_train)
plt.plot(x_train, reg.predict(x_train), color='r')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/57.png" alt="" /></p>
<h2 id="评价预测效果"><a class="header" href="#评价预测效果">评价预测效果</a></h2>
<pre><code class="language-python">y_predict = reg.predict(x_test)

mse_test = np.sum((y_predict - y_test) ** 2) / len(y_test)

from math import sqrt
rmse_test = sqrt(mse_test)

mae_test = np.sum(np.absolute(y_predict-y_test)) / len(y_test)
</code></pre>
<h2 id="使用scikit-learn中的mse和mae"><a class="header" href="#使用scikit-learn中的mse和mae">使用scikit-learn中的MSE和MAE</a></h2>
<pre><code class="language-python">from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

mean_squared_error(y_test, y_predict)
mean_absolute_error(y_test, y_predict)
</code></pre>
<p>scikit-learn中没有提供RMSE</p>
<h1 id="rmse-vs-mae"><a class="header" href="#rmse-vs-mae">RMSE vs MAE</a></h1>
<p>量纲相同<br />
RMSE比MSE大<br />
RMSE有放大y_hat与y较大差距的那个值的趋式<br />
让RMSE更小的意义更大</p>
<div style="break-before: page; page-break-before: always;"></div><p>分类问题使用accuracy来评价分类结果。<br />
1最好，0最差。<br />
即使分类的问题不同，也能很容易的比较它们之间的优劣。</p>
<p>但RMSE和MAE无些特性。<br />
解决方法：R Squared</p>
<h1 id="r-squared"><a class="header" href="#r-squared">R Squared</a></h1>
<p>$$
R^2 = 1 - \frac {SS_{residual}}{SS_{total}} = 1 - \frac{\sum_i(\hat y^{(i)}- y^{(i)})^2}{\sum_i(\bar y- y^{(i)})^2}
$$</p>
<p>说明：</p>
<p>$$SS_{residual}$$: 使用模型预测产生的错误<br />
$$SS_{total}$$: 把所有样本都预测为$$\bar y$$产生的错误<br />
$$\bar y$$：y的平均值<br />
$$\hat y^{(i)}$$：第i个样本的模型预测值</p>
<p>R Squared代表我们的模型拟合住的数据</p>
<p>R Squared的性质：</p>
<ul>
<li>R Squared &lt;= 1</li>
<li>R Squared越大越好。当我们的预测模型不犯任何错误时，R Squared达到最大值1</li>
<li>当我们的模型等于基准模型时，R Squared = 0</li>
<li>如果R Squared &lt; 0，说明我们的模型还不如基准模型。此时很有可能数据不存在任何线性关系。</li>
</ul>
<p>R Squared也可以写成这种形式：
<img src="http://windmissing.github.io/images/2019/60.png" alt="" />
其中Var(y)代表方差</p>
<h1 id="编程实现r-squared"><a class="header" href="#编程实现r-squared">编程实现R Squared</a></h1>
<p>继续使用5-4中的数据和训练结果</p>
<pre><code class="language-python">1 - mean_squared_error(y_test, y_predict)/np.var(y_test)
</code></pre>
<p>或</p>
<pre><code class="language-python">from sklearn.metrics import r2_score
r2_score(y_test, y_predict)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/61.png" alt="" /></p>
<p>化简结果：<br />
构造矩阵Xb
<img src="http://windmissing.github.io/images/2019/62.png" alt="" />
要使目标函数最小，必须满足
<img src="http://windmissing.github.io/images/2019/63.png" alt="" /></p>
<p>直接使用这个公式求解，<br />
缺点：时间复杂度太高，O(n^3)<br />
优点：不需要对数据进行规一化</p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/64.png" alt="" /></p>
<h1 id="linear-regression"><a class="header" href="#linear-regression">Linear Regression</a></h1>
<pre><code class="language-python">import numpy as np
from sklearn.metrics import r2_score

class LinearRegression:

    def __init__(self):
        &quot;&quot;&quot;初始化Linear Regression模型&quot;&quot;&quot;
        self.coef_ = None
        self.interception_ = None
        self._theta = None

    def fit_normal(self, X_train, y_train):
        &quot;&quot;&quot;根据训练数据集X_train, y_train训练Linear Regression模型&quot;&quot;&quot;
        assert X_train.shape[0] == y_train.shape[0], &quot;the size of X_train must be equal to the size of y_train&quot;

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)

        self.interception_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self

    def predict(self, X_predict):
        &quot;&quot;&quot;给定待预测数据集X_predict，返回表示X_predict的结果向量&quot;&quot;&quot;
        assert self.interception_ is not None and self.coef_ is not None, &quot;must fit before predict&quot;
        assert X_predict.shape[1] == len(self.coef_), &quot;the feature number of X_predict must equal to X_train&quot;

        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
        return X_b.dot(self._theta)

    def score(self, X_test, y_test):
        &quot;&quot;&quot;根据测试数据集X_test, y_test确定当前模型的准确度&quot;&quot;&quot;

        y_predict = self.predict(X_test)
        return r2_score(y_test, y_predict)

    def __repr__(self):
        return &quot;LinearRegression()&quot;
</code></pre>
<h1 id="boston-data"><a class="header" href="#boston-data">boston data</a></h1>
<pre><code class="language-python">import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()
x = boston.data
y = boston.target
x = x[y &lt; 50.0]
y = y[y &lt; 50.0]
</code></pre>
<h1 id="训练模型与预测结果"><a class="header" href="#训练模型与预测结果">训练模型与预测结果</a></h1>
<pre><code class="language-python">from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=666)

reg = LinearRegression()
reg.fit_normal(X_train, y_train)
reg.score(X_test, y_test)
</code></pre>
<p>输出结果：<br />
0.8129794056212832</p>
<p>使用多个特征训练的模型得分要高于使用单个特征训练的模型</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-python">import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()
x = boston.data
y = boston.target
x = x[y &lt; 50.0]
y = y[y &lt; 50.0]
</code></pre>
<h1 id="linear-regression-1"><a class="header" href="#linear-regression-1">Linear Regression</a></h1>
<pre><code class="language-python">from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=666)

from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
lin_reg.score(X_test, y_test)
</code></pre>
<h1 id="knn-regressor"><a class="header" href="#knn-regressor">KNN Regressor</a></h1>
<h2 id="默认算法"><a class="header" href="#默认算法">默认算法</a></h2>
<pre><code class="language-python">from sklearn.neighbors import KNeighborsRegressor

knn_reg = KNeighborsRegressor()
knn_reg.fit(X_train, y_train)
knn_reg.score(X_test, y_test)
</code></pre>
<h2 id="网络搜索"><a class="header" href="#网络搜索">网络搜索</a></h2>
<pre><code class="language-python">from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import GridSearchCV

param_grid = [
    {
        'weights':['uniform'],
        'n_neighbors': [i for i in range(1, 11)]
    },
    {   
        'weights':['distance'],
        'n_neighbors': [i for i in range(1, 11)],
        'p': [i for i in range(1, 6)]
    }
]

knn_reg = KNeighborsRegressor()
grid_search = GridSearchCV(knn_reg, param_grid, n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)
</code></pre>
<p>输入：<code>grid_search.best_params_</code><br />
输出：{'n_neighbors': 5, 'p': 1, 'weights': 'distance'}</p>
<p>输入：<code>grid_search.best_score_</code><br />
输出：0.6340477954176972</p>
<p>输入：<code>grid_search.best_estimator_.score(X_test, y_test)</code><br />
输出：0.7044357727037996</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-python">import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()
X = boston.data
y = boston.target
X = X[y &lt; 50.0]
y = y[y &lt; 50.0]

from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X, y)

lin_reg.coef_
</code></pre>
<p>输出结果：
<img src="http://windmissing.github.io/images/2019/65.png" alt="" /></p>
<p>怎么解释这些数字？</p>
<ul>
<li>系数的正负代表这个特征与房价是正相关还是负相关</li>
<li>系数的绝对值大小代码这个特征对房价的影响程度<br />
输入：<code>boston.feature_names[np.argsort(lin_reg.coef_)]</code><br />
输出：</li>
</ul>
<pre><code>array(['NOX', 'DIS', 'PTRATIO', 'LSTAT', 'CRIM', 'INDUS', 'AGE', 'TAX',
       'B', 'ZN', 'RAD', 'CHAS', 'RM'], dtype='&lt;U7')
</code></pre>
<p>即使使用线性回归法预测的模型不够好，观察的它的系数对分析问题也是有帮助的。</p>
<h1 id="线性回归算法的总结"><a class="header" href="#线性回归算法的总结">线性回归算法的总结</a></h1>
<p><img src="http://windmissing.github.io/images/2019/65.png" alt="" /></p>
<p>| 线性回归算法  | KNN算法
--|---|--
模型参数  | 典型的参数学习  | 非参数学习
分类问题  | 是很多分类算法的基础  | 可以解决分类问题
回归问题  | 只能解决回归问题  | 可以解决回归问题
对数据的假设性  | 有  |  没有
对数据的解释性  | 有  |  没有
时间复杂度  | 使用正规方程解，在训练模型时复杂度高。<br>解决方法：梯度下降下  | 预测时复杂度高 </p>
<div style="break-before: page; page-break-before: always;"></div><p>不是一个机器学习算法<br />
是一种基于搜索的最优化算法<br />
作用：最小化一个损失函数</p>
<p>这个二维平面与前面提到的所有二维平面不同。注意坐标系是什么。</p>
<p>在直线方程中，导数代表斜率<br />
在曲线方程中，导数代表切线斜率<br />
在梯度下降法中，导数代表theta单位变化时，J相应的变化<br />
导数可以代表方向，对应J增大的方向<br />
在梯度下降法中，theta应该向导数的负方向移动<br />
在多维函数中，要对各个方向的分量分别求导，最终得到的方向就是梯度。<br />
<img src="http://windmissing.github.io/images/2019/67.png" alt="" /><br />
<img src="http://windmissing.github.io/images/2019/68.png" alt="" /><br />
<img src="http://windmissing.github.io/images/2019/69.png" alt="" /><br />
<img src="http://windmissing.github.io/images/2019/70.png" alt="" /></p>
<p><strong>注意：并不是所有函数都有唯一的极值点</strong></p>
<p><img src="http://windmissing.github.io/images/2019/71.png" alt="" /></p>
<p>解决方法：<br />
多次运行，随机化初始化点<br />
梯度下降法的初始点也是一个超参数</p>
<p>线性回归问题，损失函数J具有唯一最优解</p>
<div style="break-before: page; page-break-before: always;"></div><p>不是一个机器学习算法<br />
是一种基于搜索的最优化算法<br />
作用：最小化一个损失函数</p>
<p>这个二维平面与前面提到的所有二维平面不同。注意坐标系是什么。</p>
<p>在直线方程中，导数代表斜率<br />
在曲线方程中，导数代表切线斜率<br />
在梯度下降法中，导数代表theta单位变化时，J相应的变化<br />
导数可以代表方向，对应J增大的方向<br />
在梯度下降法中，theta应该向导数的负方向移动<br />
在多维函数中，要对各个方向的分量分别求导，最终得到的方向就是梯度。<br />
<img src="http://windmissing.github.io/images/2019/67.png" alt="" /><br />
<img src="http://windmissing.github.io/images/2019/68.png" alt="" /><br />
<img src="http://windmissing.github.io/images/2019/69.png" alt="" /><br />
<img src="http://windmissing.github.io/images/2019/70.png" alt="" /></p>
<p><strong>注意：并不是所有函数都有唯一的极值点</strong></p>
<p><img src="http://windmissing.github.io/images/2019/71.png" alt="" /></p>
<p>解决方法：<br />
多次运行，随机化初始化点<br />
梯度下降法的初始点也是一个超参数</p>
<p>线性回归问题，损失函数J具有唯一最优解</p>
<div style="break-before: page; page-break-before: always;"></div><p>假设损失函数J是这样的：</p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

plot_x = np.linspace(-1, 6, 141)
plot_y = (plot_x - 2.5) ** 2 -1
plt.plot(plot_x, plot_y)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/72.png" alt="" /></p>
<h1 id="梯度下降算法实现"><a class="header" href="#梯度下降算法实现">梯度下降算法实现</a></h1>
<pre><code class="language-python">def dJ(theta):
    return 2*(theta - 2.5)

def J(theta):
    return (theta-2.5) ** 2 -1

eta = 0.1
epsilon = 1e-8

theta = 0
while True:
    gradient = dJ(theta)
    last_theta = theta
    theta = theta - eta * gradient
    if (abs(J(theta) - J(last_theta)) &lt; epsilon):
        break

print (theta)
print (J(theta))
</code></pre>
<p>输出结果：<br />
2.499891109642585<br />
-0.99999998814289</p>
<h1 id="在图是显示所有的theta的轨迹"><a class="header" href="#在图是显示所有的theta的轨迹">在图是显示所有的theta的轨迹</a></h1>
<pre><code class="language-python">def gradient_descent(initial_theta, eta, epsilon=1e-8):
    theta = initial_theta
    theta_history = [theta]
    while True:
        gradient = dJ(theta)
        last_theta = theta
        theta = theta - eta * gradient
        theta_history.append(theta)
        #print (theta)
        if (abs(J(theta) - J(last_theta)) &lt; epsilon):
            break
    return theta_history

def plot_theta_history():
    plt.plot(plot_x, J(plot_x))
    plt.plot(np.array(theta_history), J(np.array(theta_history)), color='r')
    plt.show()

eta = 0.01
theta_history = gradient_descent(0., eta)
plot_theta_history()
</code></pre>
<p>输出结果：
<img src="http://windmissing.github.io/images/2019/73.png" alt="" /></p>
<h1 id="eta取不同参数的绘制结果"><a class="header" href="#eta取不同参数的绘制结果">eta取不同参数的绘制结果</a></h1>
<pre><code class="language-python">def gradient_descent(initial_theta, eta, n_iters = 10, epsilon=1e-8):
    theta = initial_theta
    theta_history = [theta]
    i_iter = 0
    while i_iter &lt; n_iters:
        gradient = dJ(theta)
        last_theta = theta
        theta = theta - eta * gradient
        theta_history.append(theta)
        print (theta)
        if (abs(J(theta) - J(last_theta)) &lt; epsilon):
            break
        i_iter += 1
    return theta_history
</code></pre>
<p>输出结果：
<img src="http://windmissing.github.io/images/2019/74.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/75.png" alt="" />
<img src="http://windmissing.github.io/images/2019/76.png" alt="" /></p>
<p><img src="http://windmissing.github.io/images/2019/79.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
x = 2 * np.random.random(size=100)
y = x * 3. + 4. + np.random.normal(size=100)

X = x.reshape(-1, 1)

plt.scatter(x, y)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/80.png" alt="" /></p>
<h1 id="使用梯度下降法训练"><a class="header" href="#使用梯度下降法训练">使用梯度下降法训练</a></h1>
<pre><code class="language-python">def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta))**2) / len(X_b)
    except:
        return float('inf')

def dJ(theta, X_b, y):
    ret = np.empty(len(theta))
    ret[0] = np.sum(X_b.dot(theta) - y)
    for i in range(1, len(theta)):
        ret[i] = (X_b.dot(theta) - y).dot(X_b[:, 1])

    return ret * 2 / len(X_b)

def gradient_descent(X_b, y, initial_theta, eta, n_iters = 1e4, epsilon=1e-8):
    theta = initial_theta
    i_iter = 0
    while i_iter &lt; n_iters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon):
            break
        i_iter += 1
    return theta

X_b = np.hstack([np.ones((len(x), 1)), X])
initial_theta = np.zeros(X_b.shape[1])
eta = 0.01

theta = gradient_descent(X_b, y, initial_theta, eta)
theta
</code></pre>
<p>输出结果：<br />
array([4.02145786, 3.00706277])</p>
<h1 id="封装线性回归算法"><a class="header" href="#封装线性回归算法">封装线性回归算法</a></h1>
<pre><code class="language-python">import numpy as np
from sklearn.metrics import r2_score

class LinearRegression:

    def __init__(self):
        &quot;&quot;&quot;初始化Linear Regression模型&quot;&quot;&quot;
        self.coef_ = None
        self.interception_ = None
        self._theta = None

    def fit_normal(self, X_train, y_train):
        &quot;&quot;&quot;根据训练数据集X_train, y_train训练Linear Regression模型&quot;&quot;&quot;
        assert X_train.shape[0] == y_train.shape[0], &quot;the size of X_train must be equal to the size of y_train&quot;

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)

        self.interception_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self

    def fit_gd(self, X_train, y_train, eta=0.01, n_iters = 1e4):
        &quot;&quot;&quot;根据训练数据集X_train, y_train，使用梯度下降法训练Linear Regression模型&quot;&quot;&quot;
        assert X_train.shape[0] == y_train.shape[0], &quot;the size of X_train must be equal to the size of y_train&quot;

        def J(theta, X_b, y):
            try:
                return np.sum((y - X_b.dot(theta))**2) / len(X_b)
            except:
                return float('inf')

        def dJ(theta, X_b, y):
            ret = np.empty(len(theta))
            ret[0] = np.sum(X_b.dot(theta) - y)
            for i in range(1, len(theta)):
                ret[i] = (X_b.dot(theta) - y).dot(X_b[:, 1])

            return ret * 2 / len(X_b)

        def gradient_descent(X_b, y, initial_theta, eta, n_iters = 1e4, epsilon=1e-8):
            theta = initial_theta
            i_iter = 0
            while i_iter &lt; n_iters:
                gradient = dJ(theta, X_b, y)
                last_theta = theta
                theta = theta - eta * gradient
                if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon):
                    break
                i_iter += 1
            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])
        self._theta = gradient_descent(X_b, y_train, initial_theta, eta)
        self.interception_ = self._theta[0]
        self.coef_ = self._theta[1:]
        return self

    def predict(self, X_predict):
        &quot;&quot;&quot;给定待预测数据集X_predict，返回表示X_predict的结果向量&quot;&quot;&quot;
        assert self.interception_ is not None and self.coef_ is not None, &quot;must fit before predict&quot;
        assert X_predict.shape[1] == len(self.coef_), &quot;the feature number of X_predict must equal to X_train&quot;

        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
        return X_b.dot(self._theta)

    def score(self, X_test, y_test):
        &quot;&quot;&quot;根据测试数据集X_test, y_test确定当前模型的准确度&quot;&quot;&quot;

        y_predict = self.predict(X_test)
        return r2_score(y_test, y_predict)

    def __repr__(self):
        return &quot;LinearRegression()&quot;
</code></pre>
<h1 id="使用fit_gd"><a class="header" href="#使用fit_gd">使用fit_gd</a></h1>
<pre><code class="language-python">lin_reg = LinearRegression()
lin_reg.fit_gd(X, y)
</code></pre>
<p>输入：<code>lin_reg.coef_</code><br />
输出：array([3.00706277])</p>
<p>输入：<code>lin_reg.interception_</code><br />
输出：4.021457858204859</p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/81.png" alt="" /></p>
<h1 id="测试数据"><a class="header" href="#测试数据">测试数据</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
x = 2 * np.random.random(size=100)
y = x * 3. + 4. + np.random.normal(size=100)

X = x.reshape(-1, 1)

plt.scatter(x, y)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/80.png" alt="" /></p>
<h1 id="向量化计算dj"><a class="header" href="#向量化计算dj">向量化计算dJ</a></h1>
<pre><code class="language-python">        def dJ(theta, X_b, y):
            return X_b.T.dot(X_b.dot(theta)-y) * 2. / len(X_b)
</code></pre>
<h1 id="使用真实数据测试模型"><a class="header" href="#使用真实数据测试模型">使用真实数据测试模型</a></h1>
<h2 id="真实数据--正规化方程解"><a class="header" href="#真实数据--正规化方程解">真实数据 + 正规化方程解</a></h2>
<pre><code class="language-python">from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=666)

lin_reg1 = LinearRegression()
lin_reg1.fit_normal(X_train, y_train)  # 见5-4
lin_reg1.score(X_test, y_test)
</code></pre>
<p>输出结果：<br />
0.8129794056212832</p>
<h2 id="真实数据--梯度下降法"><a class="header" href="#真实数据--梯度下降法">真实数据 + 梯度下降法</a></h2>
<pre><code class="language-python">lin_reg2 = LinearRegression()
lin_reg2.fit_gd(X_train, y_train)
lin_reg2.score(X_test, y_test)
</code></pre>
<p>输出结果：<br />
eta太大导致搜索过程不收敛</p>
<h2 id="真实数据--梯度下降法--eta0000001"><a class="header" href="#真实数据--梯度下降法--eta0000001">真实数据 + 梯度下降法 + eta=0.000001</a></h2>
<pre><code class="language-python">lin_reg2 = LinearRegression()
lin_reg2.fit_gd(X_train, y_train, eta = 0.000001)
lin_reg2.score(X_test, y_test)
</code></pre>
<p>输出结果：<br />
0.27586818724477247</p>
<p>训练次数不够，没有达到最优值</p>
<h2 id="真实数据--梯度下降法--eta0000001--n_iters1e6"><a class="header" href="#真实数据--梯度下降法--eta0000001--n_iters1e6">真实数据 + 梯度下降法 + eta=0.000001 + n_iters=1e6</a></h2>
<pre><code class="language-python">lin_reg2 = LinearRegression()
lin_reg2.fit_gd(X_train, y_train, eta = 0.000001, n_iters=1e6)
lin_reg2.score(X_test, y_test)
</code></pre>
<p>输出结果：<br />
0.7542932581943915</p>
<p>训练次数太多，导致训练时间太长，但次数又不足以找到最优解<br />
解决方法：数据归一化</p>
<h1 id="梯度下降法与数据归一化"><a class="header" href="#梯度下降法与数据归一化">梯度下降法与数据归一化</a></h1>
<p>多元线性回归问题中，不同特征的规格一样，导致eta很难选。同一个eta可能会导致某些无法收敛而另一特征又收敛太慢。<br />
因此使用梯度下降法之前，最好对数据进行归一化</p>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler

standardScaler = StandardScaler()
standardScaler.fit(X_train)

X_train_standard = standardScaler.transform(X_train)
X_test_standard = standardScaler.transform(X_test)

lin_reg3 = LinearRegression()
lin_reg3.fit_gd(X_train_standard, y_train)
lin_reg3.score(X_test_standard, y_test)
</code></pre>
<p>输出结果：<br />
0.8129873310487505</p>
<div style="break-before: page; page-break-before: always;"></div><p>常规的梯度下降法，又叫批量梯度下降法，Batch Gradient Descent
<img src="http://windmissing.github.io/images/2019/81.png" alt="" /></p>
<p>问题：当样本数m很大时会非常耗时</p>
<p>解决方法：每次只对其中一个样本做计算<br />
<img src="http://windmissing.github.io/images/2019/82.png" alt="" /></p>
<p>把去掉m后计算的公式作为搜索的方向。<br />
由于不能保证这种方法计算得到的方向一定是损失最小的方向，甚至不能保证一定是损失函数减小的方向。也不能找到最小值的位置。<br />
但仍然能到函数的最小值附近。<br />
如果m非常大，可以用一定的精度来换时间。</p>
<p><img src="http://windmissing.github.io/images/2019/82.png" alt="" /></p>
<p>在随机梯度下降法过程中，学习率很重要。<br />
如果学习率取固定值，很有可以到了最小值附近后又跳出去了。<br />
学习率应逐渐递减。（模拟退火的思想）<br />
<img src="http://windmissing.github.io/images/2019/84.png" alt="" /><br />
通常a取5，b取50</p>
<h1 id="测试数据-1"><a class="header" href="#测试数据-1">测试数据</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

m = 100000

x = np.random.normal(size=m)
X = x.reshape(-1, 1)
y = 4. * x + 3. + np.random.normal(0, 3, size=m)
</code></pre>
<h1 id="批量梯度下降法"><a class="header" href="#批量梯度下降法">批量梯度下降法</a></h1>
<h2 id="算法"><a class="header" href="#算法">算法</a></h2>
<pre><code class="language-python">def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta))**2) / len(X_b)
    except:
        return float('inf')

def dJ(theta, X_b, y):
    return X_b.T.dot(X_b.dot(theta)-y) * 2. / len(X_b)

def gradient_descent(X_b, y, initial_theta, eta, n_iters = 1e4, epsilon=1e-8):
    theta = initial_theta
    i_iter = 0
    while i_iter &lt; n_iters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon):
            break
        i_iter += 1
    return theta
</code></pre>
<h2 id="测试性能"><a class="header" href="#测试性能">测试性能</a></h2>
<pre><code>%%time
X_b = np.hstack([np.ones((len(X), 1)), X])
initial_theta = np.zeros(X_b.shape[1])
eta = 0.01
theta = gradient_descent(X_b, y, initial_theta, eta)
</code></pre>
<p>耗时：2s<br />
theta = array([3.00456203, 3.98777265])</p>
<h1 id="随机梯度下降法"><a class="header" href="#随机梯度下降法">随机梯度下降法</a></h1>
<h2 id="算法-1"><a class="header" href="#算法-1">算法</a></h2>
<pre><code class="language-python">def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta))**2) / len(X_b)
    except:
        return float('inf')

def dJ_sgd(theta, X_b_i, y_i):
    return X_b_i.T.dot(X_b_i.dot(theta)-y_i) * 2.

def sgd(X_b, y, initial_theta, n_iters):
    t0 = 5
    t1 = 50

    def learning_rate(t):
        return t0 / (t + t1)

    theta = initial_theta
    i_iter = 0
    for i_iter in range (n_iters):
        rand_i = np.random.randint(len(X_b))
        gradient = dJ_sgd(theta, X_b[rand_i], y[rand_i])
        last_theta = theta
        theta = theta - learning_rate(i_iter) * gradient
        # 不能保证梯度一直是减小的
#         if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon):
#             break
    return theta
</code></pre>
<h2 id="测试性能-1"><a class="header" href="#测试性能-1">测试性能</a></h2>
<pre><code class="language-python">%%time
X_b = np.hstack([np.ones((len(X), 1)), X])
initial_theta = np.zeros(X_b.shape[1])
theta = sgd(X_b, y, initial_theta, n_iters=len(X_b)//3)   # 这里只检查了1/3样本，对于多元线性回归问题不能这样
</code></pre>
<p>耗时：471ms<br />
array([2.94954458, 3.95898273])</p>
<p>时间大幅度减少而结果和批量梯度下降法差不多。<br />
当m特别大时，可以牺牲一定的精度来换取时间。</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-python">import numpy as np
from sklearn.metrics import r2_score

class LinearRegression:

    def __init__(self):
        &quot;&quot;&quot;初始化Linear Regression模型&quot;&quot;&quot;
        self.coef_ = None
        self.interception_ = None
        self._theta = None

    def fit_normal(self, X_train, y_train):
        &quot;&quot;&quot;根据训练数据集X_train, y_train训练Linear Regression模型&quot;&quot;&quot;
        assert X_train.shape[0] == y_train.shape[0], &quot;the size of X_train must be equal to the size of y_train&quot;

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)

        self.interception_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self

    def fit_gd(self, X_train, y_train, eta=0.01, n_iters = 1e4):
        &quot;&quot;&quot;根据训练数据集X_train, y_train，使用梯度下降法训练Linear Regression模型&quot;&quot;&quot;
        assert X_train.shape[0] == y_train.shape[0], &quot;the size of X_train must be equal to the size of y_train&quot;

        def J(theta, X_b, y):
            try:
                return np.sum((y - X_b.dot(theta))**2) / len(X_b)
            except:
                return float('inf')

        def dJ(theta, X_b, y):
            return X_b.T.dot(X_b.dot(theta)-y) * 2. / len(X_b)

        def gradient_descent(X_b, y, initial_theta, eta, n_iters = 1e4, epsilon=1e-8):
            theta = initial_theta
            i_iter = 0
            while i_iter &lt; n_iters:
                gradient = dJ(theta, X_b, y)
                last_theta = theta
                theta = theta - eta * gradient
                if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon):
                    break
                i_iter += 1
            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])
        self._theta = gradient_descent(X_b, y_train, initial_theta, eta)
        self.interception_ = self._theta[0]
        self.coef_ = self._theta[1:]
        return self

    def fit_sgd(self, X_train, y_train, n_iters = 5, t0 = 5, t1 = 50):
        &quot;&quot;&quot;根据训练数据集X_train, y_train，使用随机梯度下降法训练Linear Regression模型&quot;&quot;&quot;
        assert X_train.shape[0] == y_train.shape[0], &quot;the size of X_train must be equal to the size of y_train&quot;

        def dJ_sgd(theta, X_b_i, y_i):
            return X_b_i.T.dot(X_b_i.dot(theta)-y_i) * 2.

        def learning_rate(t, t0, t1):
            return t0 / (t + t1)

        def sgd(X_b, y, initial_theta, n_iters, t0, t1): # n_iters:对所有的样本看几圈
            theta = initial_theta
            m = len(X_b)

            i_iter = 0
            for i_iter in range (n_iters):
                indexes = np.random.permutation(m)
                X_b_new = X_b[indexes]
                y_new = y[indexes]
                for i in range (m):
                    gradient = dJ_sgd(theta, X_b_new[i], y_new[i])
                    last_theta = theta
                    theta = theta - learning_rate(i_iter*m+i, t0, t1) * gradient
            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])
        self._theta = sgd(X_b, y_train, initial_theta, n_iters, t0, t1)
        self.interception_ = self._theta[0]
        self.coef_ = self._theta[1:]
        return self

    def predict(self, X_predict):
        &quot;&quot;&quot;给定待预测数据集X_predict，返回表示X_predict的结果向量&quot;&quot;&quot;
        assert self.interception_ is not None and self.coef_ is not None, &quot;must fit before predict&quot;
        assert X_predict.shape[1] == len(self.coef_), &quot;the feature number of X_predict must equal to X_train&quot;

        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
        return X_b.dot(self._theta)

    def score(self, X_test, y_test):
        &quot;&quot;&quot;根据测试数据集X_test, y_test确定当前模型的准确度&quot;&quot;&quot;

        y_predict = self.predict(X_test)
        return r2_score(y_test, y_predict)

    def __repr__(self):
        return &quot;LinearRegression()&quot;
</code></pre>
<h1 id="测试数据--sgd"><a class="header" href="#测试数据--sgd">测试数据 + sgd</a></h1>
<pre><code class="language-python">m = 100000

x = np.random.normal(size=m)
X = x.reshape(-1, 1)
y = 4. * x + 3. + np.random.normal(0, 3, size=m)

lin_reg = LinearRegression()
lin_reg.fit_sgd(X, y, n_iters=2)
</code></pre>
<p>刚开始在代码中犯了个错误，没有把L78的i_iter改成<code>i_iter*m+i</code>，
导致每次训练得到的模型差点都非常大，且偏离正确值也非常大。<br />
改掉之后就好了，<br />
可以如果学习率使用固定值，不能得到很好的效果。</p>
<h1 id="真实数据--sgd"><a class="header" href="#真实数据--sgd">真实数据 + sgd</a></h1>
<h2 id="真实数据"><a class="header" href="#真实数据">真实数据</a></h2>
<pre><code class="language-python">from sklearn import datasets

boston = datasets.load_boston()
X = boston.data
y = boston.target

X = X[y&lt;50.0]
y = y[y&lt;50.0]
</code></pre>
<h2 id="预处理"><a class="header" href="#预处理">预处理</a></h2>
<pre><code class="language-python">from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)

from sklearn.preprocessing import StandardScaler
standScaler = StandardScaler()
standScaler.fit(X_train)
X_train_standard = standScaler.transform(X_train)
X_test_standard = standScaler.transform(X_test)
</code></pre>
<h2 id="sgd"><a class="header" href="#sgd">SGD</a></h2>
<pre><code class="language-python">lin_reg = LinearRegression()
%time lin_reg.fit_sgd(X_train_standard, y_train)
lin_reg.score(X_test_standard, y_test)
</code></pre>
<h2 id="n_iters对score和wall-time的影响"><a class="header" href="#n_iters对score和wall-time的影响">n_iters对score和Wall time的影响</a></h2>
<table><thead><tr><th>n_iters</th><th>score</th><th>Wall time</th></tr></thead><tbody>
<tr><td>5</td><td>0.7763594773981595</td><td>30.7 ms</td></tr>
<tr><td>50</td><td>0.8130771495096732</td><td>271 ms</td></tr>
<tr><td>100</td><td>0.8131205440883096</td><td>462 ms</td></tr>
</tbody></table>
<h1 id="真实数据--sklearn的sgd"><a class="header" href="#真实数据--sklearn的sgd">真实数据 + sklearn的SGD</a></h1>
<pre><code class="language-python">from sklearn.linear_model import SGDRegressor

sgd_lin = SGDRegressor()
%time sgd_lin.fit(X_train_standard, y_train)
sgd_lin.score(X_test_standard, y_test)
</code></pre>
<p>模型的得分差不多，但sklearn的SGD的速度明显快很多。<br />
因为sklearn的SGD的实现过程与课程中有很大的不同。</p>
<p>视频还测试了SGDRegressor的n_iter参数。<br />
这个参数在我用的sklearn版本中已经没有了。</p>
<div style="break-before: page; page-break-before: always;"></div><p>梯度的求解并不容易，如果知道自己求的梯度是否正确？
<img src="http://windmissing.github.io/images/2019/85.png" alt="" /></p>
<p>图中两条直线的斜率近乎相等。<br />
两个蓝点越接近，斜率越相近。<br />
<img src="http://windmissing.github.io/images/2019/85.png" alt="" /></p>
<h1 id="推广到高维场景"><a class="header" href="#推广到高维场景">推广到高维场景</a></h1>
<p><img src="http://windmissing.github.io/images/2019/87.png" alt="" /></p>
<h1 id="代码实现"><a class="header" href="#代码实现">代码实现</a></h1>
<h2 id="准备数据-3"><a class="header" href="#准备数据-3">准备数据</a></h2>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
X = np.random.random(size=(1000, 10))
true_theta = np.arange(1, 12, dtype=float)
X_b = np.hstack([np.ones((len(X), 1)), X])
y = X_b.dot(true_theta) + np.random.normal(size=1000)

def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta))**2) / len(X_b)
    except:
        return float('inf')
</code></pre>
<h2 id="两种方法求导数"><a class="header" href="#两种方法求导数">两种方法求导数</a></h2>
<pre><code class="language-python">def dJ_math(theta, X_b, y):
    return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(y)

def dJ_debug(theta, X_b, y, epsilon=0.01):
    ret = np.empty(len(theta))
    for i in range(len(theta)):
        theta_1 = theta.copy()
        theta_1[i] +=epsilon
        theta_2 = theta.copy()
        theta_2[i] -=epsilon
        ret[i] = (J(theta_1, X_b, y) - J(theta_2, X_b, y)) / (2*epsilon)
    return ret
</code></pre>
<h2 id="两种方法训练模型"><a class="header" href="#两种方法训练模型">两种方法训练模型</a></h2>
<pre><code class="language-python">def gradient_descent(dJ, X_b, y, initial_theta, eta, n_iters = 1e4, epsilon=1e-8):
    theta = initial_theta
    i_iter = 0
    while i_iter &lt; n_iters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon):
            break
        i_iter += 1
    return theta

X_b = np.hstack([np.ones((len(X), 1)), X])
initial_theta = np.zeros(X_b.shape[1])
eta = 0.01

%time theta = gradient_descent(dJ_debug, X_b, y, initial_theta, eta)
%time theta = gradient_descent(dJ_math, X_b, y, initial_theta, eta)
</code></pre>
<h2 id="实验结果"><a class="header" href="#实验结果">实验结果</a></h2>
<p>dJ_math对应的运行时间为：660 ms<br />
dJ_debug对应的运行时间为：4.58s<br />
两种方法求出的theta完全相同</p>
<p>dJ_debug的方法可以用于求梯度，最终能得到正确的结果<br />
dJ_debug速度很慢。<br />
可以先使用dJ_debug得到想要的正确结果。<br />
再用dJ_math将得到的结果与dJ_debug的结果相比较，来验证数学是否正确。<br />
dJ_debug与J无关，可以适用于所有函数。</p>
<div style="break-before: page; page-break-before: always;"></div><ul>
<li>批量梯度下降法 Batch Gradient Descent，缺点：慢，优点：稳定</li>
<li>随机梯度下降法 Stochastic Gradient Descent，优点：快，缺点：不稳定</li>
<li>小批量梯度下降法 Mini-Batch Gradient Descent，以上两种结合</li>
</ul>
<h1 id="随机"><a class="header" href="#随机">随机</a></h1>
<p>随机梯度下降法可以跳出局部最优解<br />
随机梯度下降法运行速度更快<br />
机器学习领域很多算法都要使用随机的特点，例如随机搜索、随机森林</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="主成分分析法-pca-principal-component-analysis"><a class="header" href="#主成分分析法-pca-principal-component-analysis">主成分分析法 PCA Principal Component Analysis</a></h1>
<p>一个非监督的机器学习算法<br />
主要用于数据的降维<br />
通过降维，可以发现更便于人类理解的特征<br />
其他应用：可视化，去噪</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="主成分分析法-pca-principal-component-analysis-1"><a class="header" href="#主成分分析法-pca-principal-component-analysis-1">主成分分析法 PCA Principal Component Analysis</a></h1>
<p>一个非监督的机器学习算法<br />
主要用于数据的降维<br />
通过降维，可以发现更便于人类理解的特征<br />
其他应用：可视化，去噪</p>
<h1 id="二维降至一维"><a class="header" href="#二维降至一维">二维降至一维</a></h1>
<p><img src="http://windmissing.github.io/images/2019/88.png" alt="" /><br />
如何把图中的二维空间中的点降至一维？<br />
<img src="http://windmissing.github.io/images/2019/89.png" alt="" /><br />
以上三种方法哪种更好？<br />
第三种，因为第三种最大程度地保留了原数据之间的关系.
概率论与数理统计中, 方差(Variance)描述了样本整体疏密的一个指标.</p>
<h1 id="如何找到这个样本间间距方差最大的轴"><a class="header" href="#如何找到这个样本间间距方差最大的轴">如何找到这个样本间间距(方差)最大的轴？</a></h1>
<p><img src="http://windmissing.github.io/images/2019/91.png" alt="" /></p>
<p>$$
\begin{aligned}
Var(x) = \frac{1}{m}\sum_{i=1}^m(x_i-\bar x)^2 &amp;&amp; (1)
\end{aligned}
$$</p>
<p>其中 $$\bar x$$ 代表x的平均值。</p>
<p>第一步：将所有样本的均值归0 （demean）<br />
<img src="http://windmissing.github.io/images/2019/92.png" alt="" /><br />
由于均值为0，则公式（1）可以化简为:</p>
<p>$$
\begin{aligned}
Var(x) = \frac{1}{m}\sum_{i=1}^m(x_i-\bar x)^2 = \frac{1}{m}\sum_{i=1}^mx_i^2  &amp;&amp; (2)
\end{aligned}
$$</p>
<p>这里的x是样本映射到坐标轴以后的新的样本的值 $$X_{\text{project}}$$ 。<br />
第二步：求一个轴的方向w = (w1, w2)，使用所有的样本映射到w以后，有 $$Var(X_{\text{project}})$$ 最大:</p>
<p>$$
\begin{aligned}
Var(X_{\text{project}}) = \frac{1}{m}\sum_{i=1}^m||x^{(i)}_{\text{project}}||^2   &amp;&amp; (3)
\end{aligned}
$$</p>
<p>推导过程：<br />
<img src="http://windmissing.github.io/images/2019/95.png" alt="" /><br />
假设均值化之后的x的坐标为 </p>
<p>$$
X^{(i)}=(X^{(i)}_1, X^{(i)}_2)
$$</p>
<p>要映射的坐标轴为 $$w-{w_1, w_2}$$<br />
$$X^{(i)}$$ 映射到 $$w - {w_1, w_2}$$ 之后的新坐标为</p>
<p>$$X_{pr}^{(i)} = (X^{(i)}<em>{pr1}, X^{(i)}</em>{pr2})$$</p>
<p>那么, $$||x^{(i)}_{\text{project}}||$$ 为:</p>
<p>$$
||x^{(i)}_{\text{project}}|| = X^{(i)} \cdot  w
$$</p>
<p>代入公式（3）得:</p>
<p>$$
\begin{aligned}
Var(X_{\text{project}}) = \frac{1}{m}\sum_{i=1}^m|| X^{(i)} \cdot  w||^2   &amp;&amp; (4)
\end{aligned}
$$</p>
<p>目标是最大化公式（4），这是一个求目标函数的最优化问题，使用梯度上升法解决。</p>
<h1 id="主成分分析法和线性回归的区别"><a class="header" href="#主成分分析法和线性回归的区别">主成分分析法和线性回归的区别</a></h1>
<p><img src="http://windmissing.github.io/images/2019/97.png" alt="" /></p>
<p>| 主成分分析  | 线性回归
--|---|--
坐标轴                | 2个特征  | 1个特征和输出标记
要求的是什么          | 一个方向  | 一根直线
经过点的线与什么垂直  | 所求的方向垂直  |  x轴
目标                 | 方差最大  | MSE最小</p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/99.png" alt="" />
这是一个非监督学习算法，在式子中没有输出标记y<br />
使用梯度上升法，就要先对目标函数求梯度，计算结果如下：<br />
<img src="http://windmissing.github.io/images/2019/98.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

X = np.empty((100, 2))
X[:,0] = np.random.uniform(0., 100, size=100)
X[:,1] = 0.75 * X[:, 0] + 3. + np.random.normal(0, 10., size=100)

plt.scatter(X[:,0], X[:,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/100.png" alt="" /></p>
<h1 id="第一步demean"><a class="header" href="#第一步demean">第一步：demean</a></h1>
<pre><code class="language-python">def demean(X):
    return X - np.mean(X, axis=0)

X_demean = demean(X)
plt.scatter(X_demean[:,0], X_demean[:,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/101.png" alt="" /></p>
<h1 id="第二步梯度上升法"><a class="header" href="#第二步梯度上升法">第二步：梯度上升法</a></h1>
<pre><code class="language-python">def f(w, X):
    return np.sum((X.dot(w)**2)) / len(X)

def df_math(w, X):
    return X.T.dot(X.dot(w)) * 2. / len(X)

def df_debug(w, X, epsilon=0.0001):
    res = np.empty(len(w))
    for i in range(len(w)):
        w_1 = w.copy()
        w_1[i] += epsilon
        w_2 = w.copy()
        w_2[i] -= epsilon
        res[i] = (f(w_1, X) - f(w_2, X)) / (2 * epsilon)
    return res

# 把向量单位化
def direction(w):
    return w / np.linalg.norm(w)

def gradient_ascent(df, X, initial_w, eta, n_iters=1e4, epsilon=1e-8):
    w = direction(initial_w)
    cur_iter = 0
    while cur_iter &lt; n_iters:
        gradient = df(w, X)
        last_w = w
        w = w + eta * gradient
        w = direction(w)
        if(abs(f(w, X)) - abs(f(last_w, X)) &lt; epsilon):
           break
        cur_iter += 1
    return w
</code></pre>
<p><strong>注意1：epsilon取值比较小，因为w是方向向量，它的每个维度都很小，所以epsilon也要取很小的值</strong><br />
<strong>注意2：每次计算出w后要对其单位化</strong><br />
如果每次计算出w后不做单位化的工作，算法也可以工作，因为w本身也是代方向的。<br />
但这样会导致搜索过程不顺畅。<br />
因为如果不做单位化，w应该是公式要求的w偏大的，这就要求eta值非常小。<br />
而eta值小又会导致循环次数非常多，性能就会下降。<br />
因此遵循公式的假设条件，每次都让w成为方向向量。</p>
<h1 id="训练和绘制结果"><a class="header" href="#训练和绘制结果">训练和绘制结果</a></h1>
<pre><code class="language-python">initial_w = np.random.random(X.shape[1])
eta = 0.001
gradient_ascent(df_debug, X_demean, initial_w, eta)
w = gradient_ascent(df_math, X_demean, initial_w, eta)
plt.scatter(X_demean[:, 0], X_demean[:, 1])
plt.plot([0, w[0]*30], [0, w[1]*30], color='r')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/102.png" alt="" /><br />
<strong>注意3：w不能是零向量。因为w=0本身也是在极值点上，是极小值点，此时梯度也会0</strong><br />
<strong>注意4：不能使用StandardScaler标准化数据。</strong><br />
因为本算法的目标就是让方差最大。<br />
一但对数据做了标准化，样本的方差就肯定是1了，不存在方差最大值。</p>
<h1 id="另一个更极端的例子"><a class="header" href="#另一个更极端的例子">另一个更极端的例子</a></h1>
<pre><code class="language-python">X2 = np.empty((100, 2))
X2[:,0] = np.random.uniform(0., 100, size=100)
X2[:,1] = 0.75 * X2[:, 0] + 3.

X2_demean = demean(X2)
w2 = gradient_ascent(df_debug, X2_demean, initial_w, eta)

plt.scatter(X2_demean[:, 0], X2_demean[:, 1])
plt.plot([0, w2[0]*30], [0, w2[1]*30], color='r')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/103.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p>本质上是从一组坐标第转移到了另一组坐标系。
原来的坐标系有n个方向，那么新的坐标系也应该有n个方向。
7-2中的算法只是求出第一个轴的方向。</p>
<p>在新的坐标系中，第一个轴保存了样本最大的方差，称为第一个主成分。
第二个次之，依此类推。</p>
<h1 id="问求出第一主成分以后如何求出下一主成分呢"><a class="header" href="#问求出第一主成分以后如何求出下一主成分呢">问：求出第一主成分以后，如何求出下一主成分呢？</a></h1>
<p>答：<br />
<strong>第一步：</strong> 改变数据，将数据的第一个主成分去掉。<br />
<img src="http://windmissing.github.io/images/2019/104.png" alt="" /><br />
图中X'是X去除了第一主成分上的分量后的结果<br />
<strong>第二步：</strong> 在新数据上求第一主成分</p>
<h1 id="准备数据-4"><a class="header" href="#准备数据-4">准备数据</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

X = np.empty((100, 2))
X[:,0] = np.random.uniform(0., 100, size=100)
X[:,1] = 0.75 * X[:, 0] + 3. + np.random.normal(0, 10., size=100)

plt.scatter(X[:,0], X[:,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/100.png" alt="" /></p>
<h1 id="第一步demean-1"><a class="header" href="#第一步demean-1">第一步：demean</a></h1>
<pre><code class="language-python">def demean(X):
    return X - np.mean(X, axis=0)

X_demean = demean(X)
plt.scatter(X_demean[:,0], X_demean[:,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/101.png" alt="" /></p>
<h1 id="第二步梯度上升法-1"><a class="header" href="#第二步梯度上升法-1">第二步：梯度上升法</a></h1>
<pre><code class="language-python">def f(w, X):
    return np.sum((X.dot(w)**2)) / len(X)

def df(w, X):
    return X.T.dot(X.dot(w)) * 2. / len(X)

# 把向量单位化
def direction(w):
    return w / np.linalg.norm(w)

def first_component(X, initial_w, eta, n_iters=1e4, epsilon=1e-8):
    w = direction(initial_w)
    cur_iter = 0
    while cur_iter &lt; n_iters:
        gradient = df(w, X)
        last_w = w
        w = w + eta * gradient
        w = direction(w)
        if(abs(f(w, X)) - abs(f(last_w, X)) &lt; epsilon):
           break
        cur_iter += 1
    return w
</code></pre>
<h2 id="训练和绘制结果-1"><a class="header" href="#训练和绘制结果-1">训练和绘制结果</a></h2>
<pre><code class="language-python">initial_w = np.random.random(X.shape[1])
eta = 0.001
w = first_component(X_demean, initial_w, eta)
</code></pre>
<p>输入：w<br />
输出：array([0.77135006, 0.63641109])</p>
<h1 id="第三步去掉第一个主成分"><a class="header" href="#第三步去掉第一个主成分">第三步：去掉第一个主成分</a></h1>
<h2 id="方法一"><a class="header" href="#方法一">方法一：</a></h2>
<pre><code class="language-python">X2 = np.empty(X.shape)
for i in range(len(X)):
   X2[i] = X[i] - X[i].dot(w) * w
</code></pre>
<h2 id="方法二"><a class="header" href="#方法二">方法二：</a></h2>
<pre><code class="language-python">X2 = X - X.dot(w).reshape(-1, 1) * w
</code></pre>
<h2 id="去掉第一主成分后的数据"><a class="header" href="#去掉第一主成分后的数据">去掉第一主成分后的数据</a></h2>
<pre><code class="language-python">plt.scatter(X2[:,0], X2[:,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/105.png" alt="" /></p>
<h1 id="第四步求新数据的第一主成分"><a class="header" href="#第四步求新数据的第一主成分">第四步：求新数据的第一主成分</a></h1>
<pre><code class="language-python">w2 = first_component(X2, initial_w, eta)
</code></pre>
<p>输入：w2<br />
输出：array([-0.63639346,  0.77136461])</p>
<p>输入：w.dot(w2)<br />
输出：2.2857453091384983e-05<br />
点乘结果几乎为0，说明w和w2是垂直关系</p>
<h1 id="封装成函数"><a class="header" href="#封装成函数">封装成函数</a></h1>
<pre><code class="language-python">def first_n_component(n, X, eta = 0.01, n_iters=1e4, epsilon=1e-8):
    X_pca = X.copy()
    X_pca = demean(X_pca)
    res = []
    for i in range(n):
        initial_w = np.random.random(X.shape[1])
        eta = 0.001
        w = first_component(X_pca, initial_w, eta)
        res.append(w)
        X_pca = X_pca - X_pca.dot(w).reshape(-1, 1) * w
    return res
</code></pre>
<p>输入：<code>first_n_component(2, X)</code><br />
输出：[array([0.77135082, 0.63641018]), array([ 0.63642749, -0.77133653])]<br />
<strong>[?]遗留问题：我算出的第二个主成分的方向和视频中是反的？</strong><br />
可能是跟initial_w有关，多次运行后发现两个方向的结果都有。</p>
<div style="break-before: page; page-break-before: always;"></div><p>定义：X是样本数据，每一行是一个数据，它有m个数据，每个数据有n个特征<br />
$$
X = 
\begin{bmatrix}
X_1^{(1)} &amp;&amp; X_1^{(2)} &amp;&amp; \cdots &amp;&amp; X_1^{(n)}   \
X_2^{(1)} &amp;&amp; X_2^{(2)} &amp;&amp; \cdots &amp;&amp; X_2^{(n)}   \
\cdots &amp;&amp; \cdots &amp;&amp; \cdots &amp;&amp; \cdots \
X_m^{(1)} &amp;&amp; X_m^{(2)} &amp;&amp; \cdots &amp;&amp; X_m^{(n)}
\end{bmatrix}
$$</p>
<p>$W_k$是求得的前k个主成分矩阵，每一行是一个主成分的单位方向，它有k个主成分方向，每个主成分的方向有n个维度<br />
$$
X = 
\begin{bmatrix}
W_k^{(1)} &amp;&amp; W_1^{(2)} &amp;&amp; \cdots &amp;&amp; W_1^{(n)}   \
W_2^{(1)} &amp;&amp; W_2^{(2)} &amp;&amp; \cdots &amp;&amp; W_2^{(n)}   \
\cdots &amp;&amp; \cdots &amp;&amp; \cdots &amp;&amp; \cdots \
W_k^{(1)} &amp;&amp; W_k^{(2)} &amp;&amp; \cdots &amp;&amp; W_k^{(n)}
\end{bmatrix}
$$</p>
<p>问：如何将样本X从N维转换成K维？<br />
答：降维：把所有样本映射到K个主成分上<br />
$$
X \cdot W_k^T = X_k
$$</p>
<p>还原：把降维后的数据还原到原坐标空间<br />
$$
X_k \cdot W_k = X_m
$$</p>
<p>还原后的X与原X不同。</p>
<h1 id="把pca封装成类"><a class="header" href="#把pca封装成类">把PCA封装成类</a></h1>
<pre><code class="language-python">import numpy as np

class PCA:
    def __init__(self, n_components):
        &quot;&quot;&quot;初始化PCA&quot;&quot;&quot;
        assert n_components &gt;= 1, &quot;n_components must be valid&quot;
        self.n_components = n_components
        self.components_ = None

    def fit(self, X, eta=0.01, n_iters=1e4):
        &quot;&quot;&quot;获取数据集的前n个主成分&quot;&quot;&quot;
        assert self.n_components &lt;= X.shape[1], &quot;n_components must not be greater than the feature number of X&quot;

        def demean(X):
            return X - np.mean(X, axis=0)

        def f(w, X):
            return np.sum((X.dot(w)**2)) / len(X)

        def df(w, X):
            return X.T.dot(X.dot(w)) * 2. / len(X)

        # 把向量单位化
        def direction(w):
            return w / np.linalg.norm(w)

        def first_component(X, initial_w, eta, n_iters=1e4, epsilon=1e-8):
            w = direction(initial_w)
            cur_iter = 0
            while cur_iter &lt; n_iters:
                gradient = df(w, X)
                last_w = w
                w = w + eta * gradient
                w = direction(w)
                if(abs(f(w, X)) - abs(f(last_w, X)) &lt; epsilon):
                   break
                cur_iter += 1
            return w

        X_pca = demean(X)
        self.components_ = np.empty(shape = (self.n_components, X.shape[1]))
        for i in range(self.n_components):
            initial_w = np.random.random(X.shape[1])
            eta = 0.001
            w = first_component(X_pca, initial_w, eta)
            self.components_[i, :] = w
            X_pca = X_pca - X_pca.dot(w).reshape(-1, 1) * w
        return self

    def transform(self, X):
        &quot;&quot;&quot;将给定的X，映射到各个主成分分量中&quot;&quot;&quot;
        assert X.shape[1] == self.components_.shape[1]
        return X.dot(self.components_.T)

    def inverse_transform(self, X):
        &quot;&quot;&quot;将给定的X反向映射回原来的特征空间&quot;&quot;&quot;
        assert X.shape[1] == self.components_.shape[0]
        return X.dot(self.components_)

    def __repr__(self):
        return &quot;PCA(n_components=%d)&quot; % self.n_components
</code></pre>
<h1 id="使用pca降维"><a class="header" href="#使用pca降维">使用PCA降维</a></h1>
<h2 id="准备数据-5"><a class="header" href="#准备数据-5">准备数据</a></h2>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

X = np.empty((100,2))
X[:,0] = np.random.uniform(0., 100., size=100)
X[:,1] = 0.75 * X[:, 0] + 3. + np.random.normal(0, 10., size=100)
</code></pre>
<h2 id="训练模型1"><a class="header" href="#训练模型1">训练模型1</a></h2>
<pre><code class="language-python">pca = PCA(n_components=2)
pca.fit(X)
</code></pre>
<p>输入：<code>pca.components_</code><br />
输出：array([[ 0.75366776,  0.65725559], [-0.65723751,  0.75368352]])</p>
<h2 id="训练模型2降维"><a class="header" href="#训练模型2降维">训练模型2：降维</a></h2>
<pre><code class="language-python">pca = PCA(n_components=1)
pca.fit(X)

X_reduction = pca.transform(X)
X_restore = pca.inverse_transform(X_reduction)
</code></pre>
<p>输入：<code>X_reduction.shape</code><br />
输出：(100, 1)</p>
<p>输入：<code>X_restore.shape</code><br />
输出：(100, 2)</p>
<h2 id="对比原始数据与降维再恢复后的数据"><a class="header" href="#对比原始数据与降维再恢复后的数据">对比原始数据与降维再恢复后的数据</a></h2>
<pre><code class="language-python">plt.scatter(X[:, 0], X[:, 1], color='b', alpha=0.5)
plt.scatter(X_restore[:, 0], X_restore[:, 1], color='r', alpha=0.5)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/111.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p>沿用7-5中的测试数据，使用scikit-learn中的PCA</p>
<pre><code class="language-python">from sklearn.decomposition import PCA

pca = PCA(n_components=1)
pca.fit(X)
</code></pre>
<p>输入：<code>pca.components_</code><br />
输出：array([[-0.75366744, -0.65725595]])<br />
这个轴与7-5中的计算结果是相反的。因为scikit-learn中不是什么梯度下降法而是什么数学方法计算的。<br />
轴的方向相反不影响算法的结果</p>
<p>对比原始数据与降维再恢复后的数据</p>
<pre><code class="language-python">X_reduction = pca.transform(X)
X_restore = pca.inverse_transform(X_reduction)

plt.scatter(X[:, 0], X[:, 1], color='b', alpha=0.5)
plt.scatter(X_restore[:, 0], X_restore[:, 1], color='r', alpha=0.5)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/111.png" alt="" /></p>
<h1 id="使用真实数据测试pca降维对效率和准确度的影响"><a class="header" href="#使用真实数据测试pca降维对效率和准确度的影响">使用真实数据测试PCA降维对效率和准确度的影响</a></h1>
<h2 id="真实数据-1"><a class="header" href="#真实数据-1">真实数据</a></h2>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
</code></pre>
<h2 id="几种降维结果比较"><a class="header" href="#几种降维结果比较">几种降维结果比较</a></h2>
<table><thead><tr><th>PCA后的维数</th><th>运行时间</th><th>score</th></tr></thead><tbody>
<tr><td>不降维</td><td>82.9 ms</td><td>0.9866666666666667</td></tr>
<tr><td>2</td><td>2.2</td><td>0.6066666666666667</td></tr>
<tr><td>28</td><td>1，05<br><strong>时间更少了？</strong></td><td>0.98</td></tr>
</tbody></table>
<p>结论：  如果n_components选择合适，会大大减少训练时间而略微减少分类准确度，这样做是值得的。</p>
<h2 id="选择合适的降维效果"><a class="header" href="#选择合适的降维效果">选择合适的降维效果</a></h2>
<p>确定新坐标系中每个维度保存了原数据的方差百分比</p>
<pre><code class="language-python">pca = PCA(n_components=X_train.shape[1])
pca.fit(X_train)
pca.explained_variance_ratio_
</code></pre>
<p>输出结果：<br />
<img src="http://windmissing.github.io/images/2019/112.png" alt="" /></p>
<pre><code class="language-python">plt.plot([i for i in range(X_train.shape[1])],
    [np.sum(pca.explained_variance_ratio_[:i+1]) for  i in range(X_train.shape[1])])
plt.show()
</code></pre>
<p>输出结果：
<img src="http://windmissing.github.io/images/2019/113.png" alt="" /><br />
这张图表示了前N个维度所占方差的百分比</p>
<p>保留原始数据95%的方差</p>
<pre><code class="language-python">pca = PCA(0.95)
pca.fit(X_train)
pca.explained_variance_ratio_
</code></pre>
<h1 id="对原始数据降至2维的结果也有一定参考意义"><a class="header" href="#对原始数据降至2维的结果也有一定参考意义">对原始数据降至2维的结果也有一定参考意义</a></h1>
<pre><code class="language-python">pca = PCA(n_components=2)
pca.fit(X)
X_reduction = pca.transform(X)

for i in range(10):
    plt.scatter(X_reduction[y==i, 0], X_reduction[y==i,1], alpha=0.8)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/114.png" alt="" /></p>
<p>假如只是要区分图中紫色的数据和红色的数据，降到2维就足够了</p>
<div style="break-before: page; page-break-before: always;"></div><p>使用sklearn提供的接口加载经典的MNIST手写数据集</p>
<pre><code class="language-python">import numpy as np
from sklearn.datasets import fetch_openml

mnist_data = fetch_openml(&quot;mnist_784&quot;)

X, y = mnist_data['data'], mnist_data['target']     # X.shape = （70000， 784）
X_train = np.array(X[:60000], dtype=float)
y_train = np.array(y[:60000], dtype=float)
X_test = np.array(X[60000:], dtype=float)
y_test = np.array(y[60000:], dtype=float)
</code></pre>
<h1 id="knn算法对mnist分类"><a class="header" href="#knn算法对mnist分类">KNN算法对MNIST分类</a></h1>
<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier()
%time knn_clf.fit(X_train, y_train)   # Wall time: 1min 3s

%time knn_clf.score(X_test, y_test)   # Wall time = 15min 43, score = 0.9688
</code></pre>
<p><strong>Note 1：</strong><br />
为什么KNN算法的fit要花费这么多时间？<br />
因为当训练数据比较大的情况下，sklearn的KNN算法会使用tree来存储数据，而不是直接存储数据。</p>
<p><strong>Note 2:</strong><br />
KNN算法通常需要对数据进行归一化，为什么这里没有做归一化？<br />
因为当前的样本数据中，所有的特征都是表示图像中的像素点，整体处于同一个尺度，所以不需要归一化。</p>
<p><strong>Note 3:</strong><br />
训练样本非常大的情况下，KNN算法非常耗时</p>
<h1 id="pca--knn--mnist"><a class="header" href="#pca--knn--mnist">PCA + KNN + MNIST</a></h1>
<pre><code class="language-python">from sklearn.decomposition import PCA

pca = PCA(0.9)
pca.fit(X_train)
X_train_reduction = pca.transform(X_train)   # X_train_reductionduction.shape = (60000， 784)

knn_clf = KNeighborsClassifier()
%time knn_clf.fit(X_train_reduction, y_train)   # Wall time: 15.6 s

X_test_reduction = pca.transform(X_test)
%time knn_clf.score(X_test_reduction, y_test)   # Wall time = 2min 20s, score = 0.8728
</code></pre>
<p>运行时间减少的同时，预测准确率反面提高了<br />
<strong>PCA在降维的同时还可以降噪</strong></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="之前的例子"><a class="header" href="#之前的例子">之前的例子</a></h1>
<p>原始数据：</p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

X = np.empty((100,2))
X[:,0] = np.random.uniform(0., 100., size=100)
X[:,1] = 0.75 * X[:,0] + 3. + np.random.normal(0, 5, size=100)

plt.scatter(X[:, 0], X[:, 1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/115.png" alt="" /></p>
<p>降噪后：</p>
<pre><code class="language-python">from sklearn.decomposition import PCA
pca = PCA(n_components=1)
pca.fit(X)
X_reduction = pca.transform(X)
X_restore = pca.inverse_transform(X_reduction)
plt.scatter(X_restore[:, 0], X_restore[:, 1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/116.png" alt="" /></p>
<p>从图1到图2有信丢失了，丢失的这部分信息中很有可能有很大一部分是噪声
降维的过程中丢失了信息，同时也去除了部分噪音</p>
<h1 id="手写识别例子"><a class="header" href="#手写识别例子">手写识别例子</a></h1>
<p>原始的手写数据：</p>
<pre><code class="language-python">from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target

example_digits = X[y == 0, :][:10]
for num in range(1, 10):
    X_num = X[y==num,:][:10]
    example_digits = np.vstack([example_digits, X_num])
</code></pre>
<p>对原始的手写数据加噪</p>
<pre><code class="language-python">example_digits = example_digits + np.random.normal(0, 4, size=X.shape)
</code></pre>
<p>显示加噪后的图像：</p>
<pre><code class="language-python">def plot_digits(data):
    #fig, axes = plt.subplots(10, 10, figsize=(10, 10), subplot_kw={'xticks':[], 'yticks':[]},girdspec_kw=dict(hspace=0.1, wspace=0.1))
    fig, axes = plt.subplots(10, 10, figsize=(10, 10), subplot_kw={'xticks':[], 'yticks':[]})
    for i,ax in enumerate(axes.flat):
        ax.imshow(data[i].reshape(8, 8), cmap='binary', interpolation='nearest', clim=(0,16))
    plt.show()

plot_digits(example_digits)
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/117.png" alt="" /></p>
<p>对example_digits去噪</p>
<pre><code class="language-python">pca = PCA(0.5)
pca.fit(noisy_digits)   # pca.n_components_ = 12
components = pca.transform(example_digits)
filtered_digits = pca.inverse_transform(components)
</code></pre>
<p>去噪后的效果：</p>
<pre><code class="language-python">plot_digits(filtered_digits)
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/118.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/119.jpg" alt="" /></p>
<p>把W矩阵中的每一行看作一个方向，第一行代表最重要的方向，第二行代表次重要的方向，依次类推<br />
也可以说：<br />
将W矩阵中的每一行看作一个样本，第一行所代表的样本是最重要的样本，最能反应原X矩阵样本特征的样本。第二行所代表的样本是次重要的样本，依次类推。</p>
<p>X中的每一行都是一张人脸，W中的每一行也可以认为是一张脸，称为特征脸。每一个特征脸都是一个主成分，相当于表达了原样本数据的特征。（特征这个词与矩阵中的特征值这个词相对应）</p>
<h1 id="取数据"><a class="header" href="#取数据">取数据</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_lfw_people

faces = fetch_lfw_people()
random_indexes = np.random.permutation(len(faces.data))
X = faces.data[random_indexes]
</code></pre>
<p>查看图像:</p>
<pre><code class="language-python">def plot_faces(faces):
    #fig, axes = plt.subplots(6, 6, figsize=(10, 10), subplot_kw={'xticks':[], 'yticks':[]},girdspec_kw=dict(hspace=0.1, wspace=0.1))
    fig, axes = plt.subplots(10, 10, figsize=(10, 10), subplot_kw={'xticks':[], 'yticks':[]})
    for i,ax in enumerate(axes.flat):
        ax.imshow(data[i].reshape(62, 47), cmap='bone')
    plt.show()

plot_faces(example_faces)
</code></pre>
<h1 id="特征脸"><a class="header" href="#特征脸">特征脸</a></h1>
<pre><code class="language-python">from sklearn.decomposition import PCA
pca = PCA(svd_solver=&quot;randomized&quot;)  #因为数据样本比较大，用随机的方式会快一些

%timeit pca.fit(X)

plot_faces(pca.components_[:36, :])
</code></pre>
<h1 id="其它关于样本库"><a class="header" href="#其它关于样本库">其它关于样本库</a></h1>
<pre><code class="language-python">faces2 = fetch_lfw_people(min_faces_per_person=60) # 取一个人至少有60张照片的样本
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>线性回归要求数据存在线性关系。<br />
但实际场景中，存在强线性关系的数据是比较少的，大部分情况下数据之间是非线性关系。<br />
用一种简单的手段改进线性回归法，使得它可以处理和预测非线性数据。<br />
即多项式回归。
<img src="http://windmissing.github.io/images/2019/121.jpg" alt="" /></p>
<h1 id="准备数据-6"><a class="header" href="#准备数据-6">准备数据</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

x = np.random.uniform(-3, 3, size=100)
X = x. reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)

plt.scatter(X, y)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/120.png" alt="" /></p>
<h1 id="使用线性回归"><a class="header" href="#使用线性回归">使用线性回归</a></h1>
<pre><code class="language-python">from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X, y)
y_predict = lin_reg.predict(X)

plt.scatter(x, y)
plt.plot(x, y_predict, color='r')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/122.png" alt="" /></p>
<h1 id="解决方案-添加一个特征"><a class="header" href="#解决方案-添加一个特征">解决方案， 添加一个特征</a></h1>
<pre><code class="language-python">X2 = np.hstack([X, X**2])

lin_reg2 = LinearRegression()
lin_reg2.fit(X2, y)
y_predict2 = lin_reg2.predict(X2)

plt.scatter(x, y)
plt.plot(np.sort(x), y_predict2[np.argsort(x)], color='r')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/123.png" alt="" /></p>
<p>看上去这根曲线拟合得更好。</p>
<p>输入：<code>lin_reg2.coef_ </code><br />
输出：array([0.99902653, 0.46334749])<br />
0.999是x的系数，0.46是x^2的系数</p>
<p>输入：<code>lin_reg2.intercept_</code><br />
输出：2.0518267069340164</p>
<h1 id="结论"><a class="header" href="#结论">结论</a></h1>
<p>使用线性回归的思路，为原来的样本添加新的特征，新的特征是原有特征的多项式的组合。以此来解决非线性问题。</p>
<p>PCA是对数据做降维处理，这里则是对数据集升维。通过升维和添加特征，使算法拟合高维度的数据。</p>
<div style="break-before: page; page-break-before: always;"></div><p>线性回归要求数据存在线性关系。<br />
但实际场景中，存在强线性关系的数据是比较少的，大部分情况下数据之间是非线性关系。<br />
用一种简单的手段改进线性回归法，使得它可以处理和预测非线性数据。<br />
即多项式回归。
<img src="http://windmissing.github.io/images/2019/121.jpg" alt="" /></p>
<h1 id="准备数据-7"><a class="header" href="#准备数据-7">准备数据</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

x = np.random.uniform(-3, 3, size=100)
X = x. reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)

plt.scatter(X, y)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/120.png" alt="" /></p>
<h1 id="使用线性回归-1"><a class="header" href="#使用线性回归-1">使用线性回归</a></h1>
<pre><code class="language-python">from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X, y)
y_predict = lin_reg.predict(X)

plt.scatter(x, y)
plt.plot(x, y_predict, color='r')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/122.png" alt="" /></p>
<h1 id="解决方案-添加一个特征-1"><a class="header" href="#解决方案-添加一个特征-1">解决方案， 添加一个特征</a></h1>
<pre><code class="language-python">X2 = np.hstack([X, X**2])

lin_reg2 = LinearRegression()
lin_reg2.fit(X2, y)
y_predict2 = lin_reg2.predict(X2)

plt.scatter(x, y)
plt.plot(np.sort(x), y_predict2[np.argsort(x)], color='r')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/123.png" alt="" /></p>
<p>看上去这根曲线拟合得更好。</p>
<p>输入：<code>lin_reg2.coef_ </code><br />
输出：array([0.99902653, 0.46334749])<br />
0.999是x的系数，0.46是x^2的系数</p>
<p>输入：<code>lin_reg2.intercept_</code><br />
输出：2.0518267069340164</p>
<h1 id="结论-1"><a class="header" href="#结论-1">结论</a></h1>
<p>使用线性回归的思路，为原来的样本添加新的特征，新的特征是原有特征的多项式的组合。以此来解决非线性问题。</p>
<p>PCA是对数据做降维处理，这里则是对数据集升维。通过升维和添加特征，使算法拟合高维度的数据。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="准备数据-8"><a class="header" href="#准备数据-8">准备数据</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

x = np.random.uniform(-3, 3, size=100)
X = x. reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)
</code></pre>
<h1 id="使用polynomialfeatures为原数据升维"><a class="header" href="#使用polynomialfeatures为原数据升维">使用polynomialFeatures为原数据升维</a></h1>
<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
poly.fit(X)
X2 = poly.transform(X)
</code></pre>
<p>输入：X2.shape<br />
输出：(100, 3)</p>
<p>输入：X2[:5,:]<br />
输出：</p>
<pre><code>array([[ 1.        , -1.34284888,  1.80324311],
       [ 1.        , -0.18985858,  0.03604628],
       [ 1.        , -1.58563134,  2.51422675],
       [ 1.        ,  1.2149354 ,  1.47606802],
       [ 1.        , -2.05874706,  4.23843944]])
</code></pre>
<p>输入：X2[:5,:]<br />
输出：</p>
<pre><code>array([[-1.34284888],
       [-0.18985858],
       [-1.58563134],
       [ 1.2149354 ],
       [-2.05874706]])
</code></pre>
<p>X2中第一列是1，第二列是原数据，第三列是原数据的平方</p>
<h1 id="使用scikit-learn中的线性回归算法"><a class="header" href="#使用scikit-learn中的线性回归算法">使用scikit-learn中的线性回归算法</a></h1>
<p>这一部分与8-1相同</p>
<pre><code class="language-python">from sklearn.linear_model import LinearRegression

lin_reg2 = LinearRegression()
lin_reg2.fit(X2, y)
y_predict2 = lin_reg2.predict(X2)

plt.scatter(x, y)
plt.plot(np.sort(x), y_predict2[np.argsort(x)], color='r')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/123.png" alt="" /></p>
<p>输入：<code>lin_reg2.coef_</code><br />
输出：array([0.        , 1.01723515, 0.46407147])<br />
0是对X2是第一列数据拟合的结果</p>
<p>输入：<code>lin_reg2.intercept_</code><br />
输出：2.1789150996943945</p>
<h1 id="关于polynomialfeatures"><a class="header" href="#关于polynomialfeatures">关于PolynomialFeatures</a></h1>
<pre><code class="language-python">X = np.arange(1,11).reshape(-1, 2)
poly = PolynomialFeatures(degree=2)
poly.fit(X)
X2 = poly.transform(X)
</code></pre>
<p>输入：X.shape<br />
输出：(5, 2)</p>
<p>输入：X<br />
输出：array([[ 1,  2], [ 3,  4], [ 5,  6], [ 7,  8], [ 9, 10]])</p>
<p>输入：X2.shape<br />
输出：(5, 6)</p>
<p>输入：X2<br />
输出：</p>
<pre><code>array([[  1.,   1.,   2.,   1.,   2.,   4.],
       [  1.,   3.,   4.,   9.,  12.,  16.],
       [  1.,   5.,   6.,  25.,  30.,  36.],
       [  1.,   7.,   8.,  49.,  56.,  64.],
       [  1.,   9.,  10.,  81.,  90., 100.]])
</code></pre>
<p>第一列：1，即0次幂<br />
第二列：x1，1次幂<br />
第三列：x2，1次幂<br />
第四列：x1^2，2次幂<br />
第五列：x1*x2，2次幂<br />
第六列：x2^2，2次幂</p>
<h2 id="假设有x1-x2两个特征polynomialfeaturesdegree3会得到多少项数据"><a class="header" href="#假设有x1-x2两个特征polynomialfeaturesdegree3会得到多少项数据">假设有x1, x2两个特征，PolynomialFeatures(degree=3)，会得到多少项数据？</a></h2>
<p><img src="http://windmissing.github.io/images/2019/124.jpg" alt="" /></p>
<pre><code class="language-python">poly = PolynomialFeatures(degree=3)
poly.fit(X)
X3 = poly.transform(X)
# X3.shape = (5, 10)
# array([[   1.,    1.,    2.,    1.,    2.,    4.,    1.,    2.,    4.,    8.],
#        [   1.,    3.,    4.,    9.,   12.,   16.,   27.,   36.,   48.,   64.],
#        [   1.,    5.,    6.,   25.,   30.,   36.,  125.,  150.,  180.,  216.],
#        [   1.,    7.,    8.,   49.,   56.,   64.,  343.,  392.,  448.,  512.],
#        [   1.,    9.,   10.,   81.,   90.,  100.,  729.,  810.,  900., 1000.]])
</code></pre>
<h1 id="pipeline"><a class="header" href="#pipeline">Pipeline</a></h1>
<p>使用pipeline把多项式特征、数据规一化、线性回归三步合在一起，就不需要在每一次调用时都重复这三步<br />
sklearn没有直接提供多项式回归算法，但可以使用pipe很方便地创建一个多项式回归算法</p>
<pre><code class="language-python">x = np.random.uniform(-3, 3, size=100)
X = x. reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

poly_reg = Pipeline([
    (&quot;poly&quot;, PolynomialFeatures(degree=2)),
    (&quot;std_scaler&quot;, StandardScaler()),
    (&quot;lin_reg&quot;, LinearRegression())
])

poly_reg.fit(X, y)
y_predict = poly_reg.predict(X)

plt.scatter(x, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/123.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="准备数据-9"><a class="header" href="#准备数据-9">准备数据</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

x = np.random.uniform(-3, 3, size=100)
X = x. reshape(-1, 1)
y = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)
</code></pre>
<h1 id="使用线性回归-2"><a class="header" href="#使用线性回归-2">使用线性回归</a></h1>
<pre><code class="language-python">from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X, y)
lin_reg.score(X, y)    # score = 0.4953707811865009

y_predict = lin_reg.predict(X)
plt.scatter(x, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/122.png" alt="" /></p>
<pre><code class="language-python">from sklearn.metrics import mean_squared_error

y_predict = lin_reg.predict(X)
mean_squared_error(y, y_predict)
</code></pre>
<p>均方误差为：3.0750025765636577</p>
<h1 id="使用多项式回归"><a class="header" href="#使用多项式回归">使用多项式回归</a></h1>
<h2 id="多项式回归算法"><a class="header" href="#多项式回归算法">多项式回归算法</a></h2>
<pre><code class="language-python">from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

def PolynomialRegression(degree):
    return Pipeline([
        (&quot;poly&quot;, PolynomialFeatures(degree=degree)),
        (&quot;std_scaler&quot;, StandardScaler()),
        (&quot;lin_reg&quot;, LinearRegression())
    ])
</code></pre>
<h2 id="degree--2的多项式回归"><a class="header" href="#degree--2的多项式回归">degree = 2的多项式回归</a></h2>
<pre><code class="language-python">poly2_reg = PolynomialRegression(degree=2)
poly2_reg.fit(X, y)
y2_predict = poly2_reg.predict(X)
mean_squared_error(y, y2_predict)   # 1.0987392142417856

y_predict = lin_reg.predict(X)
plt.scatter(x, y)
plt.plot(np.sort(x), y2_predict[np.argsort(x)], color='r')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/123.png" alt="" /></p>
<h2 id="degree取不同值得到的均方误差和拟合结果"><a class="header" href="#degree取不同值得到的均方误差和拟合结果">degree取不同值得到的均方误差和拟合结果</a></h2>
<table><thead><tr><th>degree</th><th>MSE</th><th>plot</th></tr></thead><tbody>
<tr><td>线性</td><td>3.0750025765636577</td><td><img src="http://windmissing.github.io/images/2019/122.png" alt="" /></td></tr>
<tr><td>2</td><td>1.0987392142417856</td><td><img src="http://windmissing.github.io/images/2019/123.png" alt="" /></td></tr>
<tr><td>10</td><td>1.050846676376417</td><td><img src="http://windmissing.github.io/images/2019/125.png" alt="" /></td></tr>
<tr><td>100</td><td>0.6880004678712686</td><td><img src="http://windmissing.github.io/images/2019/126.png" alt="" /></td></tr>
</tbody></table>
<p>这张图不是特别准确，因为这根曲线只是原有数据点连接出来的结果.<br />
因为有些x点取不到，不能准确描述所有点的y值，<br />
均匀取值x并绘制图像如下</p>
<p><img src="http://windmissing.github.io/images/2019/127.png" alt="" /></p>
<p>显示这不是我们想要的曲线。</p>
<p>结论：degree越高，对训练样本的拟合越好。<br />
因为当degree足够大，总能找到一根曲线拟合所有的样本点，使得均方误差为0.<br />
虽然拟合结果的均方误差小了，但它并没有真的反应样本点的曲线走势。<br />
它为了拟合所有给定的样本而变得太过复杂，这就是过拟合(over fitting)<br />
相反，如果只是使用一根直线来拟合样本数据，也没有很好的拟合样本的特征。<br />
但它不是太复杂了，而是太简单了，这就是欠拟合(under fitting)</p>
<div style="break-before: page; page-break-before: always;"></div><p>机器学习主要要解决的问题是过拟合问题。</p>
<p><img src="http://windmissing.github.io/images/2019/127.png" alt="" /></p>
<p>例如这根曲线，虽然这根曲线将样本点拟合得很好，总体误差很低，但如果来了一个新样本，它就不能进行很好的预测了。也就是说，这个模型的泛能力很弱。<br />
泛化能力即由此及彼的能力。<br />
我们训练模型不是为了这些已知的点，而是为也预测。因此模型对训练数据的拟合程度有多好是没有意义的。真正需要的是模型的泛化能力。</p>
<p>解决方法：<br />
训练、测试数据分离。<br />
模型只使用训练数据集获取。测试数据对于模型是全新的数据。<br />
如果模型对于测试数据也有很好的结果，就说模型的泛化能力是很强的。<br />
如果模型对训练数据集的效果好，而对测试数据的效果很差，就说模型的泛化能力很弱，通常是过拟合。</p>
<p>仍使用8-3的数据，degree取不同值时在test数据集上的效果对比</p>
<table><thead><tr><th>degree</th><th>在训练数据集上的MSE</th><th>在测试数据集上的MSE</th><th>note</th></tr></thead><tbody>
<tr><td>线性</td><td>3.0750025765636577</td><td>2.2199965269396573</td><td></td></tr>
<tr><td>2</td><td>1.0987392142417856</td><td>0.80356410562979</td><td>使用二次模型得到的泛化结果比使用线性模型得到的要好</td></tr>
<tr><td>10</td><td>1.050846676376417</td><td>0.9212930722150768</td><td>在训练数据集上（8-3）degree取10效果更好，但在测试数据集上degree取10的效果变差了。说明degree取10时它的泛化能力变弱了。</td></tr>
<tr><td>100</td><td>0.6880004678712686</td><td>14075796434.50641</td><td>在训练数据集上效果最好，在测试数据集上效果极差</td></tr>
</tbody></table>
<h1 id="测试数据集的意义"><a class="header" href="#测试数据集的意义">测试数据集的意义</a></h1>
<p>对于不同的算法，模型复杂度代码不同的意思。<br />
对于多项式回归算法来说，模型复杂度相当于degree，degree越高，模型越复杂。<br />
对于KNN算法来说，K越小，模型越复杂。</p>
<p><img src="http://windmissing.github.io/images/2019/128.jpg" alt="" /></p>
<p>如图所示：<br />
对于训练数据来说，模型越复杂，模型准确率就越高。<br />
对于测试数据来说，模型最简单时，测试数据的准确率会比较低，当模型变复杂时，训练数据的准确率会提升。随着模型越来越复杂，当准确率提升到一定程度时又开始下降。<br />
这就是“欠拟合-合适-过拟合”的过程。<br />
这只是一个示意图，不同模型得到的具体图像不同，但整体上是这样的趋势。</p>
<h1 id="欠拟合-vs-过拟合"><a class="header" href="#欠拟合-vs-过拟合">欠拟合 vs 过拟合</a></h1>
<p>欠拟合 underfitting<br />
算法所训练的模型不能完整表述数据关系。</p>
<p>过拟合 overfitting<br />
算法所训练的模型过多地表达了数据间的噪音关系。</p>
<p><img src="http://windmissing.github.io/images/2019/129.jpg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/129.jpg" alt="" /><br />
这是8-4中提到的模型复杂度曲线。用于说明过拟合和欠拟合。<br />
还有另一种曲线，也可以可视化地表达过拟合与欠拟合的情况，即学习曲线。</p>
<h1 id="学习曲线"><a class="header" href="#学习曲线">学习曲线</a></h1>
<p>随着训练样本的逐渐增多，算法训练出的模型的表现能力。</p>
<h1 id="欠拟合拟合过拟合和学习曲线图对比"><a class="header" href="#欠拟合拟合过拟合和学习曲线图对比">欠拟合、拟合、过拟合和学习曲线图对比</a></h1>
<p>仍使用8-4中的数据</p>
<p>绘制学习曲线的函数如下：</p>
<pre><code class="language-python">from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

def plot_learning_curve(algo, X_train, X_test, y_train, y_test):
    train_score = []
    test_score = []
    for i in range(1, len(X_train)+1):
        algo.fit(X_train[:i], y_train[:i])
        y_train_predict = algo.predict(X_train[:i])
        train_score.append(mean_squared_error(y_train[:i], y_train_predict))
        y_test_predict = algo.predict(X_test)
        test_score.append(mean_squared_error(y_test, y_test_predict))

    plt.plot([i for i in range(1, len(X_train)+1)], np.sqrt(train_score), label=&quot;train&quot;)
    plt.plot([i for i in range(1, len(X_train)+1)], np.sqrt(test_score), label=&quot;test&quot;)
    plt.legend()
    plt.axis([0, len(X_train)+1, 0, 4])
    plt.show()
</code></pre>
<h2 id="线性回归欠拟合"><a class="header" href="#线性回归欠拟合">线性回归，欠拟合</a></h2>
<pre><code class="language-python">plot_learning_curve( LinearRegression(), X_train, X_test, y_train, y_test)
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/130.png" alt="" /></p>
<p>在训练数据集上，误差逐渐升高。<br />
刚开始，误差累积较快，到后面误差累积变慢。<br />
在测试数据集上，刚开始误差很大，逐渐减小，减小到一定程度后达到相对稳定。<br />
最终，训练误差与测试误差趋于大体相同。测试误差略高于训练误差。</p>
<h2 id="2阶多项式回归拟合"><a class="header" href="#2阶多项式回归拟合">2阶多项式回归，拟合</a></h2>
<pre><code class="language-python">from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

def PolynomialRegression(degree):
    return Pipeline([
        (&quot;poly&quot;, PolynomialFeatures(degree=degree)),
        (&quot;std_scaler&quot;, StandardScaler()),
        (&quot;lin_reg&quot;, LinearRegression())
    ])

plot_learning_curve( PolynomialRegression(degree=2), X_train, X_test, y_train, y_test)
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/131.png" alt="" /></p>
<p>整体趋势与使用线性回归的图像是一致的。<br />
区别在于，线性回归模型中训练误差和测试误差稳定在1.7左右<br />
而2阶多项式回归模型中训练误差和测试误差稳定在1.0左右<br />
这说明使用2阶多项式回归的结果是比较好的。</p>
<h2 id="20阶多项式回归过拟合"><a class="header" href="#20阶多项式回归过拟合">20阶多项式回归，过拟合</a></h2>
<pre><code class="language-python">plot_learning_curve( PolynomialRegression(degree=20), X_train, X_test, y_train, y_test)
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/132.png" alt="" /></p>
<p>整体趋势仍然是train逐渐上升，test逐渐下降，最终趋于稳定。<br />
在区别是，在train和test都比较稳定时，它们之间的差距是比较大。<br />
这就说明模型虽然在训练数据集上拟合得非常好，但是在测试数据集上误差仍然很大。<br />
这种情况通常就是过拟合。</p>
<h1 id="总结"><a class="header" href="#总结">总结</a></h1>
<p><img src="http://windmissing.github.io/images/2019/133.jpg" alt="" />
欠拟合情况和最佳情况相比，欠拟合情况train、test曲线趋于稳定的位置比最佳情况的要高一些。这是因为模型选择得不对，所以即使在训练数据集上误差也很大。
<img src="http://windmissing.github.io/images/2019/134.jpg" alt="" />
对于过拟合情况，train曲线和最佳情况差不多，但test曲线比较高，并且train与test之间的差距比较大。</p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/135.jpg" alt="" /><br />
问题：发生了过拟合成不自知</p>
<p>解决方法：train test split<br />
<img src="http://windmissing.github.io/images/2019/136.jpg" alt="" /><br />
训练数据集：训练模型<br />
测试数据集：调整超参数<br />
问题：针对特定测试数据集过拟合？</p>
<p>解决方法：训练 - 验证 - 测试<br />
<img src="http://windmissing.github.io/images/2019/137.jpg" alt="" /></p>
<p>训练数据集：训练模型<br />
验证数据集：调整超参数<br />
测试数据集：不参数模型创建，作为衡量最终模型性能的数据集<br />
问题：随机？一旦验证数据集有异常数据，就会导致模型不准确</p>
<p>解决方法：交叉验证<br />
<img src="http://windmissing.github.io/images/2019/138.jpg" alt="" /></p>
<h1 id="代码实现-1"><a class="header" href="#代码实现-1">代码实现</a></h1>
<pre><code class="language-python">import numpy as np
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target
</code></pre>
<h2 id="测试train_test_split"><a class="header" href="#测试train_test_split">测试train_test_split</a></h2>
<pre><code class="language-python">from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=666)

from sklearn.neighbors import KNeighborsClassifier

best_score, best_k, best_p = 0,0,0
for k in range(2, 11):
    for p in range(1, 6):
        knn_clf = KNeighborsClassifier(weights=&quot;distance&quot;, n_neighbors=k, p=p)
        knn_clf.fit(X_train, y_train)
        score = knn_clf.score(X_test, y_test)
        if score &gt; best_score:
            best_score, best_k, best_p = score, k, p

print(&quot;best k = &quot;, best_k)
print(&quot;best p = &quot;, best_p)
print(&quot;best score = &quot;, best_score)
</code></pre>
<p>输出结果：<br />
best k =  3<br />
best p =  4<br />
best score =  0.9860917941585535</p>
<h2 id="使用交叉验证"><a class="header" href="#使用交叉验证">使用交叉验证</a></h2>
<pre><code class="language-python">from sklearn.model_selection import cross_val_score

knn_clf = KNeighborsClassifier()
cross_val_score(knn_clf, X_train, y_train)
</code></pre>
<p>输出结果：<br />
array([0.98895028, 0.97777778, 0.96629213])</p>
<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier

best_score, best_k, best_p = 0,0,0
for k in range(2, 11):
    for p in range(1, 6):
        knn_clf = KNeighborsClassifier(weights=&quot;distance&quot;, n_neighbors=k, p=p, cv=3)
        scores = cross_val_score(knn_clf, X_train, y_train)
        score = np.min(scores)
        if score &gt; best_score:
            best_score, best_k, best_p = score, k, p

print(&quot;best k = &quot;, best_k)
print(&quot;best p = &quot;, best_p)
print(&quot;best score = &quot;, best_score)
</code></pre>
<p>输出结果：<br />
best k =  2<br />
best p =  2<br />
best score =  0.9823599874006478</p>
<p>train_test_split和交叉验证结果对比：<br />
两种方法得到的best_k和best_p，通常认为使用交叉验证得到参数更可靠。<br />
因为方法一得到的结果很有可能只是过拟合了那一组测试数据。<br />
方法二的分数低于方法一，因为它没有过拟合某一组数据。</p>
<pre><code class="language-python">best_knn_clf = KNeighborsClassifier(weights=&quot;distance&quot;, n_neighbors=2, p=2)
best_knn_clf.fit(X_train, y_train)
best_knn_clf.score(X_test, y_test)
</code></pre>
<p>输出结果：0.980528511821975</p>
<p><strong>Note：这个算法最终的分类准确度不是上面的0.9823，而是这里的0.9805</strong></p>
<h1 id="回顾网格搜索"><a class="header" href="#回顾网格搜索">回顾网格搜索</a></h1>
<pre><code class="language-python">from sklearn.model_selection import GridSearchCV

param_grid = [
    {   
        'weights':['distance'],
        'n_neighbors': [i for i in range(2, 11)],
        'p': [i for i in range(1, 6)]
    }
]

from sklearn.neighbors import KNeighborsClassifier
knn_clf = KNeighborsClassifier()
grid_search = GridSearchCV(knn_clf, param_grid, cv=3)
grid_search.fit(X_train, y_train)
</code></pre>
<p>输入：<code>grid_search.best_score_</code><br />
输出：0.9823747680890538</p>
<p>输入：<code>grid_search.best_params_</code><br />
输出：{'n_neighbors': 2, 'p': 2, 'weights': 'distance'}</p>
<p>输入：</p>
<pre><code class="language-python">best_knn_clf = grid_search.best_estimator_
best_knn_clf.score(X_test, y_test)
</code></pre>
<p>输出：0.980528511821975</p>
<p>使用网络搜索与使用交叉验证得到的结果相同。</p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/139.jpg" alt="" /></p>
<p>把要解决的问题看作是这个靶盘，训练的模型看作是打出去的枪。</p>
<p>模型的误差 = 偏差 + 方差 + 不可避免的误差</p>
<h1 id="偏差-bias"><a class="header" href="#偏差-bias">偏差 Bias</a></h1>
<p>导致偏差的原因：<br />
对问题本身的假设不正确。如：非线性数据使用线性回归<br />
欠拟合 underfitting<br />
特征选择不正确</p>
<h1 id="方差--variance"><a class="header" href="#方差--variance">方差  Variance</a></h1>
<p>数据的一点点扰动都会较大地影响模型。<br />
通常原因，使用的模型太复杂，如高阶多项式回归<br />
过拟合 overfitting</p>
<h1 id="偏差和方差"><a class="header" href="#偏差和方差">偏差和方差</a></h1>
<p>有一些算法天生是高方差的算法。如KNN。<br />
非参数学习算法通常都是高方差算法，因为不对数据做任何假设。</p>
<p>有一些算法天生是高偏差算法。如线性回归。<br />
参数学习算法通常都是高偏差算法。因为对数据有极强的假设性。</p>
<p>大多数算法具有相应的参数，可以调整偏差和方差。  例如KNN中的K、如多项式回归中的degree。</p>
<p>偏差和方差通常是矛盾的。降低偏差，会提高方差。降低方差会提高偏差。</p>
<p>在算法层面上，机器学习的主要挑战来自方差。</p>
<p>解决高方差的通常手段：</p>
<ol>
<li>降低模型复杂度</li>
<li>减少数据维度，降噪</li>
<li>增加样本数</li>
<li>使用验证集</li>
<li>模型正则化</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><p>模型正则化：限制参数的大小</p>
<h1 id="一个过拟合的例子"><a class="header" href="#一个过拟合的例子">一个过拟合的例子：</a></h1>
<p><img src="http://windmissing.github.io/images/2019/127.png" alt="" /></p>
<p>这是8-3中的一个过拟合的例子，把模型的参数打出来如下：</p>
<p><img src="http://windmissing.github.io/images/2019/140.png" alt="" /></p>
<p>为了尽量地拟合数据，使得线条非常陡峭，数学上表示就是系数非常大</p>
<h1 id="岭回归-ridge-regularization"><a class="header" href="#岭回归-ridge-regularization">岭回归 Ridge Regularization</a></h1>
<p><img src="http://windmissing.github.io/images/2019/141.jpg" alt="" /></p>
<p><strong>Note 1: 正则项是从1累加到n的，theta 0不在里面，因为theta 0代表偏移，不是真正的系数。</strong><br />
<strong>Note 2：系数1/2加不加都可以，加了是为了求导方便。</strong><br />
<strong>Note 3：a是一个新的超参数，表示目标函数中模型正则化的程度。</strong></p>
<h1 id="代码实现-2"><a class="header" href="#代码实现-2">代码实现</a></h1>
<h2 id="测试数据-2"><a class="header" href="#测试数据-2">测试数据</a></h2>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)
x = np.random.uniform(-3.0, 3.0, size=100)
X = x.reshape(-1, 1)
y = 0.5 * x + 3 + np.random.normal(0, 1, size=100)

plt.scatter(X, y)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/142.png" alt="" /></p>
<h2 id="多项式回归degree--20"><a class="header" href="#多项式回归degree--20">多项式回归，degree = 20</a></h2>
<h3 id="训练模型-1"><a class="header" href="#训练模型-1">训练模型</a></h3>
<pre><code class="language-python">from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

def PolynomialRegression(degree):
    return Pipeline([
        (&quot;poly&quot;, PolynomialFeatures(degree=degree)),
        (&quot;std_scaler&quot;, StandardScaler()),
        (&quot;lin_reg&quot;, LinearRegression())
    ])

from sklearn.model_selection import train_test_split
np.random.seed(666)
X_train, X_test, y_train, y_test = train_test_split(X, y)

from sklearn.metrics import mean_squared_error

poly_reg = PolynomialRegression(degree=20)
poly_reg.fit(X_train, y_train)
</code></pre>
<h3 id="绘制模型"><a class="header" href="#绘制模型">绘制模型</a></h3>
<pre><code class="language-python">def plot_model(model):
    X_plot = np.linspace(-3, 3, 100).reshape(100, 1)
    y_plot = model.predict(X_plot)

    plt.scatter(x, y)
    plt.plot(X_plot[:,0], y_plot, color='r')
    plt.axis([-3, 3, 0, 6])
    plt.show()
</code></pre>
<h3 id="训练效果"><a class="header" href="#训练效果">训练效果</a></h3>
<pre><code class="language-python">y_predict = poly_reg.predict(X_test)
mean_squared_error(y_test, y_predict)   # MSE = 167.94010867772357
plot_model(poly_reg)
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/143.png" alt="" /></p>
<h2 id="岭回归degree20-alpha--00001"><a class="header" href="#岭回归degree20-alpha--00001">岭回归，degree=20, alpha = 0.0001</a></h2>
<h3 id="训练模型-2"><a class="header" href="#训练模型-2">训练模型</a></h3>
<pre><code class="language-python">from sklearn.linear_model import Ridge
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

def RidgeRegression(degree, alpha):
    return Pipeline([
        (&quot;poly&quot;, PolynomialFeatures(degree=degree)),
        (&quot;std_scaler&quot;, StandardScaler()),
        (&quot;ridge_reg&quot;, Ridge(alpha=alpha))
    ])

ridge1_reg = RidgeRegression(20, 0.0001)
ridge1_reg.fit(X_train, y_train)
</code></pre>
<h3 id="训练效果-1"><a class="header" href="#训练效果-1">训练效果</a></h3>
<pre><code class="language-python">y1_predict = ridge1_reg.predict(X_test)
mean_squared_error(y_test, y1_predict)   # MSE = 1.3233492754136291
plot_model(ridge1_reg)
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/144.png" alt="" /></p>
<h1 id="多项式回归及岭回归不同参数的训练结果比较"><a class="header" href="#多项式回归及岭回归不同参数的训练结果比较">多项式回归及岭回归不同参数的训练结果比较</a></h1>
<p>| MSE  | 拟合曲线
--|---|--
多项式回归，degree = 20  | 167.94010867772357  | <img src="http://windmissing.github.io/images/2019/143.png" alt="" />
岭回归，degree=20, alpha = 0.0001  | 1.3233492754136291  | <img src="http://windmissing.github.io/images/2019/144.png" alt="" />
岭回归，degree=20, alpha = 1  | 1.1888759304218461  |  <img src="http://windmissing.github.io/images/2019/145.png" alt="" />
岭回归，degree=20, alpha = 100  | 1.3196456113086197  | <img src="http://windmissing.github.io/images/2019/146.png" alt="" /><br />
岭回归，degree=20, alpha = 10000000  | 1.840889124848122  | <img src="http://windmissing.github.io/images/2019/147.png" alt="" /></p>
<p>当a非常大时，本质上成了优化正则表达项，即让所有theta=0</p>
<div style="break-before: page; page-break-before: always;"></div><p>LASSO的原理与Ridge相同，只是在怎么表达theta方面选择了一个不同的指标。</p>
<p><img src="http://windmissing.github.io/images/2019/148.jpg" alt="" /></p>
<p>LASSO: Least Absolute Shrinkage and Selection Operator Regression</p>
<h1 id="lasso的效果"><a class="header" href="#lasso的效果">LASSO的效果</a></h1>
<p>使用与8-8相同的测试数据</p>
<p><img src="http://windmissing.github.io/images/2019/142.png" alt="" /></p>
<h2 id="lasso回归degree--20"><a class="header" href="#lasso回归degree--20">LASSO回归，degree = 20</a></h2>
<h3 id="训练模型-3"><a class="header" href="#训练模型-3">训练模型</a></h3>
<pre><code class="language-python">from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso

def LassoRegression(degree, alpha):
    return Pipeline([
        (&quot;poly&quot;, PolynomialFeatures(degree=degree)),
        (&quot;std_scaler&quot;, StandardScaler()),
        (&quot;lasso_reg&quot;, Lasso(alpha=alpha))
    ])

from sklearn.model_selection import train_test_split
np.random.seed(666)
X_train, X_test, y_train, y_test = train_test_split(X, y)

from sklearn.metrics import mean_squared_error

lasso1_reg = LassoRegression(degree=20, alpha=0.01)
lasso1_reg.fit(X_train, y_train)

y1_predict = lasso1_reg.predict(X_test)
mean_squared_error(y_test, y1_predict)
</code></pre>
<p>MSE = 1.149608084325997</p>
<h3 id="绘制模型-1"><a class="header" href="#绘制模型-1">绘制模型</a></h3>
<pre><code class="language-python">def plot_model(model):
    X_plot = np.linspace(-3, 3, 100).reshape(100, 1)
    y_plot = model.predict(X_plot)

    plt.scatter(x, y)
    plt.plot(X_plot[:,0], y_plot, color='r')
    plt.axis([-3, 3, 0, 6])
    plt.show()

plot_model(lasso1_reg)
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/149.png" alt="" /></p>
<h1 id="degree20-多项式回归lasso回归alpha取不同参数的结果对比"><a class="header" href="#degree20-多项式回归lasso回归alpha取不同参数的结果对比">degree=20, 多项式回归、LASSO回归alpha取不同参数的结果对比</a></h1>
<p>| MSE  | 拟合曲线
--|---|--
多项式回归，degree = 20  | 167.94010867772357  | <img src="http://windmissing.github.io/images/2019/143.png" alt="" />
LASSO，degree=20，alpha=0.01  | 1.149608084325997  | <img src="http://windmissing.github.io/images/2019/149.png" alt="" /><br />
LASSO，degree=20，alpha=0.1  | 1.1213911351818648  |  <img src="http://windmissing.github.io/images/2019/150.png" alt="" /><br />
LASSO，degree=20，alpha=1  | 1.8408939659515595  | <img src="http://windmissing.github.io/images/2019/151.png" alt="" /></p>
<h1 id="ridge-vs-lasso"><a class="header" href="#ridge-vs-lasso">Ridge VS. LASSO</a></h1>
<p>在ridge算法中，随着a的增大，曲线越来越平缓，但它始终是一条曲线。<br />
但在lasso算法中，a取0.1时模型已经接近是直线了。<br />
使用lasso模型得到的更倾向于是一根直线。</p>
<p>直线与曲线的区别在于x的系数theta是否为0。使用lasso中会导致很多特征的系数为0。</p>
<p>lasso趋向于使得一部分theta值变为0，所以可作为特征选择用。<br />
因为如果使用lasso后一部分特征的theta变为0，就代表lasso认为这部分特征是没有用的。</p>
<p>造成ridge和lasso这种区别的原因与它们的导数形式有关。</p>
<p>lasso算法可能会错误地将有用的特征也变为0，从计算准确度角度讲，ridge更准确。<br />
但如果特征数量特别大，使用lasso可以让特征变少。</p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/152.png" alt="" /></p>
<p>将类似的思想应用于不同的场景，就成了不同的名词。<br />
其实这些名词背后的数学原理是非常想像的。</p>
<p><img src="http://windmissing.github.io/images/2019/153.png" alt="" /></p>
<p>在正则化过程中，通常不会使用n&gt;=3的正则项，但理论上是存在的。</p>
<p>L0正则项如下：<br />
<img src="http://windmissing.github.io/images/2019/154.png" alt="" /><br />
求L0正则项是一个NP难问题，所以也不使用。<br />
如果要限制theta的个数，则使用L1代替L0。</p>
<h1 id="弹性网-elastic-net"><a class="header" href="#弹性网-elastic-net">弹性网 Elastic Net</a></h1>
<p>超参数r表示两种正则项之间的比例<br />
同时结合了岭回归和lasso回归的优势<br />
在实际应用中通常都先尝试岭回归<br />
但如果特征特别多，岭回归的计算量会特别大，此时优先选择弹性网。<br />
lasso回归则急于将特征化为0，可能会产生一些错误，使得结果偏差较大。</p>
<h1 id="其他"><a class="header" href="#其他">其他</a></h1>
<p>批量梯度下降法 + 随机梯度下降法 = 小批量梯度下降法<br />
岭回归 + lasso回归 = 弹性网</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="什么是逻辑回归"><a class="header" href="#什么是逻辑回归">什么是逻辑回归</a></h1>
<p>解决分类问题。<br />
将样本的特征和样本发生的概率联系起来，概率是一个连续的数，因此可以当作回归问题来处理。</p>
<p>对于线性回归，y = f(x)<br />
对于逻辑回归，p = f(x)，当p&gt;=0.5,y=1，当p&lt;0.5,y=0</p>
<p>逻辑回归即可以看作是回归算法，也可以看作是分类算法。<br />
通常作为分类算法用，只可以解决二分类问题。</p>
<p>在线性回归中，
<img src="http://windmissing.github.io/images/2019/155.jpg" alt="" /><br />
y的值域为(-inifinity, inifinity)，但概率值的值域[0, 1]<br />
解决方法：<br />
<img src="http://windmissing.github.io/images/2019/156.jpg" alt="" /></p>
<h1 id="sigmoid函数"><a class="header" href="#sigmoid函数">Sigmoid函数</a></h1>
<p><img src="http://windmissing.github.io/images/2019/157.jpg" alt="" /></p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def sigmoid(t):
    return 1 / (1 + np.exp(-t))

x = np.linspace(-10, 10, 500)
y = sigmoid(x)

plt.plot(x, y)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/158.png" alt="" /></p>
<p>曲线的性质：<br />
左端趋近于但达不到0，右端趋近于但达不到1，即值域(0, 1)<br />
t&gt;0时，p&gt;0.5。t&lt;0时，p&lt;0.5</p>
<p><img src="http://windmissing.github.io/images/2019/159.jpg" alt="" /></p>
<p>问题：对于给定的样本数据集X,y，我们如何找到参数theta，使得用这样的方式可以最大程度获得样本数据集X对应的分类输出y？</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="什么是逻辑回归-1"><a class="header" href="#什么是逻辑回归-1">什么是逻辑回归</a></h1>
<p>解决分类问题。<br />
将样本的特征和样本发生的概率联系起来，概率是一个连续的数，因此可以当作回归问题来处理。</p>
<p>对于线性回归，y = f(x)<br />
对于逻辑回归，p = f(x)，当p&gt;=0.5,y=1，当p&lt;0.5,y=0</p>
<p>逻辑回归即可以看作是回归算法，也可以看作是分类算法。<br />
通常作为分类算法用，只可以解决二分类问题。</p>
<p>在线性回归中，
<img src="http://windmissing.github.io/images/2019/155.jpg" alt="" /><br />
y的值域为(-inifinity, inifinity)，但概率值的值域[0, 1]<br />
解决方法：<br />
<img src="http://windmissing.github.io/images/2019/156.jpg" alt="" /></p>
<h1 id="sigmoid函数-1"><a class="header" href="#sigmoid函数-1">Sigmoid函数</a></h1>
<p><img src="http://windmissing.github.io/images/2019/157.jpg" alt="" /></p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def sigmoid(t):
    return 1 / (1 + np.exp(-t))

x = np.linspace(-10, 10, 500)
y = sigmoid(x)

plt.plot(x, y)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/158.png" alt="" /></p>
<p>曲线的性质：<br />
左端趋近于但达不到0，右端趋近于但达不到1，即值域(0, 1)<br />
t&gt;0时，p&gt;0.5。t&lt;0时，p&lt;0.5</p>
<p><img src="http://windmissing.github.io/images/2019/159.jpg" alt="" /></p>
<p>问题：对于给定的样本数据集X,y，我们如何找到参数theta，使得用这样的方式可以最大程度获得样本数据集X对应的分类输出y？</p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/159.jpg" alt="" /></p>
<p>问题：对于给定的样本数据集X,y，我们如何找到参数theta，使得用这样的方式可以最大程度获得样本数据集X对应的分类输出y？</p>
<p>cost =<br />
{<br />
如果y = 1，p越小，cost越大<br />
如果y = 0，p越大，cost越大<br />
}</p>
<p>对应的数学公式与几何图像如下：<br />
<img src="http://windmissing.github.io/images/2019/160.jpg" alt="" /></p>
<p>再把以上函数合并成一个函数：</p>
<pre><code>cost = -y*log(p) - (1-y)*log(1-p)
</code></pre>
<p>根据所有样本得到的损失函数如下：<br />
<img src="http://windmissing.github.io/images/2019/161.jpg" alt="" /></p>
<p>找到一组theta，使得损失函数J(theta)最小。<br />
这个损失函数没有正规方程解，只能使用梯度下降法求解。<br />
这个损失函数是一个凸函数，只有一个最优解。</p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/162.jpg" alt="" /></p>
<p>线性回归算法的梯度：
<img src="http://windmissing.github.io/images/2019/163.jpg" alt="" />
逻辑回归算法的梯度：</p>
<p>$$
\nabla J(\theta) = \frac{1}{m} \cdot <br />
\begin{Bmatrix}
\sum_{i=1}^m (\hat y^{(i)}-y^{(i)}) \
\sum_{i=1}^m (\hat y^{(i)}-y^{(i)})\cdot X_1^{(i)} \
\sum_{i=1}^m (\hat y^{(i)}-y^{(i)})\cdot X_2^{(i)} \
... \
\sum_{i=1}^m (\hat y^{(i)}-y^{(i)})\cdot X_n^{(i)} \
\end{Bmatrix}   <br />
= \frac{1}{m}\cdot X_b^T\cdot (\sigma(X_b\theta)-y)
$$</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-python">import numpy as np
from sklearn.metrics import r2_score

class LogisticRegression:

    def __init__(self):
        &quot;&quot;&quot;初始化Linear Regression模型&quot;&quot;&quot;
        self.coef_ = None
        self.interception_ = None
        self._theta = None

    def _sigmoid(self, t):
            return 1. / (1. + np.exp(-t))

    def fit(self, X_train, y_train, eta=0.01, n_iters = 1e4):
        &quot;&quot;&quot;根据训练数据集X_train, y_train，使用梯度下降法训练Linear Regression模型&quot;&quot;&quot;
        assert X_train.shape[0] == y_train.shape[0], &quot;the size of X_train must be equal to the size of y_train&quot;



        def J(theta, X_b, y):
            y_hat = self._sigmoid(X_b.dot(theta))
            try:
                return np.sum(y*np.log(y_hat) + (1-y)*np.log(1-y_hat))
            except:
                return float('inf')

        def dJ(theta, X_b, y):
            return X_b.T.dot(self._sigmoid(X_b.dot(theta))-y) / len(X_b)

        def gradient_descent(X_b, y, initial_theta, eta, n_iters = 1e4, epsilon=1e-8):
            theta = initial_theta
            i_iter = 0
            while i_iter &lt; n_iters:
                gradient = dJ(theta, X_b, y)
                last_theta = theta
                theta = theta - eta * gradient
                if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon):
                    break
                i_iter += 1
            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])
        self._theta = gradient_descent(X_b, y_train, initial_theta, eta)
        self.interception_ = self._theta[0]
        self.coef_ = self._theta[1:]
        return self

    def predict_proba(self, X_predict):
        &quot;&quot;&quot;给定待预测数据集X_predict，返回表示X_predict的结果向量&quot;&quot;&quot;
        assert self.interception_ is not None and self.coef_ is not None, &quot;must fit before predict&quot;
        assert X_predict.shape[1] == len(self.coef_), &quot;the feature number of X_predict must equal to X_train&quot;

        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
        return self._sigmoid(X_b.dot(self._theta))

    def predict(self, X_predict):
        &quot;&quot;&quot;给定待预测数据集X_predict，返回表示X_predict的结果向量&quot;&quot;&quot;
        assert self.interception_ is not None and self.coef_ is not None, &quot;must fit before predict&quot;
        assert X_predict.shape[1] == len(self.coef_), &quot;the feature number of X_predict must equal to X_train&quot;

        proba = self.predict_proba(X_predict)
        return np.array(proba&gt;=0.5, dtype=int)

    def score(self, X_test, y_test):
        &quot;&quot;&quot;根据测试数据集X_test, y_test确定当前模型的准确度&quot;&quot;&quot;

        y_predict = self.predict(X_test)
        return r2_score(y_test, y_predict)

    def __repr__(self):
        return &quot;LogisticRegression()&quot;
</code></pre>
<h1 id="准备数据-10"><a class="header" href="#准备数据-10">准备数据</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data
y = iris.target
X = X[y&lt;2, :2]  # 逻辑回归只能解决二分类问题，因此只选取其中两种花的数据
y = y[y&lt;2]

plt.scatter(X[y==0,0],X[y==0,1], color='red')
plt.scatter(X[y==1,0],X[y==1,1], color='blue')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/165.png" alt="" /></p>
<h1 id="使用逻辑回归"><a class="header" href="#使用逻辑回归">使用逻辑回归</a></h1>
<pre><code class="language-python">from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

log_reg.score(X_test, y_test)
# 输出：1.0

log_reg.predict_proba(X_test)
# array([0.93292947, 0.98717455, 0.15541379, 0.01786837, 0.03909442,
#        0.01972689, 0.05214631, 0.99683149, 0.98092348, 0.75469962,
#        0.0473811 , 0.00362352, 0.27122595, 0.03909442, 0.84902103,
#        0.80627393, 0.83574223, 0.33477608, 0.06921637, 0.21582553,
#        0.0240109 , 0.1836441 , 0.98092348, 0.98947619, 0.08342411])

y_test
# array([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
#        1, 1, 0])

log_reg.predict(X_test)
# array([1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
#        1, 1, 0])
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/166.jpg" alt="" />
如果X有两个特征，则
<img src="http://windmissing.github.io/images/2019/167.jpg" alt="" />
决策边界为：
<img src="http://windmissing.github.io/images/2019/168.jpg" alt="" /></p>
<h1 id="绘制决策边界"><a class="header" href="#绘制决策边界">绘制决策边界</a></h1>
<p>仍使用9-4中的例子</p>
<pre><code class="language-python">def x2(x1):
    return (-log_reg.coef_[0] * x1 - log_reg.interception_) / log_reg.coef_[1]

x1_plot = np.linspace(4, 8, 1000)
x2_plot = x2(x1_plot)

plt.plot(x1_plot, x2_plot)
plt.scatter(X[y==0,0],X[y==0,1], color='red')
plt.scatter(X[y==1,0],X[y==1,1], color='blue')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/169.png" alt="" /></p>
<h1 id="不规则的决策边界的绘制方法"><a class="header" href="#不规则的决策边界的绘制方法">不规则的决策边界的绘制方法</a></h1>
<p><img src="http://windmissing.github.io/images/2019/170.jpg" alt="" /></p>
<pre><code class="language-python">def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1,1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1,1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])

    plt.contourf(x0, x1, zz, cmap=custom_cmap)
</code></pre>
<h2 id="逻辑回归的决策边界"><a class="header" href="#逻辑回归的决策边界">逻辑回归的决策边界</a></h2>
<pre><code class="language-python">plot_decision_boundary(log_reg, axis=[4,7.5,1.5,4.5])
plt.scatter(X[y==0,0],X[y==0,1], color='red')
plt.scatter(X[y==1,0],X[y==1,1], color='blue')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/171.png" alt="" /></p>
<h2 id="knn分类算法的决策边界"><a class="header" href="#knn分类算法的决策边界">KNN分类算法的决策边界</a></h2>
<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_train)

plot_decision_boundary(knn_clf, axis=[4,7.5,1.5,4.5])
plt.scatter(X[y==0,0],X[y==0,1], color='red')
plt.scatter(X[y==1,0],X[y==1,1], color='blue')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/172.png" alt="" /></p>
<h2 id="用knn对三种iris进行分类的决策边界"><a class="header" href="#用knn对三种iris进行分类的决策边界">用KNN对三种iris进行分类的决策边界</a></h2>
<pre><code class="language-python">knn_clf_all = KNeighborsClassifier()
knn_clf_all.fit(iris.data[:,:2], iris.target)

plot_decision_boundary(knn_clf_all, axis=[4,8,1.5,4.5])
plt.scatter(iris.data[iris.target==0,0],iris.data[iris.target==0,1], color='red')
plt.scatter(iris.data[iris.target==1,0],iris.data[iris.target==1,1], color='blue')
plt.scatter(iris.data[iris.target==2,0],iris.data[iris.target==2,1], color='green')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/173.png" alt="" /></p>
<p>上图中，黄色与蓝色之间的边界存在过拟合</p>
<h2 id="用knn对三种iris进行分类的决策边界-k50"><a class="header" href="#用knn对三种iris进行分类的决策边界-k50">用KNN对三种iris进行分类的决策边界， K=50</a></h2>
<pre><code class="language-python">knn_clf_all = KNeighborsClassifier(n_neighbors=50)
knn_clf_all.fit(iris.data[:,:2], iris.target)

plot_decision_boundary(knn_clf_all, axis=[4,8,1.5,4.5])
plt.scatter(iris.data[iris.target==0,0],iris.data[iris.target==0,1], color='red')
plt.scatter(iris.data[iris.target==1,0],iris.data[iris.target==1,1], color='blue')
plt.scatter(iris.data[iris.target==2,0],iris.data[iris.target==2,1], color='green')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/174.png" alt="" /></p>
<p>KNN模型中，k值越大，模型越简单</p>
<div style="break-before: page; page-break-before: always;"></div><p>直线这种分类方式太简单，如图这种情况，不可能使用一根直线把样本分成两部分，但它可以使用一个圆形来分割。<br />
因此，对于图中这个样本来说，决策边界应该是x1^2 + x2^2 -r^2 = 0。怎样得到这样的决策边界呢？<br />
解决方法：引入多项式项。</p>
<p><img src="http://windmissing.github.io/images/2019/175.jpg" alt="" /></p>
<h1 id="准备数据-11"><a class="header" href="#准备数据-11">准备数据</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
X = np.random.normal(0, 1, size=(200,2))
y = np.array(X[:,0]**2 + X[:,1]**2 &lt; 1.5, dtype='int')

plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/176.png" alt="" /></p>
<h1 id="使用逻辑回归-1"><a class="header" href="#使用逻辑回归-1">使用逻辑回归</a></h1>
<pre><code class="language-python">log_reg = LogisticRegression()   # 使用9-4中实现的LogisticRegression类
log_reg.fit(X, y)

log_reg.score(X, y)   # score = 0.605
plot_decision_boundary(log_reg, axis=[-4,4,-4,4])   # 使用9-5中的绘制决策边界的函数
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/177.png" alt="" /></p>
<h1 id="逻辑回归--多项式"><a class="header" href="#逻辑回归--多项式">逻辑回归 + 多项式</a></h1>
<pre><code class="language-python">from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler

def PolynomialLogisticRegression(degree):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg', LogisticRegression())   # 遵循sklearn标准构建的类可以无缝结合到管道中
    ])

poly_log_reg = PolynomialLogisticRegression(degree=2)
poly_log_reg.fit(X, y)
plot_decision_boundary(poly_log_reg, axis=[-4,4,-4,4])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
</code></pre>
<p>degree取2时的决策边界：
<img src="http://windmissing.github.io/images/2019/178.png" alt="" /><br />
degree取20时的决策边界：
<img src="http://windmissing.github.io/images/2019/179.png" alt="" /></p>
<p><strong>Note 1:代码中的LogisticRegression是在9-4中自己实现的类。只要是遵循sklearn标准构建的类可以无缝结合到管道中。</strong><br />
<strong>Note 2:逻辑回归中如果使用了多项式，也可以使用与多项式回归相同的正则表达式来约束过拟合的情况。</strong></p>
<div style="break-before: page; page-break-before: always;"></div><p>在逻辑回归中引入了多项式，模型就会变得复杂，容易出现过拟合。<br />
解决过拟合一个常规的手段就是在模型中添加正则化。<br />
新的目标函数可以是：<br />
J(theta) + a * L2或J(theta) + a * L1，其中a用于表示正则项的重要程度。<br />
但在逻辑回归中，通常这样正则化：<br />
C * J(theata) + L1或C * J(theata) + L2</p>
<h1 id="准备数据-12"><a class="header" href="#准备数据-12">准备数据</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

np.random.seed(666)
X = np.random.normal(0, 1, size=(200,2))
y = np.array(X[:,0]**2 + X[:,1]&lt;1.5, dtype='int')
for _ in range(20):
    y[np.random.randint(200)] = 1

plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/180.png" alt="" /></p>
<h1 id="使用逻辑回归-2"><a class="header" href="#使用逻辑回归-2">使用逻辑回归</a></h1>
<pre><code class="language-python">from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

log_reg.score(X_train, y_train)   # 0.79333
log_reg.score(X_test, y_test)     # 0.86

plot_decision_boundary(log_reg, [-4, 4, -4, 4])   # 使用8-6中的边界绘制算法
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/181.png" alt="" /></p>
<h1 id="逻辑回归--多项式-1"><a class="header" href="#逻辑回归--多项式-1">逻辑回归 + 多项式</a></h1>
<pre><code class="language-python">from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

def PolynomialLogisticRegression(degree, C=1.0, penalty='l2'):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('log_reg', LogisticRegression(C=C, penalty=penalty))   # 遵循sklearn标准构建的类可以无缝结合到管道中
    ])
</code></pre>
<h1 id="使用不同阶数和正则化算法和c值的结果对比"><a class="header" href="#使用不同阶数和正则化算法和c值的结果对比">使用不同阶数和正则化算法和C值的结果对比</a></h1>
<p>| <code>poly_log_reg.score(X_train, y_train)</code>  | <code>poly_log_reg.score(X_test, y_test)</code>  | <code>plot_decision_boundary(poly_log_reg, [-4, 4, -4, 4])</code>
--|---|---|--
degree=2  | 0.9133333333333333  | 0.94  | <img src="http://windmissing.github.io/images/2019/182.png" alt="" /><br />
degree=20  | 0.94  | 0.92  |  <img src="http://windmissing.github.io/images/2019/183.png" alt="" /><br />
degree=20, C=0.1  | 0.8533333333333334  | 0.92(泛化能力没有降低)  | <img src="http://windmissing.github.io/images/2019/184.png" alt="" /><br />
degree=20, C=0.1, penalty='l1'  | 0.8266666666666667  | 0.9  | <img src="http://windmissing.github.io/images/2019/185.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p>逻辑回归只能解决二分类问题<br />
解决方法：OvR（One vs Rest）， OvO（One vs One）</p>
<p><img src="http://windmissing.github.io/images/2019/186.jpg" alt="" /><br />
N个类型就进行N次分类，选择得分最高的<br />
对于逻辑回归，这里的分类是指分类的概率</p>
<p><img src="http://windmissing.github.io/images/2019/187.jpg" alt="" /><br />
N个类别就进行C(N,2)次分类，选择赢数最多的分类</p>
<p>OvO算法耗时更多，但分类更准确。</p>
<h1 id="使用logisticregression提供的ovr和ovo"><a class="header" href="#使用logisticregression提供的ovr和ovo">使用LogisticRegression提供的ovr和ovo</a></h1>
<h2 id="加载数据"><a class="header" href="#加载数据">加载数据</a></h2>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data[:, :2]
y = iris.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
</code></pre>
<h2 id="ovr"><a class="header" href="#ovr">OvR</a></h2>
<pre><code class="language-python">from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(multi_class='ovr')
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)  # 0.6578947368421053
plot_decision_boundary(log_reg, axis=[4,8,1.5,4.5])   # 见9-5
plt.scatter(iris.data[iris.target==0,0],iris.data[iris.target==0,1], color='red')
plt.scatter(iris.data[iris.target==1,0],iris.data[iris.target==1,1], color='blue')
plt.scatter(iris.data[iris.target==2,0],iris.data[iris.target==2,1], color='green')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/188.png" alt="" /></p>
<h2 id="ovo"><a class="header" href="#ovo">OvO</a></h2>
<pre><code class="language-python">from sklearn.linear_model import LogisticRegression
log_reg2 = LogisticRegression(multi_class='multinomial', solver='newton-cg')
log_reg2.fit(X_train, y_train)
log_reg2.score(X_test, y_test)   # 0.7894736842105263
plot_decision_boundary(log_reg2, axis=[4,8,1.5,4.5])
plt.scatter(iris.data[iris.target==0,0],iris.data[iris.target==0,1], color='red')
plt.scatter(iris.data[iris.target==1,0],iris.data[iris.target==1,1], color='blue')
plt.scatter(iris.data[iris.target==2,0],iris.data[iris.target==2,1], color='green')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/189.png" alt="" /></p>
<h2 id="使用所有数据"><a class="header" href="#使用所有数据">使用所有数据</a></h2>
<pre><code class="language-python">X = iris.data
y = iris.target
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(multi_class='ovr')
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)    # 0.9473684210526315

log_reg2 = LogisticRegression(multi_class='multinomial', solver='newton-cg')
log_reg2.fit(X_train, y_train)
log_reg2.score(X_test, y_test)   # 1.0
</code></pre>
<h1 id="使用scikit-learn中的ovr和ovo能把所有二分类算法转换成多分类算法"><a class="header" href="#使用scikit-learn中的ovr和ovo能把所有二分类算法转换成多分类算法">使用scikit-learn中的OvR和OvO，能把所有二分类算法转换成多分类算法</a></h1>
<pre><code class="language-python">log_reg = LogisticRegression()

from sklearn.multiclass import OneVsRestClassifier
ovr = OneVsRestClassifier(log_reg)
ovr.fit(X_train, y_train)
ovr.score(X_test, y_test)   # 0.9473684210526315

from sklearn.multiclass import OneVsOneClassifier
log_reg = LogisticRegression()
ovo = OneVsOneClassifier(log_reg)
ovo.fit(X_train, y_train)
ovo.score(X_test, y_test)   # 1.0
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>假设有一个算法，其预测某种癌症的准确率为99.9%。这个算法好吗？</p>
<p>999%的准确率看上去很高，但是如果这种癌症本身的发病率只有0.1%，即使不训练模型而直接预测所有人都是健康人，这样的预测的准确率也能达到99.9%。<br />
更极端的情况，如果这种癌症本身的发病率只有0.01%，这算法预测的准确率还不如直接预测所有人都健康。<br />
对于极度偏斜的数据（skewed data），用准确率评价分类算法好坏有局限性。<br />
解决方法：混淆矩阵</p>
<h1 id="工具混淆矩阵以二分类为例"><a class="header" href="#工具混淆矩阵以二分类为例">工具：混淆矩阵，以二分类为例</a></h1>
<p>0 - Negative - 阴性, 1 - Positive - 阳性<br />
1是我们关注的部分。</p>
<p>| 预测值：0  | 预测值：1
--|---|--
真实值：0  | 预测Negative正确<br><strong>TN</strong>  | 预测Positive错误<br><strong>FP</strong>
真实值：1  | 预测Negative错误<br><strong>FN</strong>  | 预测Positive正确<br><strong>TP</strong></p>
<div style="break-before: page; page-break-before: always;"></div><p>假设有一个算法，其预测某种癌症的准确率为99.9%。这个算法好吗？</p>
<p>999%的准确率看上去很高，但是如果这种癌症本身的发病率只有0.1%，即使不训练模型而直接预测所有人都是健康人，这样的预测的准确率也能达到99.9%。<br />
更极端的情况，如果这种癌症本身的发病率只有0.01%，这算法预测的准确率还不如直接预测所有人都健康。<br />
对于极度偏斜的数据（skewed data），用准确率评价分类算法好坏有局限性。<br />
解决方法：混淆矩阵</p>
<h1 id="工具混淆矩阵以二分类为例-1"><a class="header" href="#工具混淆矩阵以二分类为例-1">工具：混淆矩阵，以二分类为例</a></h1>
<p>0 - Negative - 阴性, 1 - Positive - 阳性<br />
1是我们关注的部分。</p>
<p>| 预测值：0  | 预测值：1
--|---|--
真实值：0  | 预测Negative正确<br><strong>TN</strong>  | 预测Positive错误<br><strong>FP</strong>
真实值：1  | 预测Negative错误<br><strong>FN</strong>  | 预测Positive正确<br><strong>TP</strong></p>
<div style="break-before: page; page-break-before: always;"></div><p>0 - Negative, 1 - Positive</p>
<p>| 预测值：0  | 预测值：1
--|---|--
真实值：0  | 预测Negative正确<br><strong>TN</strong>  | 预测Positive错误<br><strong>FP</strong>
真实值：1  | 预测Negative错误<br><strong>FN</strong>  | 预测Positive正确<br><strong>TP</strong></p>
<h1 id="指标精准率和召回率"><a class="header" href="#指标精准率和召回率">指标：精准率和召回率</a></h1>
<p>精准率 precision = TP / (TP + FP)<br />
召回率 recall = TP / (TP + FN)</p>
<p>在有偏数据中，将1作为真正关注的事件<br />
精准率：预测的关注事件（1）中预测对了的概率<br />
召回率：关注事件发生中被预测到概率</p>
<p>这是网上关于精准率召回率的说明图：<br />
<img src="http://windmissing.github.io/images/2019/190.jpg" alt="" /></p>
<p>回到9-1中的例子，如果某种癌症的发病率为0.1%，那么预测所有人都健康的模型，虽然准确率达到99.9%。但精准率没有意义，召回率为0，可见这个模型是个无效的模型。</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-python">import numpy as np
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target.copy()

y[digits.target==9] = 1
y[digits.target!=9] = 0    # 产生极度偏斜的数据

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
</code></pre>
<h1 id="准度度"><a class="header" href="#准度度">准度度</a></h1>
<pre><code class="language-python">log_reg.score(X_test, y_test)
</code></pre>
<p>输出：0.9755555555555555</p>
<h1 id="混淆矩阵"><a class="header" href="#混淆矩阵">混淆矩阵</a></h1>
<pre><code class="language-python">y_log_predict = log_reg.predict(X_test)

def TN(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 0) &amp; (y_predict==0))   # 注意这里是一个‘&amp;’

TN(y_test, y_log_predict)   # 403

def FP(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 0) &amp; (y_predict==1))

FP(y_test, y_log_predict)   # 2

def FN(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 1) &amp; (y_predict==0))

FN(y_test, y_log_predict)   # 9

def TP(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 1) &amp; (y_predict==1))

TP(y_test, y_log_predict)   # 36

def confusion_matrix(y_true, y_predict):
    return np.array([
        [TN(y_true, y_predict), FP(y_true, y_predict)],
        [FN(y_true, y_predict), TP(y_true, y_predict)]
    ])

confusion_matrix(y_test, y_log_predict)
</code></pre>
<p>输出结果：<br />
array([[403,   2], [  9,  36]])</p>
<h1 id="精准率"><a class="header" href="#精准率">精准率</a></h1>
<pre><code class="language-python">def precision_score(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fp = FP(y_true, y_predict)
    try:
        return tp / (tp + fp)
    except:   # 处理分母为0的情况
        return 0.0

precision_score(y_test, y_log_predict)
</code></pre>
<p>输出结果：0.9473684210526315</p>
<h1 id="召回率"><a class="header" href="#召回率">召回率</a></h1>
<pre><code class="language-python">def recall_score(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fn = FN(y_true, y_predict)

    try:
        return tp / (tp + fn)
    except:
        return 0.0

recall_score(y_test, y_log_predict)
</code></pre>
<p>输出结果：0.8</p>
<h1 id="scikit-learn中的混淆矩阵精准率召回率"><a class="header" href="#scikit-learn中的混淆矩阵精准率召回率">scikit-learn中的混淆矩阵、精准率、召回率</a></h1>
<pre><code class="language-python">from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_log_predict)

from sklearn.metrics import precision_score
precision_score(y_test, y_log_predict)

from sklearn.metrics import recall_score
recall_score(y_test, y_log_predict)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>对于极度偏斜的数据，使用指标精准率和召回率都优于使用指标分类准确度。<br />
但精准率和召回率是两个指标。如果一个算法的精准率和召回率表现不同，该如何取舍这两个指标？<br />
解决方法1：视具体场景而定。<br />
有时我们注重精准率，比如股票预测，我们希望预测股票为升结果都是准确的（否则可能亏钱），而不在意错过另一些错过的股票上升的机会（错过一些赚钱的机会）。<br />
有时我们注重召回率，比如病人诊断。我们希望得病的人都能识别出来（否则这些人可能会病情恶化），而有一些没得病的人被错误地识别出来没有关系（这些人做进一步检查即可）。<br />
解决方法2：同时关注精准率和召回率，即新指标：F1 score</p>
<h1 id="指标f1-score"><a class="header" href="#指标f1-score">指标：F1 score</a></h1>
<p>F1 score是precision和recall的调和平均值。<br />
<img src="http://windmissing.github.io/images/2019/191.jpg" alt="" /><br />
调和平均值的特点：如果precision和recall非常不平衡，则f1 score也是比较低的。只有两者都高，F1 score才会高。<br />
F1 score的取值范围：[0, 1]</p>
<h1 id="自己实现f1-score"><a class="header" href="#自己实现f1-score">自己实现F1 score</a></h1>
<pre><code class="language-python">import numpy as np
def f1_score(precision, recall):
    try:
        return 2 * precision * recall / (precision + recall)
    except:
        return 0.0
</code></pre>
<p>precision和recall取不同的值对f1_score的影响</p>
<table><thead><tr><th>precision</th><th>racall</th><th>f1_score</th></tr></thead><tbody>
<tr><td>0.5</td><td>0.5</td><td>0.5</td></tr>
<tr><td>0.1</td><td>0.9</td><td>0.18</td></tr>
<tr><td>0.0</td><td>1.0</td><td>0</td></tr>
</tbody></table>
<p>对于调和平均值来说，二者有一个分数较低，都会极大的拉低结果。<br />
因此能更好的表征precison和recal这两个指标。</p>
<h1 id="f1-score来评价手写数字的识别效果"><a class="header" href="#f1-score来评价手写数字的识别效果">F1 score来评价手写数字的识别效果</a></h1>
<p>使用10-3的测试数据</p>
<pre><code class="language-python">from sklearn.metrics import f1_score
f1_score(y_test, y_predict)
</code></pre>
<p>输出：0.8674698795180723</p>
<p>对有偏数据，指标F1 score优于分类准确度。</p>
<div style="break-before: page; page-break-before: always;"></div><p>我们总是希望精准率和召回率这两个指标都尽可能地高。但事实上精准率和召回率是互相矛盾的，我们只能在其中找到一个平衡。</p>
<p>以逻辑回归为例来说明精准率和召回率之间的矛盾关系，以下是逻辑回归的公式：<br />
$$
\begin{aligned}
\hat p = \sigma(\theta^T \cdot x_b) = \frac{1}{1 + \exp(-\theta^T \cdot x_b)}  \
\hat y = 
\begin{cases}
1, &amp;&amp; \hat p \ge 0.5   &amp;&amp;   \theta^T \cdot x_b \ge 0   &amp;&amp;  \text{决策边界}   \
0, &amp;&amp; \hat p \lt 0.5   &amp;&amp;   \theta^T \cdot x_b \lt 0   &amp;&amp;  \theta^T \cdot x_b = 0
\end{cases}
\end{aligned}
$$</p>
<p>在这里决策边界是以0为分界点，如果把0改成一个自定义的threshold，threshold的改变会平移决策边界，从而影响精准率和召回率的结果。<br />
$$
\theta^T \cdot x_b = \text{threshold}
$$</p>
<h1 id="threshold是怎样影响精准率和召回率的"><a class="header" href="#threshold是怎样影响精准率和召回率的">threshold是怎样影响精准率和召回率的</a></h1>
<p><img src="http://windmissing.github.io/images/2019/194.jpg" alt="" /><br />
如图，图中的直线代表决策边界，决策边界右边的样本分类为1，决策边界左边的样本分类为0。图中五角星为实际类别为1的样本，0为实际类别为0的样本。<br />
如果以0为分界点，精准率 = 4/5 = 80，召回率 = 4 / 6 = 0.67<br />
分界点往右移，则精准率提升，召回率降低。<br />
分界点往左移，则精准率下降，召回率提升。</p>
<p>用10-4中的Logic Regression对手写数字分类的例子来说明分界点移动对精准率和召回率的影响</p>
<h1 id="回顾10-4的代码"><a class="header" href="#回顾10-4的代码">回顾10-4的代码</a></h1>
<h2 id="准备数据-13"><a class="header" href="#准备数据-13">准备数据</a></h2>
<pre><code class="language-python">import numpy as np
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target.copy()

y[digits.target==9] = 1
y[digits.target!=9] = 0

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)
</code></pre>
<h2 id="训练模型-4"><a class="header" href="#训练模型-4">训练模型</a></h2>
<pre><code class="language-python">from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
</code></pre>
<h2 id="模型指标"><a class="header" href="#模型指标">模型指标</a></h2>
<pre><code class="language-python">log_reg.score(X_test, y_test)   #  0.9755555555555555

y_predict = log_reg.predict(X_test)

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_predict)   # array([[403,   2], [  9,  36]], dtype=int64)

from sklearn.metrics import precision_score
precision_score(y_test, y_predict)   # 0.9473684210526315

from sklearn.metrics import recall_score
recall_score(y_test, y_predict)   # 0.8

from sklearn.metrics import f1_score
f1_score(y_test, y_predict)   # 0.8674698795180723
</code></pre>
<h1 id="移动logic-regression的分界点"><a class="header" href="#移动logic-regression的分界点">移动Logic Regression的分界点</a></h1>
<h2 id="分析logic-regression当前使用的分界点"><a class="header" href="#分析logic-regression当前使用的分界点">分析Logic Regression当前使用的分界点</a></h2>
<p>上文中提到，通过调整threshold来移动决策边界，但sklearn并没有直接提供这样的接口。自带predict函数都是以0作为threshold的。<br />
但sklearn提供了决策函数，把X_test传进去，得到的是每个样本的score值。<br />
predict函数就是根据样本的score值来判断它的分类结果。</p>
<pre><code class="language-python">log_reg.decision_function(X_test)
</code></pre>
<p>部分输出截图：<br />
<img src="http://windmissing.github.io/images/2019/195.png" alt="" /></p>
<p>例如前10个样本的score值是这样的，那么它们的predict结果都应该为0</p>
<p><code>log_reg.decision_function(X_test)[:10]</code>与<code>log_reg.predict(X_test)[:10]</code>对比：<br />
array([-22.05700117, -33.02940957, -16.21334087, -80.3791447 ,<br />
-48.25125396, -24.54005629, -44.39168773, -25.04292757,<br />
-0.97829292, -19.7174399 ])<br />
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])</p>
<p>所以可以基于decision_function来移动决策边界。</p>
<pre><code class="language-python">decision_scores = log_reg.decision_function(X_test)
np.min(decision_scores)   # -85.68608522646575
np.max(decision_scores)   # 19.8895858799022
</code></pre>
<h2 id="移动threshold-0-5"><a class="header" href="#移动threshold-0-5">移动threshold: 0-&gt;5</a></h2>
<pre><code class="language-python">y_predict_2 = np.array(decision_scores &gt;= 5, dtype='int')
confusion_matrix(y_test, y_predict_2)   # array([[404,   1], [ 21,  24]], dtype=int64)
precision_score(y_test, y_predict_2)   # 0.96
recall_score(y_test, y_predict_2)   # 0.5333333333333333
</code></pre>
<h2 id="不同分界点的指标对比"><a class="header" href="#不同分界点的指标对比">不同分界点的指标对比</a></h2>
<table><thead><tr><th>threshold</th><th>confusion_matrix</th><th>precision_score</th><th>recall_score</th></tr></thead><tbody>
<tr><td>decision_scores &gt;= 0(default)</td><td>array([[403,   2], [  9,  36]], dtype=int64)</td><td>0.9473684210526315</td><td>0.8</td></tr>
<tr><td>decision_scores &gt;= 5</td><td>array([[404,   1], [ 21,  24]], dtype=int64)</td><td>0.96</td><td>0.5333333333333333</td></tr>
<tr><td>decision_scores &gt;= -5</td><td>array([[390,  15], [  5,  40]], dtype=int64)</td><td>0.7272727272727273</td><td>0.8888888888888888</td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><p>先回顾一下上一节课的代码</p>
<pre><code class="language-python">import numpy as np
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target.copy()

y[digits.target==9] = 1
y[digits.target!=9] = 0

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)
</code></pre>
<p>输出：0.9755555555555555</p>
<p>skleran的Logical Regression中，通过discision score和threshold来判断分类结果。<br />
默认情况下threshold = 0。<br />
调整threshold值，精准率和召回率就会相应的变化。<br />
这一节通过可视化的方式表现threshold和精准率、召回率之间的关系。</p>
<h1 id="精准率和召回率的变化曲线"><a class="header" href="#精准率和召回率的变化曲线">精准率和召回率的变化曲线</a></h1>
<pre><code class="language-python">decision_scores = log_reg.decision_function(X_test)

from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
import matplotlib.pyplot as plt

precision_scores = []
recall_scores = []

thresholds = np.arange(np.min(decision_scores), np.max(decision_scores), step=0.1)
for threshold in thresholds:
    y_predict = np.array(decision_scores &gt;= threshold, dtype='int')
    precision_scores.append(precision_score(y_test, y_predict))
    recall_scores.append(recall_score(y_test, y_predict))

plt.plot(thresholds, precision_scores)
plt.plot(thresholds, recall_scores)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/202.png" alt="" /><br />
可以根据这张图找到想要的threshold。</p>
<h1 id="绘制precision-recall曲线"><a class="header" href="#绘制precision-recall曲线">绘制precision-recall曲线</a></h1>
<pre><code class="language-python">plt.plot(precision_scores, recall_scores)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/197.png" alt="" /></p>
<h1 id="scikit-learn中的precision-recall曲线"><a class="header" href="#scikit-learn中的precision-recall曲线">scikit-learn中的precision-recall曲线</a></h1>
<pre><code class="language-python">from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds = precision_recall_curve(y_test, decision_scores)
plt.plot(thresholds, precisions[:-1])
plt.plot(thresholds, recalls[:-1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/196.png" alt="" /></p>
<p><strong>Note 1:precisions.shape = (145,)，recalls.shape = (145,)，thresholds.shape = (145,)，这是因为“the last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold.”</strong></p>
<p><strong>Note 2：sklearn提供的precision-recall曲线自动只寻找了我们最关心的那一部分。</strong></p>
<h1 id="关于precision-recall曲线的理论说明"><a class="header" href="#关于precision-recall曲线的理论说明">关于precision-recall曲线的理论说明</a></h1>
<p><img src="http://windmissing.github.io/images/2019/196.png" alt="" /><br />
<img src="http://windmissing.github.io/images/2019/197.png" alt="" /><br />
召回率急剧下降开始的点通常精准率-召回率最好的平衡点。<br />
<img src="http://windmissing.github.io/images/2019/198.jpg" alt="" /><br />
精准率-召回率曲线整体上是这样的曲线。用不同的算法或相同的算法的不同的超参数都能训练出各自的模型。每种模型都有不同的精准率-召回率曲线。<br />
假如如图是两个模型的精准率-召回率曲线，那么明显可以得出结论外面曲线的模型优于里面曲线的模型。因此PR曲线也可以作为选择模型/超参数的指标。</p>
<div style="break-before: page; page-break-before: always;"></div><p>ROC：Receiver Operation Characteristic Curve<br />
ROC曲线描述TPR和FPR之间的关系。</p>
<p>TPR = recall = TP / (TP + FN)  # true positive rate
FPR = FP / (FP + TN)           # false positive rate</p>
<p>TPR和FPR的关系如图：<br />
<img src="http://windmissing.github.io/images/2019/199.jpg" alt="" /><br />
TPR和FRP呈现相一致的趋势</p>
<h1 id="代码"><a class="header" href="#代码">代码</a></h1>
<h2 id="回顾前面学过的代码"><a class="header" href="#回顾前面学过的代码">回顾前面学过的代码</a></h2>
<pre><code class="language-python">def TN(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 0) &amp; (y_predict==0))   # 注意这里是一个‘&amp;’

def FP(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 0) &amp; (y_predict==1))

def FN(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 1) &amp; (y_predict==0))

def TP(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 1) &amp; (y_predict==1))

def confusion_matrix(y_true, y_predict):
    return np.array([
        [TN(y_true, y_predict), FP(y_true, y_predict)],
        [FN(y_true, y_predict), TP(y_true, y_predict)]
    ])

def precision_score(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fp = FP(y_true, y_predict)
    try:
        return tp / (tp + fp)
    except:   # 处理分母为0的情况
        return 0.0

def recall_score(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fn = FN(y_true, y_predict)

    try:
        return tp / (tp + fn)
    except:
        return 0.0
</code></pre>
<h2 id="tpr和fpr"><a class="header" href="#tpr和fpr">TPR和FPR</a></h2>
<pre><code class="language-python">def TPR(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fn = FN(y_true, y_predict)

    try:
        return tp / (tp + fn)
    except:
        return 0.0

def FPR(y_true, y_predict):
    fp = FP(y_true, y_predict)
    tn = TN(y_true, y_predict)

    try:
        return fp / (fp + tn)
    except:
        return 0.0
</code></pre>
<h2 id="加载测试数据"><a class="header" href="#加载测试数据">加载测试数据</a></h2>
<pre><code class="language-python">import numpy as np
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target.copy()

y[digits.target==9] = 1
y[digits.target!=9] = 0

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
</code></pre>
<h2 id="绘制tfp和frp的曲线即roc"><a class="header" href="#绘制tfp和frp的曲线即roc">绘制TFP和FRP的曲线，即ROC</a></h2>
<pre><code class="language-python">decision_scores = log_reg.decision_function(X_test)
import matplotlib.pyplot as plt

fprs = []
tprs = []

thresholds = np.arange(np.min(decision_scores), np.max(decision_scores), step=0.1)
for threshold in thresholds:
    y_predict = np.array(decision_scores &gt;= threshold, dtype='int')
    fprs.append(FPR(y_test, y_predict))
    tprs.append(TPR(y_test, y_predict))

plt.plot(fprs, tprs)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/203.jpg" alt="" /></p>
<h2 id="sklearn中的roc曲线"><a class="header" href="#sklearn中的roc曲线">sklearn中的ROC曲线</a></h2>
<pre><code class="language-phthon">from sklearn.metrics import roc_curve

fprs, tprs, thresholds = roc_curve(y_test, decision_scores)
plt.plot(fprs, tprs)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/203.jpg" alt="" />
我们通常关注这条曲线下面的面积。面积越大，说明分类的效果越好。</p>
<h2 id="roc-score"><a class="header" href="#roc-score">ROC score</a></h2>
<p>ROC score代码曲线下面的面积。<br />
auc = area under curl</p>
<pre><code class="language-python">from sklearn.metrics import roc_auc_score
roc_auc_score(y_test, decision_scores)
</code></pre>
<p>输出：0.9830452674897119</p>
<h1 id="总结-1"><a class="header" href="#总结-1">总结</a></h1>
<p>对于有偏数据，观察它的精准率和召回率是非常有必要的。<br />
但是ROC曲线对有偏数据并不敏感，它主要用于比较两个模型的孰优孰劣。</p>
<p><img src="http://windmissing.github.io/images/2019/200.jpg" alt="" /></p>
<p>如果两根曲线分别代码两个模型的ROC曲线，在这种情况下我们会选择外面那根曲线对应模型。</p>
<div style="break-before: page; page-break-before: always;"></div><p>前面小节就针对二分类问题的，这一节把这些指标扩展到多分类问题。</p>
<h1 id="加载手写数据集-1"><a class="header" href="#加载手写数据集-1">加载手写数据集</a></h1>
<pre><code class="language-python">import numpy as np
from sklearn import datasets

digits = datasets.load_digits()
X = digits.data
y = digits.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8,random_state=666)
</code></pre>
<h1 id="使用逻辑回归训练模型"><a class="header" href="#使用逻辑回归训练模型">使用逻辑回归训练模型</a></h1>
<pre><code class="language-python">from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)
</code></pre>
<p>输出：0.93115438108484</p>
<h1 id="计算精准率"><a class="header" href="#计算精准率">计算精准率</a></h1>
<pre><code class="language-python">y_predict = log_reg.predict(X_test)

from sklearn.metrics import precision_score
precision_score(y_test, y_predict)
</code></pre>
<p>会报错，因为默认只能为二分类问题计算精准率<br />
解决方法：
<img src="http://windmissing.github.io/images/2019/201.jpg" alt="" /></p>
<pre><code class="language-python">from sklearn.metrics import precision_score
precision_score(y_test, y_predict, average='micro')
</code></pre>
<p>输出：0.93115438108484</p>
<h1 id="计算混淆矩阵并可视化"><a class="header" href="#计算混淆矩阵并可视化">计算混淆矩阵并可视化</a></h1>
<h2 id="计算混淆矩阵"><a class="header" href="#计算混淆矩阵">计算混淆矩阵</a></h2>
<p>混淆矩阵天然支持多分类问题。</p>
<pre><code class="language-python">from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_predict)
</code></pre>
<p>输出：
<img src="http://windmissing.github.io/images/2019/204.png" alt="" /></p>
<p>第i行第j列代表真值为i而预测值为j的样本数量。<br />
对角线位置是预测正确的位置。<br />
把矩阵可视化，可以直观地看到犯错误比较多的地方。</p>
<h2 id="可视化"><a class="header" href="#可视化">可视化</a></h2>
<pre><code class="language-python">import matplotlib.pyplot as plt
cfm = confusion_matrix(y_test, y_predict)
plt.matshow(cfm, cmap=plt.cm.gray)   # 映射成灰度值
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/205.png" alt="" /></p>
<p>越亮的地方代表数值越大。<br />
图中对角线部分太亮，无法观察到我们关心的部分，改进如下：</p>
<pre><code class="language-python">row_sums = np.sum(cfm, axis=1)
err_matrix = cfm / row_sums
np.fill_diagonal(err_matrix, 0)
err_matrix
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/206.png" alt="" /></p>
<pre><code class="language-python">plt.matshow(err_matrix, cmap=plt.cm.gray)
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/207.png" alt="" />
图中越亮的地方表示犯错越多的地方。<br />
针对错误比较多的地方做算法的微调。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Support Vector Machine<br />
既可以解决分类问题，也可以解决回归问题。</p>
<h1 id="支撑向量机的思想"><a class="header" href="#支撑向量机的思想">支撑向量机的思想</a></h1>
<p><img src="http://windmissing.github.io/images/2019/208.jpg" alt="" /></p>
<p>图中是一个样本空间，里面有一些样本点，分成红色和蓝色两类。<br />
逻辑回归是要找到一根决策边界，由决策边界把数据分成两类。但可能存在这样一些数据（下图），可能存在多条决策边界。（不适定问题）<br />
<img src="http://windmissing.github.io/images/2019/209.jpg" alt="" /><br />
逻辑回归定义了一个损失函数，通过最小化损失函数求出决策边界。<br />
假设逻辑回归算法最后求出的是这样一根直线（下图），它在前面的所给的样本空间中表现很好，但新来这样一个样本，那么很有可能它的分类结果是错误的。<br />
<img src="http://windmissing.github.io/images/2019/210.jpg" alt="" /><br />
这根决策边界的泛化效果不够好，因为它离红色样本点太近了。那么怎样的决策边界泛化能力比较好呢？<br />
看上去下图这根直线的泛化能力要更好一点。<br />
<img src="http://windmissing.github.io/images/2019/213.jpg" alt="" /><br />
离这根直线最近的点有3个，让这3个点离这根直线的距离尽可能地远。<br />
也就是说，让这个决策边界既离红线尽可能远，也离蓝点尽可能远。于是得到了上图这样的决策边界。<br />
它不仅将训练数据划分开类，还期望它的泛化能力尽可能好。它把对泛化能力的考量直接放到了算法的内部。</p>
<p>把以上的思考用数学方式表达出来就是，让离决策边界最近的三个点，到决策边界的距离应该是一样的并且尽可能地大。<br />
基于这三个点又找两根与决策边界平行的直线。这两根直线定义了一个区域。这个区域中不再有任何点。<br />
<img src="http://windmissing.github.io/images/2019/211.jpg" alt="" /></p>
<h1 id="什么是支撑向量机"><a class="header" href="#什么是支撑向量机">什么是支撑向量机</a></h1>
<p>SVM尝试寻找一个最优的<strong>决策边界</strong>。<br />
这个决策边界距离两个类别的最近的样本最远。<br />
这三个最近的样本称为<strong>支撑向量</strong>。<br />
支撑向定义了一个区域，这个区域又定义了<strong>最优决策边界</strong>。<br />
上下两根直线离决策边界的距离都是d。<br />
上下两根直线之间的距离称为<strong>margin</strong>.<br />
SVM要最大化margin
<img src="http://windmissing.github.io/images/2019/212.jpg" alt="" /><br />
SVM只能解决线性可分的问题，即hard Margin SVM.
对hard Margin SVM改进，能解决线性不可分的数据，即soft Margin SVM</p>
<div style="break-before: page; page-break-before: always;"></div><p>Support Vector Machine<br />
既可以解决分类问题，也可以解决回归问题。</p>
<h1 id="支撑向量机的思想-1"><a class="header" href="#支撑向量机的思想-1">支撑向量机的思想</a></h1>
<p><img src="http://windmissing.github.io/images/2019/208.jpg" alt="" /></p>
<p>图中是一个样本空间，里面有一些样本点，分成红色和蓝色两类。<br />
逻辑回归是要找到一根决策边界，由决策边界把数据分成两类。但可能存在这样一些数据（下图），可能存在多条决策边界。（不适定问题）<br />
<img src="http://windmissing.github.io/images/2019/209.jpg" alt="" /><br />
逻辑回归定义了一个损失函数，通过最小化损失函数求出决策边界。<br />
假设逻辑回归算法最后求出的是这样一根直线（下图），它在前面的所给的样本空间中表现很好，但新来这样一个样本，那么很有可能它的分类结果是错误的。<br />
<img src="http://windmissing.github.io/images/2019/210.jpg" alt="" /><br />
这根决策边界的泛化效果不够好，因为它离红色样本点太近了。那么怎样的决策边界泛化能力比较好呢？<br />
看上去下图这根直线的泛化能力要更好一点。<br />
<img src="http://windmissing.github.io/images/2019/213.jpg" alt="" /><br />
离这根直线最近的点有3个，让这3个点离这根直线的距离尽可能地远。<br />
也就是说，让这个决策边界既离红线尽可能远，也离蓝点尽可能远。于是得到了上图这样的决策边界。<br />
它不仅将训练数据划分开类，还期望它的泛化能力尽可能好。它把对泛化能力的考量直接放到了算法的内部。</p>
<p>把以上的思考用数学方式表达出来就是，让离决策边界最近的三个点，到决策边界的距离应该是一样的并且尽可能地大。<br />
基于这三个点又找两根与决策边界平行的直线。这两根直线定义了一个区域。这个区域中不再有任何点。<br />
<img src="http://windmissing.github.io/images/2019/211.jpg" alt="" /></p>
<h1 id="什么是支撑向量机-1"><a class="header" href="#什么是支撑向量机-1">什么是支撑向量机</a></h1>
<p>SVM尝试寻找一个最优的<strong>决策边界</strong>。<br />
这个决策边界距离两个类别的最近的样本最远。<br />
这三个最近的样本称为<strong>支撑向量</strong>。<br />
支撑向定义了一个区域，这个区域又定义了<strong>最优决策边界</strong>。<br />
上下两根直线离决策边界的距离都是d。<br />
上下两根直线之间的距离称为<strong>margin</strong>.<br />
SVM要最大化margin
<img src="http://windmissing.github.io/images/2019/212.jpg" alt="" /><br />
SVM只能解决线性可分的问题，即hard Margin SVM.
对hard Margin SVM改进，能解决线性不可分的数据，即soft Margin SVM</p>
<div style="break-before: page; page-break-before: always;"></div><p>SVM要最大化margin，即图中两根直线之间的距离<br />
<img src="http://windmissing.github.io/images/2019/212.jpg" alt="" /><br />
用数学语言来表达:  margin = 2 * d，最大化d就是最大化margin</p>
<p>点x到直线$$w^T * x + b = 0$$的<a href="https://windmising.gitbook.io/mathematics-basic-for-ml/gao-deng-shu-xue/distance">距离</a>为：<br />
$$
distance = \frac{|w^T+b|}{||w||}  \
||w|| = \sqrt {w_1^2+w_2^2+\cdots+w_n^2}
$$
所有的样本点到决策边界到距离都应该大于d，用于公式表达：<br />
<img src="http://windmissing.github.io/images/2019/215.jpg" alt="" /><br />
上面这个公式可写成这样的形式：<br />
<img src="http://windmissing.github.io/images/2019/216.jpg" alt="" /><br />
于是可得出margin上下两条直线的方程为：<br />
<img src="http://windmissing.github.io/images/2019/217.jpg" alt="" /><br />
**注意：这里三条直线中的已经不是原来的$$w^T$$和b了，$$w_b^T=w^T/(||w||<em>d), b_d=b/(||w||<em>d)$$</em></em>
SVM的目标是最大化d，d的公式在上文已经给出。<br />
由于支持向量x一定是在margin的上下边界点上，可以证明对于任意支持向量x，以下四个公式表达的目标的相同的：<br />
<img src="http://windmissing.github.io/images/2019/218.jpg" alt="" /><br />
结论：
SVM算法演变为有条件的最优化问题，（st：条件）
<img src="http://windmissing.github.io/images/2019/219.jpg" alt="" /><br />
有条件的最优化问题和没有条件的最优化问题，其求解方法大不相同。</p>
<div style="break-before: page; page-break-before: always;"></div><p>hard Margin:<br />
<img src="http://windmissing.github.io/images/2019/220.jpg" alt="" /><br />
使用这种方法，对下面这张图中的样本，hard SVM会给出这样一根决策边界<br />
<img src="http://windmissing.github.io/images/2019/221.jpg" alt="" /><br />
hard SVM非常明显地受到一个蓝点的影响，而这个蓝点很有可能是outlier或者是个错误点。<br />
<img src="http://windmissing.github.io/images/2019/222.jpg" alt="" /><br />
这根决策边界可能是一根更好的决策边界。<br />
而对于下图这样的样本点，线性不可分的情况，hard SVN是无解的。<br />
<img src="http://windmissing.github.io/images/2019/223.jpg" alt="" /><br />
hard SVM需要更好的容错性。<br />
改进：增加一个宽松量</p>
<p><img src="http://windmissing.github.io/images/2019/224.jpg" alt="" /><br />
即允许一些点出现在图中虚线和实线之间的位置。<br />
这里的宽松量不是一个定值，它对每一个样本都是不同的。<br />
同时还要最小化所有的宽松量。<br />
目标函数变成了：<br />
<img src="http://windmissing.github.io/images/2019/225.jpg" alt="" /></p>
<h1 id="l1正则l2正则"><a class="header" href="#l1正则l2正则">L1正则、L2正则</a></h1>
<p><img src="http://windmissing.github.io/images/2019/226.jpg" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p>和KNN一样，使用SVM之前要做数据标准化处理，因为SVM算法涉及距离。<br />
尺度不平衡的例子：<br />
<img src="http://windmissing.github.io/images/2019/227.jpg" alt="" /><br />
数据标准化之后：<br />
<img src="http://windmissing.github.io/images/2019/228.jpg" alt="" /></p>
<h1 id="准备数据-14"><a class="header" href="#准备数据-14">准备数据</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets

iris = datasets.load_iris()
X = iris.data
y = iris.target

X = X[y&lt;2,:2]
y = y[y&lt;2]

plt.scatter(X[y==0,0],X[y==0,1], color='red')
plt.scatter(X[y==1,0],X[y==1,1], color='blue')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/165.png" alt="" /></p>
<h1 id="数据标准化"><a class="header" href="#数据标准化">数据标准化</a></h1>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler

standardScaler = StandardScaler()
standardScaler.fit(X)
X_standard = standardScaler.transform(X)
</code></pre>
<h1 id="训练hard-svn模型"><a class="header" href="#训练hard-svn模型">训练hard SVN模型</a></h1>
<pre><code class="language-python">from sklearn.svm import LinearSVC

# Support Vector Classifier
svc = LinearSVC(C=1e9)  # C 越大，越hard
svc.fit(X_standard, y)
</code></pre>
<h1 id="分类效果"><a class="header" href="#分类效果">分类效果</a></h1>
<pre><code class="language-python">def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1,1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1,1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])

    plt.contourf(x0, x1, zz, cmap=custom_cmap)

plot_decision_boundary(svc, axis=[-3,3,-3,3])
plt.scatter(X_standard[y==0,0],X_standard[y==0,1], color='red')
plt.scatter(X_standard[y==1,0],X_standard[y==1,1], color='blue')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/229.png" alt="" /></p>
<h1 id="训练soft-svn-模型"><a class="header" href="#训练soft-svn-模型">训练soft SVN 模型</a></h1>
<pre><code class="language-python">svc2 = LinearSVC(C=0.01)
svc2.fit(X_standard, y)

plot_decision_boundary(svc2, axis=[-3,3,-3,3])
plt.scatter(X_standard[y==0,0],X_standard[y==0,1], color='red')
plt.scatter(X_standard[y==1,0],X_standard[y==1,1], color='blue')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/230.png" alt="" />
图中有一个点被错误地分类了，这是soft的效果</p>
<h1 id="绘制margin"><a class="header" href="#绘制margin">绘制margin</a></h1>
<p>输入：svc.coef_<br />
输出：array([[ 4.03240038, -2.50701084]])<br />
样本中有两个特征，所以有2个系数，每个特征对应一个<br />
输出是一个二维数组，因为sklearn提供的SVM算法可以处理多分类问题</p>
<p>输入：svc.intercept_<br />
输出：array([0.92736326])</p>
<pre><code class="language-python">def plot_svc_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1,1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1,1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])

    plt.contourf(x0, x1, zz, cmap=custom_cmap)

    # 取出model的系数，只取第0个决策边界
    w = model.coef_[0]
    b = model.intercept_[0]

    # 决策边界的直线方程：w0 * x0 + x1 * x1 + b = 0
    # 决策边界的斜率和截距 =&gt; x1 = -w0/w1 * x0 - b/w1
    plot_x = np.linspace(axis[0], axis[1], 200) # 绘制用的x
    up_y = -w[0]/w[1] * plot_x - b/w[1] + 1/w[1] # w0 * x0 + x1 * x1 + b = 1
    down_y = -w[0]/w[1] * plot_x - b/w[1] - 1/w[1] # w0 * x0 + x1 * x1 + b = -1
    # 过滤，防止y超出图像边界
    up_index = (up_y &gt;= axis[2]) &amp; (up_y &lt;= axis[3])
    down_index = (down_y &gt;= axis[2]) &amp; (down_y &lt;= axis[3])
    plt.plot(plot_x[up_index], up_y[up_index], color='black')
    plt.plot(plot_x[down_index], down_y[down_index], color='black')
</code></pre>
<p>svc的margin</p>
<pre><code class="language-python">plot_svc_decision_boundary(svc, axis=[-3,3,-3,3])
plt.scatter(X_standard[y==0,0],X_standard[y==0,1], color='red')
plt.scatter(X_standard[y==1,0],X_standard[y==1,1], color='blue')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/231.png" alt="" /></p>
<p>svc2的margin</p>
<pre><code class="language-python">plot_svc_decision_boundary(svc2, axis=[-3,3,-3,3])
plt.scatter(X_standard[y==0,0],X_standard[y==0,1], color='red')
plt.scatter(X_standard[y==1,0],X_standard[y==1,1], color='blue')
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/232.png" alt="" /></p>
<p><strong>Note 1:sklarn提供的SVM算法支持多分类，默认使用ovr算法</strong>
<strong>Note 2:sklarn提供的SVM算法支持正则化，默认使用L2范式</strong></p>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons()
# X.shape = (100, 2)
# y.shape = (100,)

plot_decision_boundary(poly_svc, axis=[-1.5,2.5,-1.0,1.5])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/233.png" alt="" /></p>
<pre><code class="language-python">X, y = datasets.make_moons(noise=0.15, random_state=666) # 0.15可以理解为标准差

plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/234.png" alt="" /></p>
<h1 id="使用多项式特征的svm"><a class="header" href="#使用多项式特征的svm">使用多项式特征的SVM</a></h1>
<h2 id="训练模型-5"><a class="header" href="#训练模型-5">训练模型</a></h2>
<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

def PolynomialSVC(degree, C=1.0):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scaler', StandardScaler()),
        ('linearSVC', LinearSVC(C=C))
    ])

poly_svc = PolynomialSVC(degree=3)
poly_svc.fit(X, y)
</code></pre>
<h2 id="绘制模型-2"><a class="header" href="#绘制模型-2">绘制模型</a></h2>
<pre><code class="language-python">def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1,1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1,1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])

    plt.contourf(x0, x1, zz, cmap=custom_cmap)

plot_decision_boundary(poly_svc, axis=[-1.5,2.5,-1.0,1.5])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/235.png" alt="" /></p>
<h1 id="使用多项式核函数的svm"><a class="header" href="#使用多项式核函数的svm">使用多项式核函数的SVM</a></h1>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline

def PolynomialKernelSVC(degree, C=1.0):
    return Pipeline([
        ('std_scaler', StandardScaler()),
        ('kernelSVC', SVC(C=C, kernel='poly', degree=degree))
    ])

poly_kernel_svc = PolynomialKernelSVC(degree=3)
poly_kernel_svc.fit(X, y)

plot_decision_boundary(poly_kernel_svc, axis=[-1.5,2.5,-1.0,1.5])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/236.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/225.png" alt="" /></p>
<p>这是11-3中推导出来的公式。MVC本质上是这样的一个带条件的最优化问题。<br />
在求解这个问题的过程要，要把公式变形：</p>
<p><img src="http://windmissing.github.io/images/2019/237.jpg" alt="" /></p>
<p>公式中红框标出来的部分，配合前面了两层累加符号，表示样本中任意两个x相乘。<br />
所谓的给SVM增加多项式，就是：<br />
<img src="http://windmissing.github.io/images/2019/238.jpg" alt="" /><br />
把x做一些变形得到x'，并且用x'代替公式中的x<br />
核函数的思想是：有没有可能不显式地将x转换成x'，而是直接使用一个函数得到<br />
<img src="http://windmissing.github.io/images/2019/239.jpg" alt="" /></p>
<p>那么SVM的目标函数就可以写成：<br />
<img src="http://windmissing.github.io/images/2019/240.jpg" alt="" /></p>
<p>K函数 = Kernel function = Kernel trick<br />
优点：1.节省了存储空间2.计算量变少</p>
<p>核函数本身不是SVM独有的技巧。只要算法转成了最优化问题，并且在求解最优化问题的过程中存在xi * xj或者类似这样的式子，都可以应该核函数技巧。</p>
<h1 id="多项式核函数"><a class="header" href="#多项式核函数">多项式核函数</a></h1>
<p><img src="http://windmissing.github.io/images/2019/241.jpg" alt="" /><br />
相当于给原来的x添加了二次项。<br />
系数前面的常数项根号2不会影响模型的训练效果。</p>
<p>多项式的核函数：<br />
<img src="http://windmissing.github.io/images/2019/243.jpg" alt="" /><br />
下图是sklearn的SVM的文档，当核函数指定为poly时，degree相当于公式中的d，coef0相当于公式中的c
<img src="http://windmissing.github.io/images/2019/242.jpg" alt="" /></p>
<p>线性核函数：K(x, y) = x * y</p>
<div style="break-before: page; page-break-before: always;"></div><p>K(x, y)表示x和y的点乘。高斯核函数的公式为：<br />
<img src="http://windmissing.github.io/images/2019/244.jpg" alt="" /><br />
公式中的$$\gamma$$是高斯核函数中唯一的超参数。<br />
高斯核函数 = RBF核 = Radial Basis Function Kernel = 镜像基函数<br />
高斯核函数本质是将每个样本点映射到一个无穷维的特征空间</p>
<h1 id="多项式特征为什么能处理线性不可分的问题"><a class="header" href="#多项式特征为什么能处理线性不可分的问题">多项式特征为什么能处理线性不可分的问题？</a></h1>
<p>依靠升维使得原本线性不可分的数据线性可分<br />
例如有一组原本线性不可分的一维数据：
<img src="http://windmissing.github.io/images/2019/245.jpg" alt="" /><br />
给数据增加一个内容为x^2的维度，数据就变成了这样：
<img src="http://windmissing.github.io/images/2019/246.jpg" alt="" />
现在很容易找到一根直线把两类数据区分开：
<img src="http://windmissing.github.io/images/2019/247.jpg" alt="" /></p>
<h1 id="使用高斯核函数升维"><a class="header" href="#使用高斯核函数升维">使用高斯核函数升维</a></h1>
<p>再来看复用高斯核函数的例子。为了方便可视化，将原K做一些改变。<br />
在原K中，是x相对y的映射，改成x相对两个固定的点的映射。<br />
这两个固定的点就是图中的三角形位置。  l:landmark，地标
<img src="http://windmissing.github.io/images/2019/248.jpg" alt="" /></p>
<h2 id="代码模拟高斯核函数的映射效果"><a class="header" href="#代码模拟高斯核函数的映射效果">代码模拟高斯核函数的映射效果</a></h2>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

x = np.arange(-4, 5, 1)   # x = array([-4, -3, -2, -1,  0,  1,  2,  3,  4])
y = np.array((x&gt;=-2) &amp; (x&lt;=2), dtype='int')   # y = array([0, 0, 1, 1, 1, 1, 1, 0, 0])

plt.scatter(x[y==0], [0]*len(x[y==0]))
plt.scatter(x[y==1], [0]*len(x[y==1]))
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/249.png" alt="" /></p>
<pre><code class="language-python">def gaussian(x, l):
    gamma = 1.0
    return np.exp(-gamma * (x-l)**2)

l1, l2 = -1, 1

X_new = np.empty((len(x),2))
for i, data in enumerate(x):
    X_new[i, 0] = gaussian(data, l1)
    X_new[i, 1] = gaussian(data, l2)

plt.scatter(X_new[y==0,0], X_new[y==0,1])
plt.scatter(X_new[y==1,0], X_new[y==1,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/250.png" alt="" /></p>
<p>这样就得到了一个线性可分的结果：</p>
<p><img src="http://windmissing.github.io/images/2019/251.jpg" alt="" /></p>
<h2 id="解释"><a class="header" href="#解释">解释</a></h2>
<p><img src="http://windmissing.github.io/images/2019/248.jpg" alt="" /></p>
<p>在这个例子中，使用图中的公式2来对x做映射，公式中的l1和l2是地标。<br />
但实际的高斯公式是公式1，公式中的y是每一个数据点，也就是说，在高斯核函数中，每个样本都是一个地标landmark。它将<code>m*n</code>的数据映射成了<code>m*m</code>的数据。<br />
使用高斯核函数训练样本，计算量非常大，训练时间也很长。<br />
当m &lt; n时，适用使用高斯核函数，例如自然语言处理。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gamma参数的意义"><a class="header" href="#gamma参数的意义">gamma参数的意义</a></h1>
<p><img src="http://windmissing.github.io/images/2019/252.jpg" alt="" /></p>
<h1 id="加载数据-1"><a class="header" href="#加载数据-1">加载数据</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons()
X, y = datasets.make_moons(noise=0.15, random_state=666)

plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/234.png" alt="" /></p>
<h1 id="训练高斯核的svm"><a class="header" href="#训练高斯核的svm">训练高斯核的SVM</a></h1>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline

def RBFKernelSVC(gamma=1.0):
    return Pipeline([
        ('std_scaler', StandardScaler()),
        ('rbfSVC', SVC(kernel='rbf', gamma= gamma))
    ])

svc = PolynomialKernelSVC()
svc.fit(X, y)
</code></pre>
<h1 id="绘制决策边界-1"><a class="header" href="#绘制决策边界-1">绘制决策边界</a></h1>
<pre><code class="language-python">def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1,1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1,1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])

    plt.contourf(x0, x1, zz, cmap=custom_cmap)

plot_decision_boundary(svc, axis=[-1.5,2.5,-1.0,1.5])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/253.png" alt="" /></p>
<h1 id="gamma取不同值的效果对比"><a class="header" href="#gamma取不同值的效果对比">gamma取不同值的效果对比</a></h1>
<table><thead><tr><th><img src="http://windmissing.github.io/images/2019/253.png" alt="" /></th><th><img src="http://windmissing.github.io/images/2019/254.png" alt="" /></th><th><img src="http://windmissing.github.io/images/2019/255.png" alt="" /></th><th><img src="http://windmissing.github.io/images/2019/256.png" alt="" /></th><th><img src="http://windmissing.github.io/images/2019/257.png" alt="" /></th></tr></thead><tbody>
<tr><td>gamma=1.0</td><td>gamma=100</td><td>gamma=0.5</td><td>gamma=0.1</td><td>gamma=10</td></tr>
</tbody></table>
<p>可以把训练的效果看成是俯视这些样本点。某一类的每个样本点形成了一个以它为中心的正态分布。<br />
gamma越大，这个分布的圈就越小。</p>
<div style="break-before: page; page-break-before: always;"></div><p>回归问题是指找到一根直线/曲线最大程度的拟合样本点。<br />
不同的回归算法对“拟合”有不同的理解。<br />
例如线性回归算法定义“拟合”为：所有样本点到直线的MSE最小<br />
而SVM将“拟合”定义为：尽可能多的点被包含在margin范围内，取margin中间的直线。（与解决分类问题相反的思路）<br />
margin的距离为超参数
<img src="http://windmissing.github.io/images/2019/258.jpg" alt="" /></p>
<h1 id="加载数据-2"><a class="header" href="#加载数据-2">加载数据</a></h1>
<pre><code class="language-python">import numpy as np
from sklearn import datasets

boston = datasets.load_boston()
x = boston.data
y = boston.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=666)
</code></pre>
<h1 id="训练模型-6"><a class="header" href="#训练模型-6">训练模型</a></h1>
<pre><code class="language-python">from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from sklearn.svm import LinearSVR
from sklearn.pipeline import Pipeline

# SVR = Support Vector Regression
# SVR和SVC的用法基本上一样
def StandardLinearSVR(epsilon=0.1):
    return Pipeline([
        ('std_scaler', StandardScaler()),
        ('linearSVR', LinearSVR(epsilon=epsilon))
    ])

svr = StandardLinearSVR()
svr.fit(X_train, y_train)
svr.score(X_test, y_test)
</code></pre>
<p>输出结果：0.6358140020058993</p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/261.jpg" alt="" />
假设上图是某公司招聘算法工程师的过程。<br />
图中的叶子节点代码决策，在寻找决策过程是形成了树是决策树。<br />
这棵树的深度为3，因为最多经过3次判断就能做出决策。</p>
<h1 id="使用sklearn中的决策树"><a class="header" href="#使用sklearn中的决策树">使用sklearn中的决策树</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data[:, 2:]
y = iris.target

plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.scatter(X[y==2,0],X[y==2,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/260.png" alt="" /></p>
<pre><code class="language-python">from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier(max_depth=2, criterion='entropy') # entropy = 熵
dt_clf.fit(X, y)

plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.scatter(X[y==2,0],X[y==2,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/259.png" alt="" /><br />
<em>这个分类结果与视频中的分类结果不同</em></p>
<h1 id="总结-2"><a class="header" href="#总结-2">总结</a></h1>
<h2 id="决策树怎样做决策的"><a class="header" href="#决策树怎样做决策的">决策树怎样做决策的</a></h2>
<p><img src="http://windmissing.github.io/images/2019/262.jpg" alt="" />
当属性是数值特征时，在每一个节点上，选择某一个维度上的数值，以它为域值将样本分成两类。</p>
<h2 id="决策树的特点"><a class="header" href="#决策树的特点">决策树的特点</a></h2>
<ol>
<li>是非参数学习算法</li>
<li>可以分类问题</li>
<li>天然可以解决多分类问题</li>
<li>也可以解决回归问题（用分类算法将样本归到某个叶子上，该叶子上所有样本的平均值即输出标记）</li>
<li>有非常好的可解释性</li>
</ol>
<h2 id="怎样创建决策树"><a class="header" href="#怎样创建决策树">怎样创建决策树</a></h2>
<ol>
<li>每个结点在哪个维度做划分？</li>
<li>某个维度的哪个值上做划分？</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/261.jpg" alt="" />
假设上图是某公司招聘算法工程师的过程。<br />
图中的叶子节点代码决策，在寻找决策过程是形成了树是决策树。<br />
这棵树的深度为3，因为最多经过3次判断就能做出决策。</p>
<h1 id="使用sklearn中的决策树-1"><a class="header" href="#使用sklearn中的决策树-1">使用sklearn中的决策树</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data[:, 2:]
y = iris.target

plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.scatter(X[y==2,0],X[y==2,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/260.png" alt="" /></p>
<pre><code class="language-python">from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier(max_depth=2, criterion='entropy') # entropy = 熵
dt_clf.fit(X, y)

plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.scatter(X[y==2,0],X[y==2,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/259.png" alt="" /><br />
<em>这个分类结果与视频中的分类结果不同</em></p>
<h1 id="总结-3"><a class="header" href="#总结-3">总结</a></h1>
<h2 id="决策树怎样做决策的-1"><a class="header" href="#决策树怎样做决策的-1">决策树怎样做决策的</a></h2>
<p><img src="http://windmissing.github.io/images/2019/262.jpg" alt="" />
当属性是数值特征时，在每一个节点上，选择某一个维度上的数值，以它为域值将样本分成两类。</p>
<h2 id="决策树的特点-1"><a class="header" href="#决策树的特点-1">决策树的特点</a></h2>
<ol>
<li>是非参数学习算法</li>
<li>可以分类问题</li>
<li>天然可以解决多分类问题</li>
<li>也可以解决回归问题（用分类算法将样本归到某个叶子上，该叶子上所有样本的平均值即输出标记）</li>
<li>有非常好的可解释性</li>
</ol>
<h2 id="怎样创建决策树-1"><a class="header" href="#怎样创建决策树-1">怎样创建决策树</a></h2>
<ol>
<li>每个结点在哪个维度做划分？</li>
<li>某个维度的哪个值上做划分？</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><ol>
<li>每个结点在哪个维度做划分？</li>
<li>某个维度的哪个值上做划分？<br />
用信息熵解决以上问题</li>
</ol>
<h1 id="信息熵"><a class="header" href="#信息熵">信息熵</a></h1>
<p>信息熵表示一组变量的不确定度。<br />
熵越大，不确定性越高。<br />
熵越小，不确定性越低。</p>
<p>信息熵的计算公式：
<img src="http://windmissing.github.io/images/2019/263.jpg" alt="" /></p>
<p>假设数据集中有三个类别，样本属于这三个类别的概率分别是{1/3, 1/3, 1/3}，此时信息熵H为：<br />
H = -1/3log(1/3) -1/3log(1/3)  -1/3log(1/3) = 1.0986<br />
如果三个类别的概率分别是{0.1、0.2、0.7}，那么H=0.8018<br />
如果三个类别的概率分别是{1, 0, 0}，那么H=0</p>
<h1 id="以二分类为例画出信息熵的图像"><a class="header" href="#以二分类为例画出信息熵的图像">以二分类为例，画出信息熵的图像</a></h1>
<p>数据只有两个类型，其中一类的概率为x，信息熵计算公式如下：
<img src="http://windmissing.github.io/images/2019/264.jpg" alt="" /></p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def entropy(p):
    return -p * np.log(p) - (1-p) * np.log(1-p)

x = np.linspace(0.01, 0.99, 200)

plt.plot(x, entropy(x))
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/264.png" alt="" /></p>
<p>数据是任何一个类别的概率是一样大时，信息熵最大。<br />
系统偏向于某一类，信息熵降低。<br />
当数据100%确定是某一类时，信息熵为0</p>
<h1 id="总结-4"><a class="header" href="#总结-4">总结：</a></h1>
<p>每一次划分时，让划分后的信息熵为所有划分结果的最低。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="使用sklearn提供的接口"><a class="header" href="#使用sklearn提供的接口">使用sklearn提供的接口</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets

iris = datasets.load_iris()
X = iris.data[:, 2:]
y = iris.target

from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier(max_depth=2, criterion='entropy')
dt_clf.fit(X, y)

def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1,1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1,1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])

    plt.contourf(x0, x1, zz, cmap=custom_cmap)

plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.scatter(X[y==2,0],X[y==2,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/259.jpg" alt="" /></p>
<h1 id="模拟使用信息熵进行划分"><a class="header" href="#模拟使用信息熵进行划分">模拟使用信息熵进行划分</a></h1>
<p>基于维度d上的值value对X和y进行划分</p>
<pre><code class="language-python">def split(X, y, d, value):
    index_a = (X[:,d] &lt;= value)
    index_b = (X[:,d] &gt; value)
    return X[index_a], X[index_b], y[index_a], y[index_b]
</code></pre>
<p>计算划分后的每一组的熵</p>
<pre><code class="language-python">from collections import Counter
from math import log
def entropy(y):
    counter = Counter(y) # counter是键值数据对，键是y的取值，值是y取这个键个数据
    res = 0.0
    for num in counter.values():
        p = num / len(y)
        res += -p * log(p)
    return res
</code></pre>
<p>寻找熵最小的d和value</p>
<pre><code class="language-python">def try_split(X, y):
    best_entropy = float('inf')
    best_d, best_v = -1, -1
    for d in range(X.shape[1]):  # 穷搜每一个维度
        sorted_index = np.argsort(X[:,d])
        for i in range(1, len(X)): # 对每个样本遍历，可选的域值为两个点之间的值
            if X[sorted_index[i-1], d] != X[sorted_index[i], d]:
                v = (X[sorted_index[i-1], d] + X[sorted_index[i], d]) / 2
                x_l, x_r, y_l, y_r = split(X, y, d, v)
                e = entropy(y_l) + entropy(y_r)
                if e &lt; best_entropy:
                    best_entropy, best_d, best_v = e, d, v
    return best_entropy, best_d, best_v
</code></pre>
<p>进行一次划分：</p>
<pre><code class="language-python">best_entropy, best_d, best_v = try_split(X, y)
print(&quot;best_entropy = &quot;, best_entropy)
print(&quot;best_d = &quot;, best_d)
print(&quot;best_v = &quot;, best_v)
</code></pre>
<p>输出：
best_entropy =  0.6931471805599453<br />
best_d =  0  # 代码横轴划分，表现是一根竖线<br />
best_v =  2.45<br />
与上面的图像结果不同，但与视频中的结果相同</p>
<p>存储划分结果：</p>
<pre><code class="language-python"> X1_l, X1_r, y1_l, y1_r = split(X, y, best_d, best_v)
</code></pre>
<p>entropy(y1_l) = 0.0  # 左边只有一种数据，因此信息熵为0<br />
entropy(y1_r) = 0.6931471805599453</p>
<p>左边已经不需要划分了，继续划分右边即可</p>
<pre><code class="language-python">best_entropy2, best_d2, best_v2 = try_split(X1_r, y1_r)
print(&quot;best_entropy = &quot;, best_entropy2)
print(&quot;best_d = &quot;, best_d2)
print(&quot;best_v = &quot;, best_v2)
</code></pre>
<p>输出结果：<br />
best_entropy =  0.4132278899361904
best_d =  1
best_v =  1.75</p>
<pre><code class="language-python"> X2_l, X2_r, y2_l, y2_r = split(X1_r, y1_r, best_d2, best_v2)
</code></pre>
<p>entropy(y2_l) = 0.30849545083110386<br />
entropy(y2_r) = 0.10473243910508653</p>
<p>两个结果都不为零，都可以继续划分</p>
<div style="break-before: page; page-break-before: always;"></div><p>基尼系数的计算公式：
<img src="http://windmissing.github.io/images/2019/265.jpg" alt="" />
其性质与信息熵相同。<br />
基尼系数越大，不确定性越高。<br />
基尼系数越小，不确定性越低。</p>
<p>以二分类为例，在二分类中，其中一类的概率为x，则G = -2x^2 + 2x<br />
如果所有类别的概率相等时，基尼系数最大。类别确定时，基尼系数为0。</p>
<h1 id="使用scikit-learn中的基尼系数划分"><a class="header" href="#使用scikit-learn中的基尼系数划分">使用scikit learn中的基尼系数划分</a></h1>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets

iris = datasets.load_iris()
X = iris.data[:, 2:]
y = iris.target

from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier(max_depth=2, criterion='gini')
dt_clf.fit(X, y)

def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1,1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1,1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])

    plt.contourf(x0, x1, zz, cmap=custom_cmap)

plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.scatter(X[y==2,0],X[y==2,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/266.png" alt="" /></p>
<h1 id="模拟使用基尼系数进行划分"><a class="header" href="#模拟使用基尼系数进行划分">模拟使用基尼系数进行划分</a></h1>
<p>代码和14-3基本上一样</p>
<pre><code class="language-python">from collections import Counter
from math import log

def split(X, y, d, value):
    index_a = (X[:,d] &lt;= value)
    index_b = (X[:,d] &gt; value)
    return X[index_a], X[index_b], y[index_a], y[index_b]

def gini(y):
    counter = Counter(y)
    res = 1.0
    for num in counter.values():
        p = num / len(y)
        res -= p ** 2
    return res

def try_split(X, y):
    best_gini = float('inf')
    best_d, best_v = -1, -1
    for d in range(X.shape[1]):
        sorted_index = np.argsort(X[:,d])
        for i in range(1, len(X)):
            if X[sorted_index[i-1], d] != X[sorted_index[i], d]:
                v = (X[sorted_index[i-1], d] + X[sorted_index[i], d]) / 2
                x_l, x_r, y_l, y_r = split(X, y, d, v)
                e = gini(y_l) + gini(y_r)
                if e &lt; best_gini:
                    best_gini, best_d, best_v = e, d, v
    return best_gini, best_d, best_v
</code></pre>
<p>进行一次划分：</p>
<pre><code class="language-python">best_g, best_d, best_v = try_split(X, y)
print(&quot;best_g = &quot;, best_g)
print(&quot;best_d = &quot;, best_d)
print(&quot;best_v = &quot;, best_v)
</code></pre>
<p>输出：
best_g =  0.5<br />
best_d =  0  # 代码横轴划分，表现是一根竖线<br />
best_v =  2.45</p>
<p>存储划分结果：</p>
<pre><code class="language-python"> X1_l, X1_r, y1_l, y1_r = split(X, y, best_d, best_v)
</code></pre>
<p>gini(y1_l) = 0.0  # 左边只有一种数据，因此信息熵为0<br />
gini(y1_r) = 0.5</p>
<p>左边已经不需要划分了，继续划分右边即可</p>
<pre><code class="language-python">best_g2, best_d2, best_v2 = try_split(X1_r, y1_r)
print(&quot;best_g = &quot;, best_g2)
print(&quot;best_d = &quot;, best_d2)
print(&quot;best_v = &quot;, best_v2)
</code></pre>
<p>输出结果：<br />
best_g =  0.2105714900645938<br />
best_d =  1<br />
best_v =  1.75</p>
<pre><code class="language-python"> X2_l, X2_r, y2_l, y2_r = split(X1_r, y1_r, best_d2, best_v2)
</code></pre>
<p>gini(y2_l) = 0.1680384087791495<br />
gini(y2_r) = 0.04253308128544431</p>
<h1 id="信息熵-vs-基尼系数"><a class="header" href="#信息熵-vs-基尼系数">信息熵 VS 基尼系数</a></h1>
<p>信息熵计算比基尼系数稍慢。<br />
scikit-learn中默认使用基尼系数。<br />
大多数时候二者没有特别的效果优劣。 </p>
<div style="break-before: page; page-break-before: always;"></div><p>决策树又叫CART<br />
CART = Classification And Regression Tree<br />
根据某一个维度v和某一个域值d进行二分，这样得到的树就叫CART<br />
scikit-learn创建决策树的方法就叫CART。<br />
还有其它创建决策树的方法，例如ID3、C4.5、C5.0</p>
<p>创建好决策树好，预测一个样本的时间复杂度为O(log(m))<br />
训练模型的时间复杂度为：O(n<em>m</em>log(m))<br />
是非参数学习算法，非常容易产生过拟合。<br />
实际使用决策树时都要进行剪枝，1.降低复杂度2.解决过拟合</p>
<h1 id="决策树使用不同的超参数的结果对比"><a class="header" href="#决策树使用不同的超参数的结果对比">决策树使用不同的超参数的结果对比</a></h1>
<h2 id="生成数据"><a class="header" href="#生成数据">生成数据</a></h2>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(noise=0.25, random_state=666)

plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/267.png" alt="" /></p>
<h2 id="使用默认参数训练"><a class="header" href="#使用默认参数训练">使用默认参数训练</a></h2>
<pre><code class="language-python">from sklearn.tree import DecisionTreeClassifier

# 默认情况下使用gini系数，且一直向下划分到所有gini系数都是0为止
dt_clf = DecisionTreeClassifier()
dt_clf.fit(X, y)

def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1,1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1,1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])

    plt.contourf(x0, x1, zz, cmap=custom_cmap)

plot_decision_boundary(dt_clf, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.scatter(X[y==2,0],X[y==2,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/268.png" alt="" />
典型的过拟合</p>
<h2 id="使用不同的超参数的结果对比"><a class="header" href="#使用不同的超参数的结果对比">使用不同的超参数的结果对比</a></h2>
<table><thead><tr><th>default</th><th>max_depth=2</th><th><code>min_samples_split=10</code></th><th><code>min_samples_leaf=6</code></th><th><code>max_leaf_nodes=4</code></th></tr></thead><tbody>
<tr><td>一直向下划分到所有gini系数都是0为止</td><td>最大深度</td><td>对于一个节点至少有多个样本才继续划分</td><td>叶子节点至少有几个样本</td><td>最多有多少个叶子</td></tr>
<tr><td><img src="http://windmissing.github.io/images/2019/268.png" alt="" /></td><td><img src="http://windmissing.github.io/images/2019/269.png" alt="" /></td><td><img src="http://windmissing.github.io/images/2019/270.png" alt="" /></td><td><img src="http://windmissing.github.io/images/2019/271.png" alt="" /></td><td><img src="http://windmissing.github.io/images/2019/272.png" alt="" /></td></tr>
</tbody></table>
<p>使用这些参数时：</p>
<ol>
<li>避免欠拟合</li>
<li>使用风格搜索来组合这些参数</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><p>回归问题是指找到一根直线/曲线最大程度的拟合样本点。<br />
不同的回归算法对“拟合”有不同的理解。<br />
例如线性回归算法定义“拟合”为：所有样本点到直线的MSE最小<br />
而SVM将“拟合”定义为：尽可能多的点被包含在margin范围内，取margin中间的直线。（与解决分类问题相反的思路）<br />
margin的距离为超参数
<img src="http://windmissing.github.io/images/2019/258.jpg" alt="" /></p>
<h1 id="代码实现-3"><a class="header" href="#代码实现-3">代码实现</a></h1>
<pre><code class="language-python">import numpy as np
from sklearn import datasets

boston = datasets.load_boston()
x = boston.data
y = boston.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=666)

from sklearn.tree import DecisionTreeRegressor

dt_reg = DecisionTreeRegressor()
dt_reg.fit(X_train, y_train)

dt_reg.score(X_test, y_test)   # 0.6872676909790005
dt_reg.score(X_train, y_train)   # 1.0
</code></pre>
<p>训练数据集上预测的准确率是100%，而在测试数据集上效果不好。
决策树非常容易产生过拟合现象，要消除过拟合就要使用12-5中介绍的参数。</p>
<div style="break-before: page; page-break-before: always;"></div><ol>
<li>决策树的边界都是与某个轴平行的。<br />
场景1：<br />
<img src="http://windmissing.github.io/images/2019/273.jpg" alt="" /><br />
对于图中这样的数据点，决策树给出的决策边界可能是中间这样的，而右边这样的决策边界可能是更好的
场景2：
<img src="http://windmissing.github.io/images/2019/274.jpg" alt="" /><br />
对于这些情况可以很好的划分，但如果对数据稍微做一些旋转，划分结果就不一样了，<br />
<img src="http://windmissing.github.io/images/2019/275.jpg" alt="" /></li>
<li>高度依赖于参数，对样本敏感</li>
<li>在之前的训练中得到这样的决策边界
<img src="http://windmissing.github.io/images/2019/276.jpg" alt="" />
但如果去掉其中一个点，可能就会得到这样的的决策边界
<img src="http://windmissing.github.io/images/2019/277.jpg" alt="" /></li>
</ol>
<p>决策树本身并不能很好的分类，但它是决策森林的基础。使用决策森林能得到很好的结果。</p>
<div style="break-before: page; page-break-before: always;"></div><p>多种机器学习算法都能做同样的事情。让不同的算法针对同一个数据都跑一遍，最终使用投票的方法，少数服从多数，用多数投票的结果作为最终的结果。</p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(noise=0.25, random_state=666)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/278.jpg" alt="" /></p>
<h1 id="自己实现集成学习"><a class="header" href="#自己实现集成学习">自己实现集成学习</a></h1>
<h2 id="逻辑回归"><a class="header" href="#逻辑回归">逻辑回归</a></h2>
<pre><code class="language-python">from sklearn.linear_model import LogisticRegression

log_clf = LogisticRegression()
log_clf.fit(X_train, y_train)
log_clf.score(X_test, y_test)
</code></pre>
<p>输出：0.864</p>
<h2 id="svm"><a class="header" href="#svm">SVM</a></h2>
<pre><code class="language-python">from sklearn.svm import SVC

svm_clf = SVC()
svm_clf.fit(X_train, y_train)
svm_clf.score(X_test, y_test)
</code></pre>
<p>输出：0.888</p>
<h2 id="决策树"><a class="header" href="#决策树">决策树</a></h2>
<pre><code class="language-python">from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier()
dt_clf.fit(X_train, y_train)
dt_clf.score(X_test, y_test)
</code></pre>
<p>输出：0.84</p>
<h2 id="集成学习"><a class="header" href="#集成学习">集成学习</a></h2>
<pre><code class="language-python">y_predict1 = log_clf.predict(X_test)
y_predict2 = svm_clf.predict(X_test)
y_predict3 = dt_clf.predict(X_test)

y_predict = np.array((y_predict1+y_predict2+y_predict3) &gt;= 2, dtype='int')

from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_predict)
</code></pre>
<p>输出：0.896<br />
使用集成学习方法提高了准确率</p>
<h1 id="使用voting-classifier"><a class="header" href="#使用voting-classifier">使用Voting Classifier</a></h1>
<pre><code class="language-python">from sklearn.ensemble import VotingClassifier

voting_clf = VotingClassifier(estimators=[
    ('log_clf', LogisticRegression()),
    ('svm_clf', SVC()),
    ('dt_clf', DecisionTreeClassifier())
], voting='hard')

voting_clf.fit(X_train, y_train)
voting_clf.score(X_test, y_test)
</code></pre>
<p>输出：0.896</p>
<div style="break-before: page; page-break-before: always;"></div><p>多种机器学习算法都能做同样的事情。让不同的算法针对同一个数据都跑一遍，最终使用投票的方法，少数服从多数，用多数投票的结果作为最终的结果。</p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(noise=0.25, random_state=666)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/278.jpg" alt="" /></p>
<h1 id="自己实现集成学习-1"><a class="header" href="#自己实现集成学习-1">自己实现集成学习</a></h1>
<h2 id="逻辑回归-1"><a class="header" href="#逻辑回归-1">逻辑回归</a></h2>
<pre><code class="language-python">from sklearn.linear_model import LogisticRegression

log_clf = LogisticRegression()
log_clf.fit(X_train, y_train)
log_clf.score(X_test, y_test)
</code></pre>
<p>输出：0.864</p>
<h2 id="svm-1"><a class="header" href="#svm-1">SVM</a></h2>
<pre><code class="language-python">from sklearn.svm import SVC

svm_clf = SVC()
svm_clf.fit(X_train, y_train)
svm_clf.score(X_test, y_test)
</code></pre>
<p>输出：0.888</p>
<h2 id="决策树-1"><a class="header" href="#决策树-1">决策树</a></h2>
<pre><code class="language-python">from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier()
dt_clf.fit(X_train, y_train)
dt_clf.score(X_test, y_test)
</code></pre>
<p>输出：0.84</p>
<h2 id="集成学习-1"><a class="header" href="#集成学习-1">集成学习</a></h2>
<pre><code class="language-python">y_predict1 = log_clf.predict(X_test)
y_predict2 = svm_clf.predict(X_test)
y_predict3 = dt_clf.predict(X_test)

y_predict = np.array((y_predict1+y_predict2+y_predict3) &gt;= 2, dtype='int')

from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_predict)
</code></pre>
<p>输出：0.896<br />
使用集成学习方法提高了准确率</p>
<h1 id="使用voting-classifier-1"><a class="header" href="#使用voting-classifier-1">使用Voting Classifier</a></h1>
<pre><code class="language-python">from sklearn.ensemble import VotingClassifier

voting_clf = VotingClassifier(estimators=[
    ('log_clf', LogisticRegression()),
    ('svm_clf', SVC()),
    ('dt_clf', DecisionTreeClassifier())
], voting='hard')

voting_clf.fit(X_train, y_train)
voting_clf.score(X_test, y_test)
</code></pre>
<p>输出：0.896</p>
<div style="break-before: page; page-break-before: always;"></div><p>更合理的投票，应该有权值。</p>
<p>假如一个二分类问题，5个模型分别对一个样本进行分类。以下是每个模型认为每种分类的概率：</p>
<p>模型1：A-99%，B-1%<br />
模型2：A-49%，B-51%<br />
模型3：A-40%，B-60%<br />
模型4：A-90%，B-10%<br />
模型5：A-30%，B-70%</p>
<p>按照hard voting，投票结果为B<br />
但考虑上每种类的概率，投票结果为A<br />
把每个分类的概率当作权值，就是soft voting</p>
<p>soft voting要求集合中的每一个模型都能估计概率<br />
即有predict_proba这个函数</p>
<p>逻辑回归，KNN，决策树（叶子结点的每个类的比例），都能估计概率。<br />
SVM本身没有考虑概率，因为它是计算margin。但SVM可以有一种方法来计算概率。</p>
<h1 id="自己实现集成学习-2"><a class="header" href="#自己实现集成学习-2">自己实现集成学习</a></h1>
<p>使用13-1的数据：</p>
<h2 id="hard-voting-classifier"><a class="header" href="#hard-voting-classifier">Hard Voting Classifier</a></h2>
<pre><code class="language-python">from sklearn.ensemble import VotingClassifier

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

voting_clf = VotingClassifier(estimators=[
    ('log_clf', LogisticRegression()),
    ('svm_clf', SVC()),
    ('dt_clf', DecisionTreeClassifier(random_state=666))
], voting='hard')

voting_clf.fit(X_train, y_train)
voting_clf.score(X_test, y_test)
</code></pre>
<p>输出：0.896</p>
<h2 id="soft-voting-classifier"><a class="header" href="#soft-voting-classifier">Soft Voting Classifier</a></h2>
<pre><code class="language-python">from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

voting_clf2 = VotingClassifier(estimators=[
    ('log_clf', LogisticRegression()),
    ('svm_clf', SVC(probability=True)),
    ('dt_clf', DecisionTreeClassifier(random_state=42))
], voting='soft')

voting_clf2.fit(X_train, y_train)
voting_clf2.score(X_test, y_test)
</code></pre>
<p>输出：0.92</p>
<div style="break-before: page; page-break-before: always;"></div><p>虽然有很多机器学习的算法，但从投票的角度看，仍然不够多<br />
创建更多的子模型，集成更多子模型的意见<br />
子模型之间不能一致，子模型之间要有差异性<br />
如果创建差异性？</p>
<p>解决方法：<br />
每个子模型只看样本数据的一部分。<br />
每个子模型不太需要太高的准确率。只要子模型足够多，准确率就会提高。<br />
例如500个子模型，每个子模型的准确率是60%，最终准确率能达到99.9%</p>
<p>取样方法：</p>
<ul>
<li>放回取样 bagging（bootstrap）</li>
<li>不放回取样 pasting<br />
bagging更常用，优点：</li>
</ul>
<ol>
<li>没有那么依赖随机</li>
<li>数据量要求没那么高</li>
</ol>
<p>使用13-1中的数据</p>
<pre><code class="language-python">from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

bagging_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=500, max_samples=100, bootstrap=True)
# 决策树这种非参数的算法更容易产生差异较大的子模型
# 所有集成学习如果要集成成百上千个子模型，通常首先决策树
# n_estimators：子模型数
# max_samples：每个子模型看的样本树
# bootstrap：放回取样

bagging_clf.fit(X_train, y_train)
bagging_clf.score(X_test, y_test)
</code></pre>
<p>输出：0.912</p>
<pre><code>bagging_clf2 = BaggingClassifier(DecisionTreeClassifier(),n_estimators=1000, max_samples=100, bootstrap=True)
bagging_clf2.fit(X_train, y_train)
bagging_clf2.score(X_test, y_test)
</code></pre>
<p>输出：0.92</p>
<div style="break-before: page; page-break-before: always;"></div><p>OOB：out of bag<br />
放回取样导致一部分样本很有可能没有取到。<br />
平均大约有37%的样本没有取到。<br />
不使用train_test_split，而是使用这部分没有取到的样本做测试/验证。</p>
<p>使用13-1中的数据，但不做train_test_split</p>
<pre><code class="language-python">from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

bagging_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=500, max_samples=100, bootstrap=True, oob_score=True)
bagging_clf.fit(X, y)
bagging_clf.oob_score_
</code></pre>
<p>输出：0.918</p>
<p>bagging的思路极易并行化处理，n_jobs<br />
针对特征进行随机采样： Random Subspaces<br />
当样本的特征非常多的时使用这种方法，比如图像样本<br />
既针对样本，又针对特征进行随机采样：Random Patches</p>
<h2 id="random-subspaces"><a class="header" href="#random-subspaces">Random Subspaces</a></h2>
<pre><code class="language-python">random_subspaces_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=500, max_samples=500, bootstrap=True, oob_score=True, n_jobs=-1, max_features=1, bootstrap_features=True)
random_subspaces_clf.fit(X, y)
random_subspaces_clf.oob_score_
</code></pre>
<p>输出：0.838</p>
<h2 id="random-patches"><a class="header" href="#random-patches">Random Patches</a></h2>
<pre><code class="language-python">random_subspaces_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=500, max_samples=100, bootstrap=True, oob_score=True, n_jobs=-1, max_features=1, bootstrap_features=True)
random_subspaces_clf.fit(X, y)
random_subspaces_clf.oob_score_
</code></pre>
<p>输出：0.874</p>
<div style="break-before: page; page-break-before: always;"></div><p>Bagging<br />
Base Estimator: Decision Tree<br />
只要是以决策树为基础的集成学习算法都叫随机森林。<br />
scikit-learn提供了随机森林算法，并为算法提供了更多的随机性。<br />
sickit-learn中，决策树在节点上划分，在随机的特征子集上寻找最优划分的特征。</p>
<h1 id="随机森林"><a class="header" href="#随机森林">随机森林</a></h1>
<p>使用13-1中的数据，但random_state=666</p>
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=666)

plt.scatter(X[y==0,0],X[y==0,1])
plt.scatter(X[y==1,0],X[y==1,1])
plt.show()
</code></pre>
<p><img src="http://windmissing.github.io/images/2019/280.jpg" alt="" /></p>
<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
rf_clf = RandomForestClassifier(n_estimators=500, random_state=666, oob_score=True)
rf_clf.fit(X, y)
rf_clf.oob_score_
</code></pre>
<p>输出：0.892</p>
<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
rf_clf2 = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=666, oob_score=True)
rf_clf2.fit(X, y)
rf_clf2.oob_score_
</code></pre>
<p>输出：0.906</p>
<h1 id="使用extra-tree"><a class="header" href="#使用extra-tree">使用extra-tree</a></h1>
<p>extra-tree = extreme random trees，极其随机的的随机树。
极其随机表现在：决策树在节点上划分，使用随机的特征的随机的阈值。<br />
提供额外的随机性，抑制过拟合，但增大了bias<br />
相对于随机森林，速度更快</p>
<pre><code class="language-python">from sklearn.ensemble import ExtraTreesClassifier
et_clf = ExtraTreesClassifier(n_estimators=500, bootstrap=True, oob_score=True, random_state=666)
et_clf.fit(X, y)
et_clf.oob_score_
</code></pre>
<p>输出：0.892</p>
<h1 id="集成学习解决回归问题"><a class="header" href="#集成学习解决回归问题">集成学习解决回归问题</a></h1>
<pre><code class="language-python">from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreesRegressor
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>一类集成学习的思路：独立地集成多个模型，让各种子模型在视角上有差异化，并最终综合这些子模型的结果，获得学习的最终结果。<br />
另一类集成学习的思路叫做boosting。boosting即增强的意思。<br />
boosting也要集成多个模型，但每个模型都在尝试增强(boosting)整体的效果。子模型之间不是独立的关系。</p>
<h1 id="ada-boosting"><a class="header" href="#ada-boosting">Ada Boosting</a></h1>
<p><img src="http://windmissing.github.io/images/2019/279.jpg" alt="" /></p>
<p>原始数据集1 --某个算法1--&gt; 某个模型1<br />
模型1没有很好学习的点的权值增大，很好学习到的点的权值减小，得到数据集2 --某个算法2 --&gt; 某个模型2<br />
。。。<br />
每一个子模型都在推动上一个子模型犯的错误<br />
用这些子模型投票得到最终结果</p>
<p>使用13-5中的数据<br />
因为boosting算法没有oob_score，所以使用train_test_split测试算法</p>
<pre><code class="language-python">from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# 使用决策树作为基础模型，决策树的参数在这里都能用
ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=500)
ada_clf.fit(X_train, y_train)
ada_clf.score(X_test, y_test)
</code></pre>
<p>输出：0.864</p>
<h1 id="gradient-boosting"><a class="header" href="#gradient-boosting">Gradient Boosting</a></h1>
<p>训练一个模型m1，产生错误e1<br />
针对e1训练第二个模型m2，产生错误e2<br />
针对e2训练第三个模型m3，产生错误e3<br />
。。。<br />
最终预测结果是m1+m2+m3+...</p>
<p><img src="http://windmissing.github.io/images/2019/281.jpg" alt="" /></p>
<pre><code class="language-python">from sklearn.ensemble import GradientBoostingClassifier

# 指定以决策树为基础，不能选择
gd_clf = GradientBoostingClassifier(max_depth=2, n_estimators=500)
gd_clf.fit(X_train, y_train)
gd_clf.score(X_test, y_test)
</code></pre>
<p>输出：0.904</p>
<h1 id="boosting解决回归问题"><a class="header" href="#boosting解决回归问题">boosting解决回归问题</a></h1>
<pre><code class="language-python">from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p><img src="http://windmissing.github.io/images/2019/282.jpg" alt="" /></p>
<p>stackgin的思路：假设使用三个算法分别得到机器学习的结果，但是不像前面那么直接将三个结果综合，而是用这三个算法作为输出，训练一个新的模型，使用这个模型的输出作为最终的输出。</p>
<p><img src="http://windmissing.github.io/images/2019/283.jpg" alt="" /></p>
<p>训练方法：将数据集分成2分，一份用于训练第一层的3个模型，第二份用于训练第二层的模型。最终得到整体模型。</p>
<p>这个模型还可以更复杂，比如：
<img src="http://windmissing.github.io/images/2019/284.jpg" alt="" />
要训练这个模型需要把数据分成3份，每一份用于训练一层模型。</p>
<p>这样的模型非常容易过拟合。<br />
这样的模型有点类似于神经网络。这是深度学习的范畴。<br />
scikit-learn没有提供stacking模型的接口。</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
    </body>
</html>
