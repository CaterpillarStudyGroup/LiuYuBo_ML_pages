# Summary

* [Introduction](README.md)
* 第四章：K近邻算法
   * [4-1 K近邻算法](Chapter4/4-1.md)
   * [4-2 scikit-learn中的机器学习算法的封装](Chapter4/4-2.md)
   * [4-3 训练数据集，测试数据集](Chapter4/4-3.md)
   * [4-4 分类准确度](Chapter4/4-4.md)
   * [4-5 超参数](Chapter4/4-5.md)
   * [4-6 网格搜索](Chapter4/4-6.md)
   * [4-7 数据归一化 Feature Scaling](Chapter4/4-7.md)
   * [4-8 scikit-learn中的Scaler](Chapter4/4-8.md)
   * [4-9 更多有关K近邻算法的思考](Chapter4/4-9.md)
* [第五章：线性回归法](Chapter5/5-1.md)
   * [5-1 简单线性回归](Chapter5/5-1.md)
   * [5-2 最小二乘法](Chapter5/5-2.md)
   * [5-3 简单线性回归的实现](Chapter5/5-3.md)
   * [5-4 参数计算向量化](Chapter5/5-4.md)
   * [5-5 衡量线性回归算法的指标](Chapter5/5-5.md)
   * [5-6 最好的衡量线性回归法的指标 R Squared](Chapter5/5-6.md)
   * [5-7 简单线性回归和正规方程解](Chapter5/5-7.md)
   * [5-8 实现多元线性回归](Chapter5/5-8.md)
   * [5-9 scikit-learn中的回归算法](Chapter5/5-9.md)
   * [5-10 线性回归的可解释性和更多思考](Chapter5/5-10.md)
* [第六章：梯度下降法](Chapter6/6-1.md)
   * [6-1 什么是梯度下降法](Chapter6/6-1.md)
   * [6-2 模拟实现梯度下降法](Chapter6/6-2.md)
   * [6-3 多元线性回归中的梯度下降法](Chapter6/6-3.md)
   * [6-4 在线性回归模型中使用梯度下降法](Chapter6/6-4.md)
   * [6-5 梯度下降的向量化](Chapter6/6-5.md)
   * [6-6 随机梯度下降](Chapter6/6-6.md)
   * [6-7 代码实现随机梯度下降](Chapter6/6-7.md)
   * [6-8 调试梯度下降法](Chapter6/6-8.md)
   * [6-9 有关梯度下降法的更多深入讨论](Chapter6/6-9.md)
* [第七章：PCA与梯度上升法](Chapter7/7-1.md)   
   * [7-1 什么是PCA](Chapter7/7-1.md)
   * [7-2 使用梯度上升法求解主成分分析问题](Chapter7/7-2.md)
   * [7-3 代码实现主成分分析问题](Chapter7/7-3.md)
   * [7-4 求数据的前N个主成分](Chapter7/7-4.md)
   * [7-5 高维数据向低维数据映射](Chapter7/7-5.md)
   * [7-6 scikit learn中的PCA](Chapter7/7-6.md)
   * [7-7 MNIST数据集](Chapter7/7-7.md)
   * [7-8 使用PCA降噪](Chapter7/7-8.md)
   * [7-9 人脸识别和特征脸(未完成)](Chapter7/7-9.md)
* [第八章：多项式回归与模型泛化](Chapter8/8-1.md)  
   * [8-1 什么是多项式回归](Chapter8/8-1.md)
   * [8-2 scikit-learn中的多项式回归和pipeline](Chapter8/8-2.md)
   * [8-3 过拟合和欠拟合](Chapter8/8-3.md)
   * [8-4 为什么要训练数据集和测试数据集](Chapter8/8-4.md)
   * [8-5 学习曲线](Chapter8/8-5.md)
   * [8-6 验证数据集与交叉验证](Chapter8/8-6.md)
   * [8-7 偏差方差权衡 Bias Variance Trade off](Chapter8/8-7.md)
   * [8-8 模型正则化 Regularization](Chapter8/8-8.md)
   * [8-9 LASSO Regularization](Chapter8/8-9.md)
   * [8-10 L1,L2和弹性网络](Chapter8/8-10.md)
* [第九章：逻辑回归](Chapter9/9-1.md)  
   * [9-1 逻辑回归 Logistic Regression](Chapter9/9-1.md)
   * [9-2 逻辑回归的损失函数](Chapter9/9-2.md)
   * [9-3 逻辑回归算法损失函数的梯度](Chapter9/9-3.md)
   * [9-4 实现逻辑回归算法](Chapter9/9-4.md)
   * [9-5 决策边界](Chapter9/9-5.md)
   * [9-6 在逻辑回归中使用多项式特征](Chapter9/9-6.md)
   * [9-7 scikit-learn中的逻辑回归](Chapter9/9-7.md)
   * [9-8 OvR与OvO](Chapter9/9-8.md)
* [第十章：评价分类结果](Chapter10/10-1.md)  
   * [10-1 准确度的陷阱和混淆矩阵](Chapter10/10-1.md)
   * [10-2 精确率和召回率](Chapter10/10-2.md)
   * [10-3 实现混淆矩阵、精准率、召回率](Chapter10/10-3.md)
   * [10-4 F1 score](Chapter10/10-4.md)
   * [10-5 Precision-Recall平衡](Chapter10/10-5.md)
   * [10-6 precision-recall曲线](Chapter10/10-6.md)
   * [10-7 ROC曲线](Chapter10/10-7.md)
   * [10-8 多分类问题中的混淆矩阵](Chapter10/10-8.md)
* 第十一章：支撑向量机 SVM
   * [11-1 什么是支撑向量机](Chapter11/11-1.md)
   * [11-2 支撑向量机的推导过程](Chapter11/11-2.md)
   * [11-3 Soft Margin和SVM的正则化](Chapter11/11-3.md)
   * [11-4 scikit-leran中的SVM](Chapter11/11-4.md)
   * [11-5 SVM中使用多项式特征](Chapter11/11-5.md)
   * [11-6 什么是核函数](Chapter11/11-6.md)
   * [11-7 高斯核函数](Chapter11/11-7.md)
   * [11-8 scikit-learn中的高斯核函数](Chapter11/11-8.md)
   * [11-9 SVM思想解决回归问题](Chapter11/11-9.md)
* [第十二章：决策树](Chapter12/12-1.md)  
   * [12-1 什么是决策树](Chapter12/12-1.md)
   * [12-2 信息熵](Chapter12/12-2.md)
   * [12-3 使用信息寻找最优划分](Chapter12/12-3.md)
   * [12-4 基尼系数](Chapter12/12-4.md)
   * [12-5 CART和决策树中的超参数](Chapter12/12-5.md)
   * [12-6 决策树解决回归问题](Chapter12/12-6.md)
   * [12-7 决策树的局限性](Chapter12/12-7.md)
* [第十三章：集成学习和随机森林](Chapter13/13-1.md)  
   * [13-1 什么是集成学习](Chapter13/13-1.md)
   * [13-2 soft voting](Chapter13/13-2.md)
   * [13-3 bagging和pasting](Chapter13/13-3.md)
   * [13-4 更多关于bagging的讨论](Chapter13/13-4.md)
   * [13-5 随机森林和extra-trees](Chapter13/13-5.md)
   * [13-6 ada boosting和gradiesnt boosting](Chapter13/13-6.md)
   * [13-7 Stacking](Chapter13/13-7.md)